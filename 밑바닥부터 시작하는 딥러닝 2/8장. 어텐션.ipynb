{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1 어텐션의 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.3 Decoder 개선 (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n",
      "(5, 4)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "T, H = 5, 4\n",
    "hs = np.random.randn(T, H)\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
    "\n",
    "ar = a.reshape(5, 1).repeat(4, axis=1)\n",
    "print(ar.shape)\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "\n",
    "c = np.sum(t, axis=0)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 4)\n"
     ]
    }
   ],
   "source": [
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "a = np.random.randn(N, T)\n",
    "ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "\n",
    "c = np.sum(t, axis=1)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중합 계층 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "        \n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "    \n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.reshape\n",
    "        \n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1) # sum의 역전파\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2) # repeat의 역전파\n",
    "        \n",
    "        return dhs, da\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.4 Decoder 개선 (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 5)\n",
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\deep-learning-from-scratch-2-master')\n",
    "from common.layers import Softmax\n",
    "import numpy as np\n",
    "\n",
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "h = np.random.randn(N, H)\n",
    "hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
    "\n",
    "t = hs * hr\n",
    "print(t.shape)\n",
    "\n",
    "s = np.sum(t, axis=2)\n",
    "print(s.shape)\n",
    "\n",
    "softmax = Softmax()\n",
    "a = softmax.forward(s)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AttentionWeight 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\deep-learning-from-scratch-2-master')\n",
    "from common.np import *\n",
    "from common.layers import Softmax\n",
    "\n",
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, hs, h):\n",
    "        N, T, H - hs.shape\n",
    "        \n",
    "        hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "        \n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "    \n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "        \n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.5 Decoder 개선 (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention 계층 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "    \n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Attention 계층 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "    \n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:, t, :] = dh\n",
    "        \n",
    "        return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2 어텐션을 갖춘 seq2seq 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.1 Encoder 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AttentionEncoder 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\deep-learning-from-scratch-2-master')\n",
    "from common.time_layers import *\n",
    "from ch07.seq2seq import Encoder, Seq2seq\n",
    "from ch08.attention_layer import TimeAttention\n",
    "\n",
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "    \n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.2 Decoder 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "        \n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads +=layer.grads\n",
    "    \n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "        \n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        c = self.attention.forward(enc_hs, dec_hs)\n",
    "        out = np.concatenate((c, dec_hs), axis=2)\n",
    "        score = self.affine.forward(out)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh\n",
    "        self.embed.backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "    \n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.3 seq2seq 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ch07.seq2seq import Encoder, Seq2seq\n",
    "\n",
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttnetionDecoder(*args)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "        \n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.3 어텐션 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.2 어텐션을 갖춘 seq2seq의 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
      "| 에폭 1 |  반복 21 / 351 | 시간 10[s] | 손실 3.09\n",
      "| 에폭 1 |  반복 41 / 351 | 시간 20[s] | 손실 1.90\n",
      "| 에폭 1 |  반복 61 / 351 | 시간 30[s] | 손실 1.72\n",
      "| 에폭 1 |  반복 81 / 351 | 시간 39[s] | 손실 1.46\n",
      "| 에폭 1 |  반복 101 / 351 | 시간 49[s] | 손실 1.19\n",
      "| 에폭 1 |  반복 121 / 351 | 시간 60[s] | 손실 1.14\n",
      "| 에폭 1 |  반복 141 / 351 | 시간 70[s] | 손실 1.09\n",
      "| 에폭 1 |  반복 161 / 351 | 시간 81[s] | 손실 1.06\n",
      "| 에폭 1 |  반복 181 / 351 | 시간 92[s] | 손실 1.04\n",
      "| 에폭 1 |  반복 201 / 351 | 시간 103[s] | 손실 1.03\n",
      "| 에폭 1 |  반복 221 / 351 | 시간 115[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 241 / 351 | 시간 126[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 261 / 351 | 시간 137[s] | 손실 1.01\n",
      "| 에폭 1 |  반복 281 / 351 | 시간 147[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 301 / 351 | 시간 158[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 321 / 351 | 시간 168[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 341 / 351 | 시간 178[s] | 손실 1.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1978-08-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1978-08-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1978-08-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1978-08-11\n",
      "---\n",
      "val acc 0.000%\n",
      "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 1.00\n",
      "| 에폭 2 |  반복 21 / 351 | 시간 10[s] | 손실 1.00\n",
      "| 에폭 2 |  반복 41 / 351 | 시간 21[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 61 / 351 | 시간 31[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 81 / 351 | 시간 42[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 101 / 351 | 시간 53[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 121 / 351 | 시간 64[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 141 / 351 | 시간 75[s] | 손실 0.98\n",
      "| 에폭 2 |  반복 161 / 351 | 시간 86[s] | 손실 0.98\n",
      "| 에폭 2 |  반복 181 / 351 | 시간 96[s] | 손실 0.97\n",
      "| 에폭 2 |  반복 201 / 351 | 시간 107[s] | 손실 0.95\n",
      "| 에폭 2 |  반복 221 / 351 | 시간 117[s] | 손실 0.94\n",
      "| 에폭 2 |  반복 241 / 351 | 시간 128[s] | 손실 0.90\n",
      "| 에폭 2 |  반복 261 / 351 | 시간 140[s] | 손실 0.83\n",
      "| 에폭 2 |  반복 281 / 351 | 시간 152[s] | 손실 0.74\n",
      "| 에폭 2 |  반복 301 / 351 | 시간 164[s] | 손실 0.66\n",
      "| 에폭 2 |  반복 321 / 351 | 시간 175[s] | 손실 0.58\n",
      "| 에폭 2 |  반복 341 / 351 | 시간 187[s] | 손실 0.46\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 2006-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2007-08-09\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1983-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 2016-11-08\n",
      "---\n",
      "val acc 51.640%\n",
      "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 3 |  반복 21 / 351 | 시간 11[s] | 손실 0.30\n",
      "| 에폭 3 |  반복 41 / 351 | 시간 22[s] | 손실 0.21\n",
      "| 에폭 3 |  반복 61 / 351 | 시간 34[s] | 손실 0.14\n",
      "| 에폭 3 |  반복 81 / 351 | 시간 45[s] | 손실 0.09\n",
      "| 에폭 3 |  반복 101 / 351 | 시간 55[s] | 손실 0.07\n",
      "| 에폭 3 |  반복 121 / 351 | 시간 66[s] | 손실 0.05\n",
      "| 에폭 3 |  반복 141 / 351 | 시간 76[s] | 손실 0.04\n",
      "| 에폭 3 |  반복 161 / 351 | 시간 86[s] | 손실 0.03\n",
      "| 에폭 3 |  반복 181 / 351 | 시간 97[s] | 손실 0.03\n",
      "| 에폭 3 |  반복 201 / 351 | 시간 108[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 221 / 351 | 시간 119[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 241 / 351 | 시간 131[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 261 / 351 | 시간 142[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 281 / 351 | 시간 152[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 301 / 351 | 시간 163[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 321 / 351 | 시간 173[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 341 / 351 | 시간 183[s] | 손실 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 99.900%\n",
      "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 21 / 351 | 시간 12[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 41 / 351 | 시간 23[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 61 / 351 | 시간 35[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 81 / 351 | 시간 46[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 101 / 351 | 시간 57[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 121 / 351 | 시간 67[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 141 / 351 | 시간 77[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 161 / 351 | 시간 88[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 181 / 351 | 시간 98[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 201 / 351 | 시간 109[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 221 / 351 | 시간 121[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 241 / 351 | 시간 132[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 261 / 351 | 시간 143[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 281 / 351 | 시간 154[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 301 / 351 | 시간 164[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 321 / 351 | 시간 175[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 341 / 351 | 시간 185[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 99.900%\n",
      "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 21 / 351 | 시간 12[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 41 / 351 | 시간 23[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 61 / 351 | 시간 33[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 81 / 351 | 시간 44[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 101 / 351 | 시간 54[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 121 / 351 | 시간 64[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 141 / 351 | 시간 75[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 161 / 351 | 시간 86[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 181 / 351 | 시간 97[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 201 / 351 | 시간 109[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 221 / 351 | 시간 120[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 241 / 351 | 시간 131[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 261 / 351 | 시간 141[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 281 / 351 | 시간 152[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 301 / 351 | 시간 162[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 321 / 351 | 시간 172[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 341 / 351 | 시간 183[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 99.920%\n",
      "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 21 / 351 | 시간 11[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 41 / 351 | 시간 21[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 61 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 81 / 351 | 시간 41[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 101 / 351 | 시간 52[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 121 / 351 | 시간 63[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 141 / 351 | 시간 74[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 161 / 351 | 시간 86[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 181 / 351 | 시간 97[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 201 / 351 | 시간 109[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 221 / 351 | 시간 120[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 241 / 351 | 시간 130[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 261 / 351 | 시간 143[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 281 / 351 | 시간 155[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 301 / 351 | 시간 168[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 321 / 351 | 시간 182[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 341 / 351 | 시간 196[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 99.940%\n",
      "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 21 / 351 | 시간 10[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 41 / 351 | 시간 20[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 61 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 81 / 351 | 시간 42[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 101 / 351 | 시간 53[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 121 / 351 | 시간 64[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 141 / 351 | 시간 76[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 161 / 351 | 시간 87[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 181 / 351 | 시간 98[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 201 / 351 | 시간 108[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 221 / 351 | 시간 119[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 241 / 351 | 시간 129[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 261 / 351 | 시간 139[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 281 / 351 | 시간 150[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 301 / 351 | 시간 161[s] | 손실 0.01\n",
      "| 에폭 7 |  반복 321 / 351 | 시간 173[s] | 손실 0.05\n",
      "| 에폭 7 |  반복 341 / 351 | 시간 185[s] | 손실 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 99.200%\n",
      "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
      "| 에폭 8 |  반복 21 / 351 | 시간 10[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 41 / 351 | 시간 20[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 61 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 81 / 351 | 시간 42[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 101 / 351 | 시간 53[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 121 / 351 | 시간 64[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 141 / 351 | 시간 76[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 161 / 351 | 시간 87[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 181 / 351 | 시간 98[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 201 / 351 | 시간 108[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 221 / 351 | 시간 118[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 241 / 351 | 시간 129[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 261 / 351 | 시간 140[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 281 / 351 | 시간 151[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 301 / 351 | 시간 162[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 321 / 351 | 시간 174[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 341 / 351 | 시간 185[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 100.000%\n",
      "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 21 / 351 | 시간 10[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 41 / 351 | 시간 21[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 61 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 81 / 351 | 시간 42[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 101 / 351 | 시간 54[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 121 / 351 | 시간 66[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 141 / 351 | 시간 77[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 161 / 351 | 시간 88[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 181 / 351 | 시간 98[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 201 / 351 | 시간 109[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 221 / 351 | 시간 119[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 241 / 351 | 시간 129[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 261 / 351 | 시간 140[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 281 / 351 | 시간 151[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 301 / 351 | 시간 163[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 321 / 351 | 시간 174[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 341 / 351 | 시간 185[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 100.000%\n",
      "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 21 / 351 | 시간 10[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 41 / 351 | 시간 21[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 61 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 81 / 351 | 시간 43[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 101 / 351 | 시간 54[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 121 / 351 | 시간 66[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 141 / 351 | 시간 76[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 161 / 351 | 시간 87[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 181 / 351 | 시간 97[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 201 / 351 | 시간 107[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 221 / 351 | 시간 118[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 241 / 351 | 시간 128[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 261 / 351 | 시간 139[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 281 / 351 | 시간 151[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 301 / 351 | 시간 162[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 321 / 351 | 시간 173[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 341 / 351 | 시간 184[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 100.000%\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\deep-learning-from-scratch-2-master')\n",
    "sys.path.append('C:\\deep-learning-from-scratch-2-master/ch07')\n",
    "import numpy as np\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from ch08.attention_seq2seq import AttentionSeq2seq\n",
    "from ch07.seq2seq import Seq2seq\n",
    "from ch07.peeky_seq2seq import PeekySeq2seq\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "                batch_size=batch_size, max_grad=max_grad)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                    id_to_char, verbose, is_reverse=True)\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('val acc %.3f%%' % (acc * 100))\n",
    "\n",
    "model.save_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.3 어텐션 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\deep-learning-from-scratch-2-master')\n",
    "import numpy as np\n",
    "from dataset import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from ch08.attention_seq2seq import AttentionSeq2seq\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "model.load_params()\n",
    "\n",
    "_idx = 0\n",
    "def visualize(attention_map, row_labels, column_labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pcolor(attention_map, cmap=plt.cm.Greys_r, vmin=0.0, vmax=1.0)\n",
    "\n",
    "    ax.patch.set_facecolor('black')\n",
    "    ax.set_yticks(np.arange(attention_map.shape[0])+0.5, minor=False)\n",
    "    ax.set_xticks(np.arange(attention_map.shape[1])+0.5, minor=False)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xticklabels(row_labels, minor=False)\n",
    "    ax.set_yticklabels(column_labels, minor=False)\n",
    "\n",
    "    global _idx\n",
    "    _idx += 1\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "np.random.seed(1984)\n",
    "for _ in range(5):\n",
    "    idx = [np.random.randint(0, len(x_test))]\n",
    "    x = x_test[idx]\n",
    "    t = t_test[idx]\n",
    "\n",
    "    model.forward(x, t)\n",
    "    d = model.decoder.attention.attention_weights\n",
    "    d = np.array(d)\n",
    "    attention_map = d.reshape(d.shape[0], d.shape[2])\n",
    "\n",
    "    # 출력하기 위해 반전\n",
    "    attention_map = attention_map[:,::-1]\n",
    "    x = x[:,::-1]\n",
    "\n",
    "    row_labels = [id_to_char[i] for i in x[0]]\n",
    "    column_labels = [id_to_char[i] for i in t[0]]\n",
    "    column_labels = column_labels[1:]\n",
    "\n",
    "    visualize(attention_map, row_labels, column_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.4 어텐션에 관한 남은 이야기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4.1 양방향 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeBiLSTM:\n",
    "    def __init__(self, Wx1, Wh1, b1,\n",
    "                 Wx2, Wh2, b2, stateful=False):\n",
    "        self.forward_lstm = TimeLSTM(Wx1, Wh1, b1, stateful)\n",
    "        self.backward_lstm = TimeLSTM(Wx2, Wh2, b2, stateful)\n",
    "        self.params = self.forward_lstm.params + self.backward_lstm.params\n",
    "        self.grads = self.forward_lstm.grads + self.backward_lstm.grads\n",
    "\n",
    "    def forward(self, xs):\n",
    "        o1 = self.forward_lstm.forward(xs)\n",
    "        o2 = self.backward_lstm.forward(xs[:, ::-1])\n",
    "        o2 = o2[:, ::-1]\n",
    "\n",
    "        out = np.concatenate((o1, o2), axis=2)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        H = dhs.shape[2] // 2\n",
    "        do1 = dhs[:, :, :H]\n",
    "        do2 = dhs[:, :, H:]\n",
    "\n",
    "        dxs1 = self.forward_lstm.backward(do1)\n",
    "        do2 = do2[:, ::-1]\n",
    "        dxs2 = self.backward_lstm.backward(do2)\n",
    "        dxs2 = dxs2[:, ::-1]\n",
    "        dxs = dxs1 + dxs2\n",
    "        return dxs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
