{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2626,
     "status": "ok",
     "timestamp": 1606129753094,
     "user": {
      "displayName": "‍구병모[ 학부재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "10190861927292343283"
     },
     "user_tz": -540
    },
    "id": "tK7C1yvax3Pw"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('C:\\deep-learning-from-scratch-master') \n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "\n",
    "\n",
    "class DeepConvNet:\n",
    "    \"\"\"정확도 99% 이상의 고정밀 합성곱 신경망\n",
    "\n",
    "    네트워크 구성은 아래와 같음\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        affine - relu - dropout - affine - dropout - softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 hidden_size=50, output_size=10):\n",
    "        # 가중치 초기화===========\n",
    "        # 각 층의 뉴런 하나당 앞 층의 몇 개 뉴런과 연결되는가（TODO: 자동 계산되게 바꿀 것）\n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
    "        wight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLU를 사용할 때의 권장 초깃값\n",
    "        \n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0]\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = wight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        self.params['W7'] = wight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = wight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성===========\n",
    "        self.layers = []\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
    "                           conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
    "                           conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
    "                           conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "executionInfo": {
     "elapsed": 2040,
     "status": "error",
     "timestamp": 1606132353139,
     "user": {
      "displayName": "‍구병모[ 학부재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "10190861927292343283"
     },
     "user_tz": -540
    },
    "id": "4F4Xc4ctyPKd",
    "outputId": "fc2bbdaf-c22a-434b-d8a6-0cc24616c1ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3010117661031013\n",
      "=== epoch:1, train acc:0.115, test acc:0.108 ===\n",
      "train loss:2.284569640420327\n",
      "train loss:2.251622435418108\n",
      "train loss:2.2510426652522817\n",
      "train loss:2.3138054553770555\n",
      "train loss:2.2587934372281238\n",
      "train loss:2.2645213253882512\n",
      "train loss:2.2452238509492513\n",
      "train loss:2.247810114471489\n",
      "train loss:2.2499466261083567\n",
      "train loss:2.2139028981607005\n",
      "train loss:2.2351346822720313\n",
      "train loss:2.15285451275638\n",
      "train loss:2.1957379808921496\n",
      "train loss:2.155389781083886\n",
      "train loss:2.1804601817769993\n",
      "train loss:2.096009170024893\n",
      "train loss:2.203151571489256\n",
      "train loss:2.070936467269962\n",
      "train loss:2.064361411313009\n",
      "train loss:2.157145704605557\n",
      "train loss:2.090334056140737\n",
      "train loss:1.9989235680090365\n",
      "train loss:2.075746469523381\n",
      "train loss:1.9812640610246328\n",
      "train loss:2.0115711980510924\n",
      "train loss:2.0441021800021217\n",
      "train loss:1.9989693211733033\n",
      "train loss:2.0352503575605976\n",
      "train loss:2.004960628139425\n",
      "train loss:1.9903624093667154\n",
      "train loss:1.844940353894261\n",
      "train loss:1.8192272925914537\n",
      "train loss:1.8613859250727196\n",
      "train loss:1.8799139466775667\n",
      "train loss:1.820029405566901\n",
      "train loss:1.795816645178312\n",
      "train loss:1.8771042105533007\n",
      "train loss:2.090849581730783\n",
      "train loss:1.7831712137990618\n",
      "train loss:1.899057227453106\n",
      "train loss:1.7389565088647247\n",
      "train loss:1.9537742323053453\n",
      "train loss:1.7000844512226596\n",
      "train loss:1.737245870828055\n",
      "train loss:1.7695859476289073\n",
      "train loss:1.6810081502084544\n",
      "train loss:1.777528049358061\n",
      "train loss:1.691913823942599\n",
      "train loss:1.7759993536245053\n",
      "train loss:1.7644572729372792\n",
      "train loss:1.7324665351298874\n",
      "train loss:1.665444182985548\n",
      "train loss:1.6495233104923874\n",
      "train loss:1.8744102677922334\n",
      "train loss:1.647200275556644\n",
      "train loss:1.752082627261411\n",
      "train loss:1.632401853626958\n",
      "train loss:1.5498349970966203\n",
      "train loss:1.717155285449374\n",
      "train loss:1.7023497919725301\n",
      "train loss:1.7524457570376037\n",
      "train loss:1.5815933600030652\n",
      "train loss:1.4668094913435397\n",
      "train loss:1.6090424993652623\n",
      "train loss:1.5825207382574167\n",
      "train loss:1.566166083709217\n",
      "train loss:1.4849923032126466\n",
      "train loss:1.6758293825310695\n",
      "train loss:1.7051980970567657\n",
      "train loss:1.5568202071117867\n",
      "train loss:1.3907694681860203\n",
      "train loss:1.624074933518376\n",
      "train loss:1.5881615267293834\n",
      "train loss:1.623845458898673\n",
      "train loss:1.5732154953413164\n",
      "train loss:1.9097519324275627\n",
      "train loss:1.5997379226175936\n",
      "train loss:1.3632637699356909\n",
      "train loss:1.6102665927268558\n",
      "train loss:1.6500276151861306\n",
      "train loss:1.564336684523271\n",
      "train loss:1.7279970756673333\n",
      "train loss:1.5706870953147785\n",
      "train loss:1.6073113255336775\n",
      "train loss:1.730659916799178\n",
      "train loss:1.6643046552585394\n",
      "train loss:1.5114611468322912\n",
      "train loss:1.499075243216406\n",
      "train loss:1.3893522267220961\n",
      "train loss:1.6166794030701899\n",
      "train loss:1.5947540114840828\n",
      "train loss:1.5108083739563452\n",
      "train loss:1.5350374461418743\n",
      "train loss:1.5385808484779857\n",
      "train loss:1.4993833141623616\n",
      "train loss:1.6012230801875607\n",
      "train loss:1.3348946273286948\n",
      "train loss:1.3696637708154793\n",
      "train loss:1.5397570087051997\n",
      "train loss:1.4968437716384486\n",
      "train loss:1.3208981596290863\n",
      "train loss:1.2865960933994378\n",
      "train loss:1.3876555161660775\n",
      "train loss:1.3635928547178553\n",
      "train loss:1.3002132530222748\n",
      "train loss:1.494105600297259\n",
      "train loss:1.3005309954689617\n",
      "train loss:1.4606157494530967\n",
      "train loss:1.166287375994995\n",
      "train loss:1.6141175779810921\n",
      "train loss:1.4055879129063802\n",
      "train loss:1.497822140064063\n",
      "train loss:1.4260789072543432\n",
      "train loss:1.4651969964364389\n",
      "train loss:1.2695504528138786\n",
      "train loss:1.4140194717051444\n",
      "train loss:1.4525396224724845\n",
      "train loss:1.478419041814914\n",
      "train loss:1.5182889107053106\n",
      "train loss:1.485607479975155\n",
      "train loss:1.4207820745759434\n",
      "train loss:1.2942373333301507\n",
      "train loss:1.581346777704466\n",
      "train loss:1.668766883143103\n",
      "train loss:1.2507508454936491\n",
      "train loss:1.3446829540823095\n",
      "train loss:1.5748008458574938\n",
      "train loss:1.359711582127534\n",
      "train loss:1.4764691597872002\n",
      "train loss:1.246473694746271\n",
      "train loss:1.3213475880478407\n",
      "train loss:1.2971470775425322\n",
      "train loss:1.2748125262560643\n",
      "train loss:1.3717546015407711\n",
      "train loss:1.5857184900288315\n",
      "train loss:1.396112638929045\n",
      "train loss:1.496019931999025\n",
      "train loss:1.4541660967031083\n",
      "train loss:1.2773110737137374\n",
      "train loss:1.3034495832194841\n",
      "train loss:1.4118543164281891\n",
      "train loss:1.490269590213902\n",
      "train loss:1.4331660939466528\n",
      "train loss:1.280015793688567\n",
      "train loss:1.595104223121138\n",
      "train loss:1.4835241712480394\n",
      "train loss:1.3759633248463892\n",
      "train loss:1.4475780131999028\n",
      "train loss:1.50438736921472\n",
      "train loss:1.523792995419031\n",
      "train loss:1.3203931485777705\n",
      "train loss:1.4523006315576494\n",
      "train loss:1.319956105871218\n",
      "train loss:1.3648358630000836\n",
      "train loss:1.4392631030143008\n",
      "train loss:1.2676730467184447\n",
      "train loss:1.3960408501786015\n",
      "train loss:1.335997001445965\n",
      "train loss:1.4738552462031984\n",
      "train loss:1.3178205553009261\n",
      "train loss:1.433106222846029\n",
      "train loss:1.1918921035775967\n",
      "train loss:1.1210210269827603\n",
      "train loss:1.408678004420398\n",
      "train loss:1.2668143530821252\n",
      "train loss:1.4886560663495934\n",
      "train loss:1.3359992137084316\n",
      "train loss:1.5481272336608976\n",
      "train loss:1.2062104924063661\n",
      "train loss:1.0083970901066608\n",
      "train loss:1.4479204704992668\n",
      "train loss:1.1307343118306243\n",
      "train loss:1.2561102712094043\n",
      "train loss:1.4043871844697187\n",
      "train loss:1.1276409669675953\n",
      "train loss:1.2666409970597408\n",
      "train loss:1.2742962218848124\n",
      "train loss:1.5278539017806736\n",
      "train loss:1.336157074382523\n",
      "train loss:1.3183585226380858\n",
      "train loss:1.2264629635850763\n",
      "train loss:1.3132405732680177\n",
      "train loss:1.2839335032812338\n",
      "train loss:1.3113297737862508\n",
      "train loss:1.3902633403443454\n",
      "train loss:1.3493887477130189\n",
      "train loss:1.2130923565365015\n",
      "train loss:1.3403837212943173\n",
      "train loss:1.2744173427714716\n",
      "train loss:1.3623616131750886\n",
      "train loss:1.457375555386488\n",
      "train loss:1.3753270881037951\n",
      "train loss:1.2118545701840853\n",
      "train loss:1.3962107544097493\n",
      "train loss:1.3478178130180132\n",
      "train loss:1.4838467422494992\n",
      "train loss:1.1663887861419053\n",
      "train loss:1.5872471185331938\n",
      "train loss:1.163487215393787\n",
      "train loss:1.3249490675700728\n",
      "train loss:1.385641343569809\n",
      "train loss:1.3854596869708669\n",
      "train loss:1.1748881364813983\n",
      "train loss:1.3137618848501043\n",
      "train loss:1.3886751795468248\n",
      "train loss:1.2484443227154824\n",
      "train loss:1.4165358715537264\n",
      "train loss:1.1152232907082082\n",
      "train loss:1.1933105896377998\n",
      "train loss:1.1833720117432964\n",
      "train loss:1.2228132470925357\n",
      "train loss:1.2270861746090638\n",
      "train loss:1.3757526976529098\n",
      "train loss:1.1623854087236047\n",
      "train loss:1.0256704043129758\n",
      "train loss:1.167572687082972\n",
      "train loss:1.1793266610253716\n",
      "train loss:1.2701982865054813\n",
      "train loss:1.2337836733971783\n",
      "train loss:1.3857773472771129\n",
      "train loss:1.17191107890947\n",
      "train loss:1.0208716471026127\n",
      "train loss:1.283888118184612\n",
      "train loss:1.1701323014817537\n",
      "train loss:1.3884599248655691\n",
      "train loss:1.3859874459437709\n",
      "train loss:1.3884294876087007\n",
      "train loss:1.1853119936628806\n",
      "train loss:1.2291760174464859\n",
      "train loss:1.0682438299518162\n",
      "train loss:1.2678550306667054\n",
      "train loss:1.2013654188385547\n",
      "train loss:1.2856187104980557\n",
      "train loss:1.2620839406418842\n",
      "train loss:1.3104610215217756\n",
      "train loss:1.2510077233548296\n",
      "train loss:1.156519188121156\n",
      "train loss:1.4043305846831289\n",
      "train loss:1.1469897363216655\n",
      "train loss:1.2564777381685877\n",
      "train loss:1.0879450364187582\n",
      "train loss:1.173023637648874\n",
      "train loss:1.232083780968654\n",
      "train loss:1.4124355153301291\n",
      "train loss:1.130392382827528\n",
      "train loss:1.2991583711100345\n",
      "train loss:1.2638210941006474\n",
      "train loss:1.1830149930106653\n",
      "train loss:1.170832378366281\n",
      "train loss:1.4108107619900716\n",
      "train loss:1.1625602168392222\n",
      "train loss:1.1076618843649773\n",
      "train loss:1.2353136092190689\n",
      "train loss:1.2711984854428322\n",
      "train loss:1.3218442478742352\n",
      "train loss:1.192999693334579\n",
      "train loss:0.9791999818393912\n",
      "train loss:1.2108599627568308\n",
      "train loss:1.0962751327174332\n",
      "train loss:1.212514838797362\n",
      "train loss:1.3320942895512176\n",
      "train loss:1.1671409185696804\n",
      "train loss:1.3551363153486022\n",
      "train loss:1.160756367568177\n",
      "train loss:1.2426272858491754\n",
      "train loss:1.0157552317355607\n",
      "train loss:1.1944023443678866\n",
      "train loss:1.1458332774386764\n",
      "train loss:1.135004846371014\n",
      "train loss:1.1873337853330321\n",
      "train loss:1.1012793348412666\n",
      "train loss:1.0554025032360925\n",
      "train loss:1.1808209701996908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.302763119773286\n",
      "train loss:1.1508207596013698\n",
      "train loss:1.345596556347781\n",
      "train loss:1.0332631061281947\n",
      "train loss:1.1373916855679191\n",
      "train loss:1.239179528054233\n",
      "train loss:1.228902501757224\n",
      "train loss:1.2731505978253452\n",
      "train loss:1.336982782390798\n",
      "train loss:1.2335382947573796\n",
      "train loss:1.2537839233563606\n",
      "train loss:1.2846901932059474\n",
      "train loss:1.124870651696935\n",
      "train loss:1.162035801703898\n",
      "train loss:1.0344230444593223\n",
      "train loss:1.0072474768332982\n",
      "train loss:1.197498031654225\n",
      "train loss:1.2294674720934091\n",
      "train loss:1.2294983882493513\n",
      "train loss:0.9165247277061569\n",
      "train loss:1.1928496101252513\n",
      "train loss:1.160279112932101\n",
      "train loss:1.1641690265172502\n",
      "train loss:1.1205497850890365\n",
      "train loss:1.2924082348950756\n",
      "train loss:1.1696463868562115\n",
      "train loss:1.183996059397118\n",
      "train loss:1.3988128558829669\n",
      "train loss:1.2603879102597495\n",
      "train loss:1.2567595347101834\n",
      "train loss:1.090022080436286\n",
      "train loss:1.1593064552597747\n",
      "train loss:1.021249404628274\n",
      "train loss:1.2466609035484837\n",
      "train loss:1.2163823634016895\n",
      "train loss:1.289750632811467\n",
      "train loss:0.9771456679396402\n",
      "train loss:1.1000255762871125\n",
      "train loss:1.1234367945025023\n",
      "train loss:1.1111987187304362\n",
      "train loss:1.0460641571845577\n",
      "train loss:1.4402692007798752\n",
      "train loss:1.1599981532274803\n",
      "train loss:1.3273363030647523\n",
      "train loss:1.3389069236516493\n",
      "train loss:1.134876119892945\n",
      "train loss:1.0890052294938397\n",
      "train loss:1.0355453202926208\n",
      "train loss:1.3256231664768094\n",
      "train loss:1.105953013693317\n",
      "train loss:1.2681238129239012\n",
      "train loss:1.1332455935809498\n",
      "train loss:1.2618443434101096\n",
      "train loss:0.9555489930602178\n",
      "train loss:1.3455015813658324\n",
      "train loss:1.2015363952564275\n",
      "train loss:1.333421728926334\n",
      "train loss:1.138453229196699\n",
      "train loss:1.0951118326151117\n",
      "train loss:1.097543095101955\n",
      "train loss:1.2806817692737686\n",
      "train loss:1.282968722995755\n",
      "train loss:1.3778700614737374\n",
      "train loss:1.1968971955621903\n",
      "train loss:0.9958314365503093\n",
      "train loss:1.1466606976435456\n",
      "train loss:1.1922313345499886\n",
      "train loss:1.241966666771716\n",
      "train loss:1.3787724550181164\n",
      "train loss:1.2210200827636053\n",
      "train loss:1.090036701413797\n",
      "train loss:1.0643035675701693\n",
      "train loss:1.235661629065921\n",
      "train loss:0.9959121820544307\n",
      "train loss:1.1132640237831501\n",
      "train loss:1.3236813176523012\n",
      "train loss:1.0746997688975937\n",
      "train loss:0.9900270082233782\n",
      "train loss:1.008714680865605\n",
      "train loss:1.3071786338166624\n",
      "train loss:1.1685128647031475\n",
      "train loss:1.1545556657853604\n",
      "train loss:1.1032566008017775\n",
      "train loss:0.9844633492442888\n",
      "train loss:1.2344580192816037\n",
      "train loss:1.3466208663548427\n",
      "train loss:1.1524060507589236\n",
      "train loss:1.1291099284037174\n",
      "train loss:1.0992912238173675\n",
      "train loss:0.9776445059556377\n",
      "train loss:1.235403363930817\n",
      "train loss:1.0625850686340426\n",
      "train loss:1.249932812312981\n",
      "train loss:1.175221237285327\n",
      "train loss:1.1977257182480459\n",
      "train loss:1.2691281921308772\n",
      "train loss:1.1449467833871134\n",
      "train loss:1.0998092271796371\n",
      "train loss:1.1112848530385007\n",
      "train loss:0.9764974529609836\n",
      "train loss:1.07811166078015\n",
      "train loss:1.3234390526249344\n",
      "train loss:1.2775205458113639\n",
      "train loss:1.2362996138154694\n",
      "train loss:0.9609912815129269\n",
      "train loss:1.0323233510545609\n",
      "train loss:1.1807798573580124\n",
      "train loss:1.093571986165887\n",
      "train loss:1.0106933966476515\n",
      "train loss:1.2355550192172782\n",
      "train loss:1.2006668506614475\n",
      "train loss:1.1992285135306462\n",
      "train loss:1.1581398575789201\n",
      "train loss:0.9529648942544158\n",
      "train loss:1.1385786493654677\n",
      "train loss:1.0446528612196688\n",
      "train loss:1.2626805664092529\n",
      "train loss:1.2086544560039234\n",
      "train loss:1.0830628824888109\n",
      "train loss:1.0435133184104992\n",
      "train loss:1.0319379434213725\n",
      "train loss:1.0326203264604659\n",
      "train loss:1.226605880905356\n",
      "train loss:1.235486755944359\n",
      "train loss:1.1657287526050657\n",
      "train loss:1.1728727946003206\n",
      "train loss:1.0922001667924206\n",
      "train loss:1.0977307052942151\n",
      "train loss:1.222623629606965\n",
      "train loss:0.900641779818003\n",
      "train loss:1.1926361688987703\n",
      "train loss:1.0921696541522175\n",
      "train loss:1.280098468119715\n",
      "train loss:1.079983329068699\n",
      "train loss:1.1411445595563348\n",
      "train loss:1.0428990247822287\n",
      "train loss:1.1231864178468134\n",
      "train loss:1.0920898872195737\n",
      "train loss:1.0165174929206895\n",
      "train loss:1.0152975924427252\n",
      "train loss:1.2252618830710515\n",
      "train loss:0.9618956499746196\n",
      "train loss:1.0323369324844562\n",
      "train loss:1.3164730742166322\n",
      "train loss:1.2286816696763436\n",
      "train loss:1.225087524913672\n",
      "train loss:1.1967630281390043\n",
      "train loss:1.1634265017339016\n",
      "train loss:1.2622949258252716\n",
      "train loss:1.065506641678627\n",
      "train loss:1.2648300584225425\n",
      "train loss:1.1383759882209998\n",
      "train loss:1.1587548371887566\n",
      "train loss:1.1031129450244896\n",
      "train loss:1.1188287597950135\n",
      "train loss:1.2376983036478013\n",
      "train loss:1.031811169604242\n",
      "train loss:1.0275578075764376\n",
      "train loss:1.245988333207818\n",
      "train loss:1.1448154492807499\n",
      "train loss:1.0327364926194065\n",
      "train loss:1.1390773569848884\n",
      "train loss:1.357336728523121\n",
      "train loss:1.0739075053765212\n",
      "train loss:1.0384876116148056\n",
      "train loss:1.3132639345170702\n",
      "train loss:1.0443812490529676\n",
      "train loss:1.2562922523901372\n",
      "train loss:1.112835450274934\n",
      "train loss:1.1646232098430942\n",
      "train loss:1.1677104182265707\n",
      "train loss:1.1884988329435013\n",
      "train loss:1.1247964882750845\n",
      "train loss:1.0237123616297483\n",
      "train loss:0.9389755193320769\n",
      "train loss:0.870717710097514\n",
      "train loss:1.1511301140302486\n",
      "train loss:1.0406539760436824\n",
      "train loss:1.2189647577226097\n",
      "train loss:1.083105906792036\n",
      "train loss:0.9508570061068172\n",
      "train loss:1.1614880281908613\n",
      "train loss:0.9732404101706975\n",
      "train loss:0.9794205759335854\n",
      "train loss:0.9386409700671359\n",
      "train loss:1.1452710589887125\n",
      "train loss:1.099336948923047\n",
      "train loss:1.1909281206474582\n",
      "train loss:1.1461956453650977\n",
      "train loss:1.1653900434476052\n",
      "train loss:1.0709922582312843\n",
      "train loss:1.1244973549109216\n",
      "train loss:1.1782838963458873\n",
      "train loss:1.0464984312007592\n",
      "train loss:1.2256940679909192\n",
      "train loss:1.025003917853038\n",
      "train loss:0.9799737752435711\n",
      "train loss:1.1025094601812038\n",
      "train loss:1.066592692273685\n",
      "train loss:1.0095878924273776\n",
      "train loss:1.166659437321071\n",
      "train loss:1.2735517048169211\n",
      "train loss:0.9252910758702173\n",
      "train loss:1.0561480715980256\n",
      "train loss:1.126595960557955\n",
      "train loss:1.0147379414274218\n",
      "train loss:1.3160166867211232\n",
      "train loss:1.3889011762325865\n",
      "train loss:1.2306946283486335\n",
      "train loss:0.9427598305414662\n",
      "train loss:1.0518628250483588\n",
      "train loss:0.9627670633494171\n",
      "train loss:1.063419169283943\n",
      "train loss:1.07639401649848\n",
      "train loss:1.2653948484567232\n",
      "train loss:1.0336497098695743\n",
      "train loss:1.164111134220012\n",
      "train loss:1.1887747318965645\n",
      "train loss:0.9374931433802409\n",
      "train loss:1.173903303232745\n",
      "train loss:0.9970515826054666\n",
      "train loss:1.0936876228874473\n",
      "train loss:1.1279297853315395\n",
      "train loss:1.1475053951618195\n",
      "train loss:1.0114141178014362\n",
      "train loss:0.9499818146336975\n",
      "train loss:0.9791909785840263\n",
      "train loss:1.0892283538231933\n",
      "train loss:1.168570931761236\n",
      "train loss:0.9521153656081859\n",
      "train loss:1.002160982933426\n",
      "train loss:1.0786779292592348\n",
      "train loss:1.094267494304474\n",
      "train loss:1.080940410126084\n",
      "train loss:1.115521991591035\n",
      "train loss:1.1094867006631084\n",
      "train loss:1.0350918558522144\n",
      "train loss:1.1532452501491703\n",
      "train loss:1.224183905011107\n",
      "train loss:1.3418842751800062\n",
      "train loss:1.150110944070952\n",
      "train loss:0.994392848804887\n",
      "train loss:0.981157606047434\n",
      "train loss:0.9026409014197223\n",
      "train loss:1.278190704344112\n",
      "train loss:1.1433873364479197\n",
      "train loss:1.063136375210241\n",
      "train loss:1.0387878853675288\n",
      "train loss:1.054340888215075\n",
      "train loss:0.9823846191106171\n",
      "train loss:1.149795532056657\n",
      "train loss:1.1503293035858095\n",
      "train loss:1.0477000138609147\n",
      "train loss:1.0787274431109488\n",
      "train loss:1.0270146214220122\n",
      "train loss:0.9856945839879812\n",
      "train loss:1.1774282451226354\n",
      "train loss:0.9955215712028815\n",
      "train loss:0.9914788673056566\n",
      "train loss:0.9728989411814557\n",
      "train loss:1.0301449309153696\n",
      "train loss:0.9204653743065769\n",
      "train loss:0.9662097797138357\n",
      "train loss:0.9725982288013987\n",
      "train loss:1.103699736692014\n",
      "train loss:1.0981950662853364\n",
      "train loss:1.2808571738857983\n",
      "train loss:1.0384320333128005\n",
      "train loss:1.0463150913771175\n",
      "train loss:1.1295847556257548\n",
      "train loss:0.9231469193686854\n",
      "train loss:1.257125016596241\n",
      "train loss:1.0258496086946416\n",
      "train loss:1.0089916436194322\n",
      "train loss:1.0000219194101156\n",
      "train loss:0.7020973444461416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.1331122348256677\n",
      "train loss:1.145999320511366\n",
      "train loss:1.1276137336720968\n",
      "train loss:0.9185114189672473\n",
      "train loss:1.118778551483663\n",
      "train loss:0.9621527558514886\n",
      "train loss:1.0825808980697287\n",
      "train loss:1.0522795224128902\n",
      "train loss:1.2510963450205865\n",
      "train loss:1.0418174023878553\n",
      "train loss:0.8918750807629826\n",
      "train loss:1.0974733939478831\n",
      "train loss:1.0129822545930531\n",
      "train loss:1.19453875503885\n",
      "train loss:1.0431677328590312\n",
      "train loss:1.0437646582030695\n",
      "train loss:1.1514525704017564\n",
      "train loss:1.0488787599456755\n",
      "train loss:0.9717985312281529\n",
      "train loss:0.9874094195945148\n",
      "train loss:1.0399622291543154\n",
      "train loss:1.0627871180666522\n",
      "train loss:0.9345737462186338\n",
      "train loss:0.9704859431679523\n",
      "train loss:1.4008141725005376\n",
      "train loss:1.0500497134597186\n",
      "train loss:0.9679303596528274\n",
      "train loss:1.002267369493403\n",
      "train loss:1.0850501841631222\n",
      "train loss:1.002597434502102\n",
      "train loss:1.0657060592524648\n",
      "train loss:1.040250151048982\n",
      "train loss:1.134883587376773\n",
      "train loss:1.2254328081072758\n",
      "train loss:1.0695639361653972\n",
      "train loss:1.352278813820207\n",
      "train loss:1.0937984035707558\n",
      "train loss:1.2493093743166053\n",
      "train loss:0.958574983105585\n",
      "train loss:1.1083882048827518\n",
      "train loss:0.9608848629464259\n",
      "train loss:0.949222483908018\n",
      "train loss:0.9353324441840362\n",
      "train loss:1.1649137372005083\n",
      "train loss:1.070435807924082\n",
      "train loss:1.165052852349212\n",
      "train loss:1.1283207497370276\n",
      "train loss:1.0218367668165818\n",
      "train loss:1.1874828088857665\n",
      "train loss:1.0453872989560742\n",
      "train loss:0.9434256487887409\n",
      "=== epoch:2, train acc:0.972, test acc:0.976 ===\n",
      "train loss:1.1752412659823885\n",
      "train loss:1.00811802022067\n",
      "train loss:1.1168644987434384\n",
      "train loss:1.0338029250890306\n",
      "train loss:1.1815636452407487\n",
      "train loss:1.19432830300036\n",
      "train loss:0.9080927717042306\n",
      "train loss:0.8558682670418466\n",
      "train loss:0.8933932052673296\n",
      "train loss:0.8821663200782646\n",
      "train loss:0.9993558864793318\n",
      "train loss:1.112541976797971\n",
      "train loss:1.0823571618924155\n",
      "train loss:0.9297861418125509\n",
      "train loss:1.0052245948347034\n",
      "train loss:0.9168292097487053\n",
      "train loss:1.0660413714312462\n",
      "train loss:0.9719493333620696\n",
      "train loss:1.1133375809010897\n",
      "train loss:1.0437997788041662\n",
      "train loss:1.0360581493591086\n",
      "train loss:1.083250474775213\n",
      "train loss:0.931873297374528\n",
      "train loss:0.9872872033146343\n",
      "train loss:0.9645799888947996\n",
      "train loss:1.011091950273618\n",
      "train loss:1.0021326955358054\n",
      "train loss:1.1466785936900659\n",
      "train loss:0.9355481215862861\n",
      "train loss:0.9599469686597233\n",
      "train loss:1.1422598584659251\n",
      "train loss:1.0771338152349827\n",
      "train loss:1.1110228818356687\n",
      "train loss:1.1530572824471224\n",
      "train loss:0.9292997347058041\n",
      "train loss:0.9520103995932578\n",
      "train loss:1.1748328724922519\n",
      "train loss:1.1617611406225552\n",
      "train loss:0.7792091785005845\n",
      "train loss:0.7349309636052996\n",
      "train loss:1.0403768263052817\n",
      "train loss:1.1314161342572695\n",
      "train loss:1.0063443679126107\n",
      "train loss:0.8719645534095293\n",
      "train loss:1.1390382615203754\n",
      "train loss:1.064654936280606\n",
      "train loss:1.1823388875765384\n",
      "train loss:0.987571645128278\n",
      "train loss:1.0443480976963646\n",
      "train loss:1.308043972598597\n",
      "train loss:1.0578668282609405\n",
      "train loss:1.1128841503329574\n",
      "train loss:0.9953994602555245\n",
      "train loss:1.0819542079626068\n",
      "train loss:0.970383692272937\n",
      "train loss:1.014685684628794\n",
      "train loss:1.0756107303049802\n",
      "train loss:1.0569886645818942\n",
      "train loss:1.0849285100053647\n",
      "train loss:1.068484594354818\n",
      "train loss:1.1314413411376336\n",
      "train loss:1.0766837538175167\n",
      "train loss:1.1700633237645721\n",
      "train loss:1.1857429874899685\n",
      "train loss:1.2059511793844668\n",
      "train loss:0.9582171816471515\n",
      "train loss:1.021903998337973\n",
      "train loss:1.2152561070839223\n",
      "train loss:1.1729755045813512\n",
      "train loss:1.1759860140673595\n",
      "train loss:1.0775329666659312\n",
      "train loss:0.9867793366265429\n",
      "train loss:0.9836811907749595\n",
      "train loss:1.146294839200544\n",
      "train loss:1.1937719857057931\n",
      "train loss:0.9821992181733965\n",
      "train loss:0.9974636476988138\n",
      "train loss:0.9268145619017244\n",
      "train loss:0.9425545410310504\n",
      "train loss:1.1534023842155272\n",
      "train loss:1.0588256629900834\n",
      "train loss:0.9938345431186074\n",
      "train loss:1.0936686300282505\n",
      "train loss:1.108346061246535\n",
      "train loss:1.121617387731888\n",
      "train loss:1.002882701022091\n",
      "train loss:1.1390233866357529\n",
      "train loss:1.065904324565252\n",
      "train loss:1.0434580970841174\n",
      "train loss:0.9042534801777349\n",
      "train loss:1.0880357735178292\n",
      "train loss:1.1196304399187653\n",
      "train loss:1.0698587110126148\n",
      "train loss:1.102935936855899\n",
      "train loss:1.1394317707479003\n",
      "train loss:0.9026719093865058\n",
      "train loss:0.9741816267422668\n",
      "train loss:1.16225469163694\n",
      "train loss:1.003844259110615\n",
      "train loss:0.925524247351365\n",
      "train loss:0.9971841190254109\n",
      "train loss:1.0807332382900525\n",
      "train loss:1.0388432493912758\n",
      "train loss:1.0253744845659796\n",
      "train loss:1.1169549540390593\n",
      "train loss:1.0159032296937913\n",
      "train loss:0.9519402124093979\n",
      "train loss:1.0626864796783746\n",
      "train loss:1.1832511307272935\n",
      "train loss:1.081982795941812\n",
      "train loss:0.919348096338998\n",
      "train loss:0.9945460730454836\n",
      "train loss:0.9894363212656079\n",
      "train loss:1.194386696526338\n",
      "train loss:1.1746380204923317\n",
      "train loss:1.0282693087191885\n",
      "train loss:0.9021445525841021\n",
      "train loss:0.8754894602619554\n",
      "train loss:0.8979635557999189\n",
      "train loss:0.7537015745394987\n",
      "train loss:0.9631930253551585\n",
      "train loss:1.0064336636501892\n",
      "train loss:0.7067383002226528\n",
      "train loss:1.0917500340944244\n",
      "train loss:0.9264820477527153\n",
      "train loss:1.0186620343066886\n",
      "train loss:1.1429428506030126\n",
      "train loss:1.1239493078325953\n",
      "train loss:1.0514009490613565\n",
      "train loss:0.995607514998503\n",
      "train loss:1.0439878311414932\n",
      "train loss:1.0028771639654404\n",
      "train loss:1.003884995728704\n",
      "train loss:1.1635831379900572\n",
      "train loss:1.1151401533981515\n",
      "train loss:1.109749124340853\n",
      "train loss:1.236659004659986\n",
      "train loss:1.1413296654033878\n",
      "train loss:1.07152882508607\n",
      "train loss:0.9753846371113499\n",
      "train loss:1.0985771624816885\n",
      "train loss:1.0374247070322142\n",
      "train loss:0.9993077640164139\n",
      "train loss:0.9441250151502136\n",
      "train loss:0.9658244840464413\n",
      "train loss:1.0444461307293826\n",
      "train loss:1.0134256808919486\n",
      "train loss:1.06008598359485\n",
      "train loss:1.098999159894284\n",
      "train loss:1.0863589125408692\n",
      "train loss:0.8915693565865009\n",
      "train loss:1.2514974190093533\n",
      "train loss:1.0489625141594707\n",
      "train loss:0.8010053215190825\n",
      "train loss:1.0638732141550766\n",
      "train loss:0.8487261821701922\n",
      "train loss:1.0226665834878235\n",
      "train loss:1.1871162140097886\n",
      "train loss:0.985754815974689\n",
      "train loss:1.0762553315385075\n",
      "train loss:0.8780930630356703\n",
      "train loss:1.001304719277288\n",
      "train loss:0.9394045306410377\n",
      "train loss:1.1556752150200902\n",
      "train loss:0.9335870339108936\n",
      "train loss:1.244645907537649\n",
      "train loss:1.0518307043585424\n",
      "train loss:1.1002195975756563\n",
      "train loss:1.0095047966706858\n",
      "train loss:0.9541293848093179\n",
      "train loss:1.1794221220037509\n",
      "train loss:1.0879713458170956\n",
      "train loss:1.0634122181846457\n",
      "train loss:1.142201452260836\n",
      "train loss:0.7797058766346003\n",
      "train loss:1.2895765631485077\n",
      "train loss:0.9374223966426137\n",
      "train loss:0.9762333000841086\n",
      "train loss:1.001608436381594\n",
      "train loss:1.0978670708445235\n",
      "train loss:0.8982218218985853\n",
      "train loss:0.9549534664354422\n",
      "train loss:1.059878727462938\n",
      "train loss:1.1295399042820384\n",
      "train loss:1.2896483958642337\n",
      "train loss:1.1463023152166258\n",
      "train loss:0.9802644077020116\n",
      "train loss:0.9861521268550323\n",
      "train loss:0.837358621850925\n",
      "train loss:0.9244160997454668\n",
      "train loss:1.1090441915681981\n",
      "train loss:1.0860295677437843\n",
      "train loss:1.1574882112863591\n",
      "train loss:0.9392703814733088\n",
      "train loss:0.9989458362291352\n",
      "train loss:0.9991863518877392\n",
      "train loss:1.1139194529284853\n",
      "train loss:1.1081164682213906\n",
      "train loss:1.0927623178021355\n",
      "train loss:0.8648804508724698\n",
      "train loss:1.07158749610823\n",
      "train loss:0.9601352463119084\n",
      "train loss:0.8664178642205107\n",
      "train loss:0.8086148364638477\n",
      "train loss:0.7482614684145329\n",
      "train loss:1.1734311099615298\n",
      "train loss:0.979593787380928\n",
      "train loss:1.0445865340046714\n",
      "train loss:0.8426372827457469\n",
      "train loss:1.0297067201482928\n",
      "train loss:1.095904227733511\n",
      "train loss:1.0186853441898716\n",
      "train loss:1.0700451983556825\n",
      "train loss:1.1313695174883869\n",
      "train loss:1.1147074076337566\n",
      "train loss:1.0802934879970456\n",
      "train loss:1.0249002623061723\n",
      "train loss:0.8272942494006478\n",
      "train loss:0.7910287582689008\n",
      "train loss:1.1784916253061477\n",
      "train loss:1.066704703687133\n",
      "train loss:0.9401399117232785\n",
      "train loss:0.8004320224756186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9520713974488065\n",
      "train loss:0.937041311267818\n",
      "train loss:0.8172362898460235\n",
      "train loss:0.8945618698360875\n",
      "train loss:1.1046689008055044\n",
      "train loss:0.9450138682180961\n",
      "train loss:1.0632091406553115\n",
      "train loss:1.2554044622717773\n",
      "train loss:1.099995109823551\n",
      "train loss:0.9733349764679946\n",
      "train loss:0.9498472541664011\n",
      "train loss:1.0559490488823513\n",
      "train loss:1.013269142669368\n",
      "train loss:1.007368916671979\n",
      "train loss:0.944818933430104\n",
      "train loss:0.9814430759165222\n",
      "train loss:1.100231953559275\n",
      "train loss:0.9781188215098071\n",
      "train loss:0.9279812246410757\n",
      "train loss:0.9948793554880179\n",
      "train loss:0.9959741580616666\n",
      "train loss:1.0206264736597639\n",
      "train loss:0.9220512990006778\n",
      "train loss:0.9654729168333018\n",
      "train loss:1.0075169104472794\n",
      "train loss:0.9368106594571547\n",
      "train loss:0.9097952471747464\n",
      "train loss:1.0778034810959796\n",
      "train loss:1.17420857201914\n",
      "train loss:0.8287791700349824\n",
      "train loss:1.1122129244017205\n",
      "train loss:0.965786505793317\n",
      "train loss:0.9066670372849291\n",
      "train loss:1.0346688921554676\n",
      "train loss:1.1004072920881964\n",
      "train loss:0.950564742011721\n",
      "train loss:0.9160000804056943\n",
      "train loss:1.0213438689492618\n",
      "train loss:0.9650944346433438\n",
      "train loss:1.2129426994287607\n",
      "train loss:1.021379862400634\n",
      "train loss:1.0941472714755256\n",
      "train loss:1.201317831818096\n",
      "train loss:0.7663882594405049\n",
      "train loss:1.0967826672787369\n",
      "train loss:0.9181574186259962\n",
      "train loss:0.8401775041320139\n",
      "train loss:1.0901280435720104\n",
      "train loss:1.0897799918839672\n",
      "train loss:0.7476773012729545\n",
      "train loss:0.9327966369660701\n",
      "train loss:1.0721180389248768\n",
      "train loss:1.1101131517703626\n",
      "train loss:0.9009997556177248\n",
      "train loss:1.2361613225247239\n",
      "train loss:0.9432853422870877\n",
      "train loss:0.8941837606280094\n",
      "train loss:0.9036539191531958\n",
      "train loss:1.0648992837458768\n",
      "train loss:0.8223512191737941\n",
      "train loss:1.038181725383773\n",
      "train loss:0.8332319831901356\n",
      "train loss:1.0071018980517121\n",
      "train loss:0.974261669359825\n",
      "train loss:0.8893571403261036\n",
      "train loss:0.9506252887853285\n",
      "train loss:0.9850451398026339\n",
      "train loss:0.9340278776373446\n",
      "train loss:0.9425454136026794\n",
      "train loss:0.9336334026799298\n",
      "train loss:0.9704969863710518\n",
      "train loss:0.8290746482141245\n",
      "train loss:0.828748156345329\n",
      "train loss:0.8925184253801937\n",
      "train loss:1.0750335628352132\n",
      "train loss:1.1229625189659957\n",
      "train loss:0.959340109404486\n",
      "train loss:1.066031544750462\n",
      "train loss:0.8069879155171814\n",
      "train loss:1.0990105028733024\n",
      "train loss:0.9030033080410016\n",
      "train loss:0.8698530926519795\n",
      "train loss:1.0371051415098014\n",
      "train loss:0.9338133254464277\n",
      "train loss:0.8519523480982071\n",
      "train loss:1.0606335009814654\n",
      "train loss:1.151138667357965\n",
      "train loss:0.9443162498138793\n",
      "train loss:0.8773087126387028\n",
      "train loss:0.9567786206028264\n",
      "train loss:1.120109353087066\n",
      "train loss:1.0309348487626904\n",
      "train loss:0.9628037793061935\n",
      "train loss:0.9817089235608251\n",
      "train loss:0.9157384115493119\n",
      "train loss:1.0628543595137645\n",
      "train loss:1.0612148273427822\n",
      "train loss:1.030097497743992\n",
      "train loss:1.005228944810107\n",
      "train loss:1.0469500482266965\n",
      "train loss:1.249048824108091\n",
      "train loss:0.906911746832037\n",
      "train loss:1.2270231471009918\n",
      "train loss:1.1758827216042627\n",
      "train loss:1.0146782677656667\n",
      "train loss:0.9653089857386273\n",
      "train loss:1.0713887264287343\n",
      "train loss:1.0264573537911637\n",
      "train loss:1.1063219605083818\n",
      "train loss:1.0577006021316644\n",
      "train loss:0.9607284031840763\n",
      "train loss:1.028549989501493\n",
      "train loss:0.9085812032777888\n",
      "train loss:0.8277873334951233\n",
      "train loss:0.9417905019725908\n",
      "train loss:0.9539755087124361\n",
      "train loss:1.0778853306842515\n",
      "train loss:1.0110716123015668\n",
      "train loss:0.9985831697786606\n",
      "train loss:0.8104295909803432\n",
      "train loss:0.9982965489196981\n",
      "train loss:1.0489345568799195\n",
      "train loss:1.258431648777035\n",
      "train loss:1.1938239217178106\n",
      "train loss:1.044526520557704\n",
      "train loss:0.9427023617671871\n",
      "train loss:0.9301957601112048\n",
      "train loss:1.192176698688451\n",
      "train loss:0.9873848074529231\n",
      "train loss:0.9192545480131105\n",
      "train loss:1.1005775024636868\n",
      "train loss:0.8638672558915838\n",
      "train loss:1.1289832187601976\n",
      "train loss:1.0491524635392766\n",
      "train loss:0.9592900867731181\n",
      "train loss:0.939094106555361\n",
      "train loss:1.059487115325426\n",
      "train loss:1.0051851202879714\n",
      "train loss:0.8293679810659502\n",
      "train loss:0.9376988179086045\n",
      "train loss:1.0678772061997446\n",
      "train loss:1.099393685121488\n",
      "train loss:0.8569849658374196\n",
      "train loss:1.073076215148059\n",
      "train loss:0.9869365761368288\n",
      "train loss:1.0407762322195027\n",
      "train loss:1.0362877384760225\n",
      "train loss:1.0641639731465187\n",
      "train loss:0.9914894285816167\n",
      "train loss:0.9461087426763671\n",
      "train loss:0.9579423722341073\n",
      "train loss:0.9614393960296269\n",
      "train loss:0.9581240318992227\n",
      "train loss:0.8657298016253451\n",
      "train loss:1.0417985320151084\n",
      "train loss:1.03627779450486\n",
      "train loss:1.095818534863643\n",
      "train loss:0.9325037711630911\n",
      "train loss:1.0428532084713282\n",
      "train loss:1.055092834670962\n",
      "train loss:0.9341117248406937\n",
      "train loss:1.131159265352962\n",
      "train loss:1.0830718613173258\n",
      "train loss:1.2992663905745518\n",
      "train loss:0.8781874616505978\n",
      "train loss:1.1263459909485676\n",
      "train loss:0.9061322416315535\n",
      "train loss:1.0392513603936484\n",
      "train loss:0.9755510563259139\n",
      "train loss:1.0097860731005956\n",
      "train loss:0.9885754702858055\n",
      "train loss:0.9382776132954429\n",
      "train loss:0.9610085135010816\n",
      "train loss:0.9860483846447987\n",
      "train loss:1.0053219990882232\n",
      "train loss:0.819441491536119\n",
      "train loss:1.0602957780563729\n",
      "train loss:0.7419716301463255\n",
      "train loss:1.0912230553504925\n",
      "train loss:1.0748517252858574\n",
      "train loss:1.009936265276374\n",
      "train loss:1.0597163475156202\n",
      "train loss:1.1152023547121381\n",
      "train loss:1.0688491671893905\n",
      "train loss:0.9375083404165729\n",
      "train loss:1.1143655829770678\n",
      "train loss:1.2472153638120518\n",
      "train loss:0.8893391823051539\n",
      "train loss:1.0277405476499033\n",
      "train loss:1.0577269570373757\n",
      "train loss:0.8657995056286658\n",
      "train loss:1.080043263910111\n",
      "train loss:1.053346324940614\n",
      "train loss:1.0072940671971347\n",
      "train loss:1.1998606220199806\n",
      "train loss:0.9415901579443215\n",
      "train loss:1.1187635823016595\n",
      "train loss:0.9172600543759455\n",
      "train loss:0.9275970964680695\n",
      "train loss:0.8841737483803316\n",
      "train loss:1.0606792321985028\n",
      "train loss:0.8103002360261167\n",
      "train loss:0.9460254592807815\n",
      "train loss:1.1661754560038848\n",
      "train loss:1.0255912702730632\n",
      "train loss:0.8003412207539004\n",
      "train loss:0.9265184935262848\n",
      "train loss:0.9811423226149312\n",
      "train loss:1.0373436589409188\n",
      "train loss:1.0334950514499095\n",
      "train loss:0.997293340000052\n",
      "train loss:0.7672211751501695\n",
      "train loss:1.0760695546959174\n",
      "train loss:1.0832623700389956\n",
      "train loss:0.9971348137117925\n",
      "train loss:0.8078140178633706\n",
      "train loss:1.0665506552931132\n",
      "train loss:1.0267794115243252\n",
      "train loss:1.0517601396206608\n",
      "train loss:0.9562427823247964\n",
      "train loss:1.0522199898915685\n",
      "train loss:1.0344225008381467\n",
      "train loss:0.8018337768308141\n",
      "train loss:0.982175990858256\n",
      "train loss:0.9588614254311656\n",
      "train loss:0.9488824096885478\n",
      "train loss:1.1066441355837906\n",
      "train loss:1.0280819043565277\n",
      "train loss:0.9238332449650655\n",
      "train loss:0.9016584420680536\n",
      "train loss:0.8476842651729986\n",
      "train loss:0.9542366391649358\n",
      "train loss:0.881891838699407\n",
      "train loss:1.0775813021607994\n",
      "train loss:0.858033121678528\n",
      "train loss:0.9567583942292743\n",
      "train loss:1.1106287316389225\n",
      "train loss:1.1365769974760662\n",
      "train loss:0.8548844260932189\n",
      "train loss:1.010568143090175\n",
      "train loss:1.0869423038667965\n",
      "train loss:1.0860062207014722\n",
      "train loss:0.8468360145817247\n",
      "train loss:0.7910326776082808\n",
      "train loss:1.0813299060454034\n",
      "train loss:1.125194942782697\n",
      "train loss:1.1040441961156642\n",
      "train loss:1.06274889190477\n",
      "train loss:0.8344129673146827\n",
      "train loss:1.0704687241682498\n",
      "train loss:1.0137290345272651\n",
      "train loss:0.92843279891896\n",
      "train loss:0.8954193621672476\n",
      "train loss:0.937911593402718\n",
      "train loss:0.9456108005685212\n",
      "train loss:0.9715385454780227\n",
      "train loss:1.0609057150401175\n",
      "train loss:0.8992102165180974\n",
      "train loss:0.8960322162007884\n",
      "train loss:0.9111009952597054\n",
      "train loss:0.8935044754304731\n",
      "train loss:0.9174084971005834\n",
      "train loss:1.0088783549523677\n",
      "train loss:0.9610741185923348\n",
      "train loss:0.8053309125991825\n",
      "train loss:0.9655855508757256\n",
      "train loss:1.0728495188087384\n",
      "train loss:0.9646029786305246\n",
      "train loss:1.0185259866997693\n",
      "train loss:0.9536150251628213\n",
      "train loss:1.022841295012038\n",
      "train loss:0.9769025010165264\n",
      "train loss:0.9006534938942811\n",
      "train loss:0.9352173652260636\n",
      "train loss:1.0134060188042222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8620922182019971\n",
      "train loss:1.0210368297488754\n",
      "train loss:0.9015093036031575\n",
      "train loss:1.0444540900794466\n",
      "train loss:1.1108275379106798\n",
      "train loss:1.0264832352919107\n",
      "train loss:0.9668694719804389\n",
      "train loss:1.1084954393454323\n",
      "train loss:0.9742931365365994\n",
      "train loss:1.0595443697561766\n",
      "train loss:1.0917720162757583\n",
      "train loss:0.791135520813387\n",
      "train loss:1.0528302498244277\n",
      "train loss:0.9960065285323279\n",
      "train loss:1.040386858520864\n",
      "train loss:1.07986466647214\n",
      "train loss:0.9782943253147713\n",
      "train loss:1.0205893440162226\n",
      "train loss:0.8385310625522958\n",
      "train loss:0.9974809412497543\n",
      "train loss:0.9601413145664863\n",
      "train loss:1.0975886322369397\n",
      "train loss:0.9947815217314195\n",
      "train loss:0.901448684285649\n",
      "train loss:1.0941287922480447\n",
      "train loss:0.9710447261488838\n",
      "train loss:0.8793805092823991\n",
      "train loss:1.1746735492448308\n",
      "train loss:1.0680495694497865\n",
      "train loss:0.9126490851689831\n",
      "train loss:0.9149413315846422\n",
      "train loss:1.0788763772555798\n",
      "train loss:0.9630341994260263\n",
      "train loss:1.0096563214740453\n",
      "train loss:0.9988573916833825\n",
      "train loss:1.031891613335098\n",
      "train loss:1.0237560138060398\n",
      "train loss:0.9608568863181922\n",
      "train loss:1.0605582036751888\n",
      "train loss:1.173743523742565\n",
      "train loss:1.3143907772269516\n",
      "train loss:1.0215743819130558\n",
      "train loss:1.102209251939323\n",
      "train loss:1.0380150704783677\n",
      "train loss:0.9244647640905204\n",
      "train loss:0.9105354119522383\n",
      "train loss:0.8667255310275247\n",
      "train loss:1.1612530423647187\n",
      "train loss:1.1453078991751022\n",
      "train loss:1.0020593820500028\n",
      "train loss:1.0431158173865276\n",
      "train loss:1.2822203892381216\n",
      "train loss:0.874332403107847\n",
      "train loss:1.025057753077592\n",
      "train loss:0.8135329343051056\n",
      "train loss:0.9422293030127176\n",
      "train loss:0.9905766072316983\n",
      "train loss:0.9550204941112725\n",
      "train loss:0.8990035539369401\n",
      "train loss:1.0042034846388883\n",
      "train loss:1.0065381990271498\n",
      "train loss:0.9810714044678549\n",
      "train loss:0.7525275017029608\n",
      "train loss:1.0222385127375195\n",
      "train loss:1.1301516016168334\n",
      "train loss:0.9737698759630419\n",
      "train loss:1.0706321915379913\n",
      "train loss:0.9749764742089326\n",
      "train loss:1.0712864414387782\n",
      "train loss:0.9259693836711894\n",
      "train loss:1.0236848801674359\n",
      "train loss:1.208359763138717\n",
      "train loss:1.0277229893890318\n",
      "train loss:0.998230915808052\n",
      "train loss:0.9264119561372022\n",
      "train loss:0.9812811686588929\n",
      "train loss:0.9903951037433881\n",
      "train loss:0.9241795746406684\n",
      "train loss:0.9296553028360576\n",
      "train loss:0.911359272628054\n",
      "train loss:0.8757848346788083\n",
      "train loss:0.9463672665453408\n",
      "train loss:0.9466484150112146\n",
      "train loss:0.81090048872456\n",
      "train loss:0.7776866184832442\n",
      "train loss:0.9522243851614371\n",
      "train loss:1.1142230922981586\n",
      "train loss:1.018621031466473\n",
      "train loss:0.933552157237491\n",
      "train loss:0.9591088471373053\n",
      "train loss:0.9173164517754957\n",
      "train loss:1.0879308244836072\n",
      "train loss:1.129186437632145\n",
      "train loss:1.0705250322393722\n",
      "train loss:1.0299042783891545\n",
      "train loss:0.9334580641204462\n",
      "train loss:0.8334791683266105\n",
      "train loss:0.8259278698852934\n",
      "train loss:1.0057380379358234\n",
      "train loss:0.9065107886761652\n",
      "train loss:1.2095851144867913\n",
      "train loss:0.9705046716105994\n",
      "=== epoch:3, train acc:0.981, test acc:0.984 ===\n",
      "train loss:0.9601008863079455\n",
      "train loss:1.1487116579679233\n",
      "train loss:0.9898042137671387\n",
      "train loss:0.9188374679023985\n",
      "train loss:1.074850409122762\n",
      "train loss:0.8540161451866015\n",
      "train loss:1.0028097451070404\n",
      "train loss:1.0050040121353883\n",
      "train loss:0.7753158324613482\n",
      "train loss:0.8224705447261738\n",
      "train loss:0.9127348363966423\n",
      "train loss:1.0135307699569658\n",
      "train loss:0.9670942296071797\n",
      "train loss:1.025626981664229\n",
      "train loss:0.8976693714242013\n",
      "train loss:1.0468123302257226\n",
      "train loss:0.9795324515422708\n",
      "train loss:1.108738074094172\n",
      "train loss:0.8852052032593279\n",
      "train loss:1.0040880206253613\n",
      "train loss:1.1549607114638853\n",
      "train loss:0.9298381531551475\n",
      "train loss:0.9723590878155413\n",
      "train loss:0.976323494571121\n",
      "train loss:1.042129127983602\n",
      "train loss:0.9949836471425902\n",
      "train loss:0.8139838852306027\n",
      "train loss:1.3265004545535297\n",
      "train loss:1.2506240472457737\n",
      "train loss:0.9556630364821033\n",
      "train loss:0.8888071049372726\n",
      "train loss:1.0641691550915113\n",
      "train loss:0.970445899009154\n",
      "train loss:0.967857967499142\n",
      "train loss:0.9775764176253431\n",
      "train loss:0.7876664935505001\n",
      "train loss:1.0319121799410975\n",
      "train loss:0.9835553847063062\n",
      "train loss:0.9245954226785186\n",
      "train loss:1.0929244708836698\n",
      "train loss:1.092958879482983\n",
      "train loss:0.9197730597024987\n",
      "train loss:1.013696182992372\n",
      "train loss:0.9514590879169588\n",
      "train loss:1.0000348045754004\n",
      "train loss:1.0218798239120357\n",
      "train loss:0.9622166499165793\n",
      "train loss:0.9358007472654293\n",
      "train loss:1.0175933383587092\n",
      "train loss:1.0450789287522737\n",
      "train loss:0.982855148271159\n",
      "train loss:1.0155609756161081\n",
      "train loss:1.0145956784763244\n",
      "train loss:0.9945948378009561\n",
      "train loss:0.9468064209929326\n",
      "train loss:1.0061656577239655\n",
      "train loss:1.0283768042222823\n",
      "train loss:0.9428186082496457\n",
      "train loss:0.9489366014129794\n",
      "train loss:0.9845419844233689\n",
      "train loss:0.9381681441890999\n",
      "train loss:0.7796617413808262\n",
      "train loss:0.8751112562520236\n",
      "train loss:0.9648130024750174\n",
      "train loss:0.9969767435706081\n",
      "train loss:0.8154365655115686\n",
      "train loss:1.0768253843115418\n",
      "train loss:0.9553804807616382\n",
      "train loss:0.96503270030084\n",
      "train loss:0.9491175508613523\n",
      "train loss:0.9647278131282695\n",
      "train loss:1.0685893232050645\n",
      "train loss:0.9154981561667366\n",
      "train loss:1.1429349367530839\n",
      "train loss:1.0250962997238535\n",
      "train loss:1.1763565812153167\n",
      "train loss:0.7919950402975428\n",
      "train loss:0.9832960185060093\n",
      "train loss:1.171764465464416\n",
      "train loss:0.6783034313187712\n",
      "train loss:0.8423992999105805\n",
      "train loss:0.9233214195383507\n",
      "train loss:1.08194357718248\n",
      "train loss:0.9220778672612084\n",
      "train loss:0.961537345757594\n",
      "train loss:0.899255010407106\n",
      "train loss:0.8363768630939293\n",
      "train loss:0.9754924899671815\n",
      "train loss:0.8917885603998283\n",
      "train loss:0.8871258474817151\n",
      "train loss:0.8458128748006757\n",
      "train loss:0.7660089593363345\n",
      "train loss:1.108143688602819\n",
      "train loss:0.8365338751308129\n",
      "train loss:1.071723418249415\n",
      "train loss:0.9620688034726049\n",
      "train loss:1.0509924235618555\n",
      "train loss:1.0678570685958213\n",
      "train loss:1.0028577642371577\n",
      "train loss:0.830852027817258\n",
      "train loss:0.9999091057919776\n",
      "train loss:1.1290197637168589\n",
      "train loss:1.0243482277500464\n",
      "train loss:0.9643282731333594\n",
      "train loss:1.0203337011969935\n",
      "train loss:1.0012324549306524\n",
      "train loss:1.0262596261726764\n",
      "train loss:0.9162775888371235\n",
      "train loss:0.8469368861536906\n",
      "train loss:0.7704400279772315\n",
      "train loss:1.062323320249896\n",
      "train loss:0.807201688884586\n",
      "train loss:0.939221899007939\n",
      "train loss:1.0265839388756752\n",
      "train loss:0.9214194976800405\n",
      "train loss:0.7981928251829001\n",
      "train loss:0.9678074327086479\n",
      "train loss:1.0639076342042428\n",
      "train loss:1.0180440210266606\n",
      "train loss:0.9238165399911261\n",
      "train loss:0.9922601911906184\n",
      "train loss:1.0006611998388357\n",
      "train loss:1.0751330632463527\n",
      "train loss:1.0308523521498945\n",
      "train loss:1.0090865332526628\n",
      "train loss:1.0552689060998524\n",
      "train loss:0.9463666652591857\n",
      "train loss:0.8457567006548501\n",
      "train loss:0.9267588961593367\n",
      "train loss:1.0001053557482817\n",
      "train loss:0.970894812825305\n",
      "train loss:1.0472221468153753\n",
      "train loss:0.9348925224647338\n",
      "train loss:0.8933205841575521\n",
      "train loss:0.9442400722592734\n",
      "train loss:0.9768895721877664\n",
      "train loss:0.8811621485206216\n",
      "train loss:1.2135972880146122\n",
      "train loss:1.1019784418493392\n",
      "train loss:1.0669295589620902\n",
      "train loss:0.9663959722023963\n",
      "train loss:1.000804020593517\n",
      "train loss:1.0026007080313013\n",
      "train loss:0.8532886251848875\n",
      "train loss:0.966283633370724\n",
      "train loss:0.9791075356076334\n",
      "train loss:0.9432896561945475\n",
      "train loss:1.2104988462768338\n",
      "train loss:0.9070856238311481\n",
      "train loss:0.8502928952228896\n",
      "train loss:0.8487833131140258\n",
      "train loss:0.8430884211031925\n",
      "train loss:0.8562189140968406\n",
      "train loss:1.0940623406115082\n",
      "train loss:0.9695200685340869\n",
      "train loss:0.9257294790219235\n",
      "train loss:0.8906707094641788\n",
      "train loss:0.9642635979104631\n",
      "train loss:1.1411812547784066\n",
      "train loss:0.9297004588534261\n",
      "train loss:1.0384054432261827\n",
      "train loss:0.9076961338419964\n",
      "train loss:0.8950642672619948\n",
      "train loss:0.914207597443676\n",
      "train loss:1.0423393375780425\n",
      "train loss:1.0322032218645152\n",
      "train loss:1.0217952555534098\n",
      "train loss:0.9121734232200821\n",
      "train loss:0.8189871538765494\n",
      "train loss:0.9037839205200706\n",
      "train loss:0.8524112729318987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.1636592906178256\n",
      "train loss:1.0557131742612471\n",
      "train loss:0.897740969273208\n",
      "train loss:1.1042560449961316\n",
      "train loss:0.9988717636295881\n",
      "train loss:1.0976424855404323\n",
      "train loss:1.0624117680374932\n",
      "train loss:0.7731592588924164\n",
      "train loss:1.0331769988337975\n",
      "train loss:0.8924742302553876\n",
      "train loss:1.0048896010531323\n",
      "train loss:0.7448670788787187\n",
      "train loss:0.9176506959016709\n",
      "train loss:0.979446805839789\n",
      "train loss:1.0043784560393536\n",
      "train loss:0.9506399069122259\n",
      "train loss:0.8767637454951348\n",
      "train loss:0.9562617675586147\n",
      "train loss:0.958662321915146\n",
      "train loss:1.0870839013201126\n",
      "train loss:0.8841717379417786\n",
      "train loss:0.9873770418072186\n",
      "train loss:1.0112605460988369\n",
      "train loss:1.1028867468385077\n",
      "train loss:0.8812306575778485\n",
      "train loss:0.8599349943578696\n",
      "train loss:0.9056731219027148\n",
      "train loss:0.9822687549742024\n",
      "train loss:0.8197331540709999\n",
      "train loss:1.0953334305156657\n",
      "train loss:1.0884314436539395\n",
      "train loss:0.7787484918729612\n",
      "train loss:1.115324751699149\n",
      "train loss:0.884797787178573\n",
      "train loss:0.9881430794816721\n",
      "train loss:1.004393101865787\n",
      "train loss:0.8819853578400637\n",
      "train loss:0.7496547639479746\n",
      "train loss:1.1440163855911734\n",
      "train loss:1.0392918362085604\n",
      "train loss:0.9036706662952727\n",
      "train loss:0.9427249332753873\n",
      "train loss:1.1022834868371039\n",
      "train loss:1.0690415616097138\n",
      "train loss:0.9005038480563123\n",
      "train loss:0.9806619766645275\n",
      "train loss:0.8724940042877769\n",
      "train loss:1.1447771917695437\n",
      "train loss:0.994974790364675\n",
      "train loss:0.9655901754631562\n",
      "train loss:0.9749807848399759\n",
      "train loss:0.9947691282977877\n",
      "train loss:1.0606068326095148\n",
      "train loss:0.9816018658487745\n",
      "train loss:1.097454573701237\n",
      "train loss:0.9461914763465421\n",
      "train loss:0.9178888333923437\n",
      "train loss:0.9413062541591171\n",
      "train loss:0.9208107038168811\n",
      "train loss:0.8196522901002699\n",
      "train loss:0.9108983248344055\n",
      "train loss:0.9345334594053764\n",
      "train loss:1.0090572606587818\n",
      "train loss:0.9796240924596605\n",
      "train loss:0.8252853807566499\n",
      "train loss:1.0400795520563675\n",
      "train loss:0.9416117945482525\n",
      "train loss:0.9699813462014134\n",
      "train loss:0.9104021106238324\n",
      "train loss:0.9393142576218875\n",
      "train loss:0.8075616110148593\n",
      "train loss:0.9914449543345267\n",
      "train loss:0.963859144881245\n",
      "train loss:0.9010404087237736\n",
      "train loss:0.8813571812370621\n",
      "train loss:0.9309356950416025\n",
      "train loss:0.8278457731901632\n",
      "train loss:0.9622506714212561\n",
      "train loss:0.9580963410244739\n",
      "train loss:0.8477651729116643\n",
      "train loss:0.9670302881774435\n",
      "train loss:0.9958372296959124\n",
      "train loss:1.001873680381158\n",
      "train loss:0.9673841383711703\n",
      "train loss:1.0076783449222924\n",
      "train loss:0.9986209669922316\n",
      "train loss:1.0087939242859312\n",
      "train loss:0.9853852033515262\n",
      "train loss:0.8149981784841144\n",
      "train loss:0.9228487852639364\n",
      "train loss:1.1154440940229609\n",
      "train loss:0.9265670830794331\n",
      "train loss:1.0348874355498279\n",
      "train loss:1.0754145354984042\n",
      "train loss:1.0142168450925098\n",
      "train loss:1.0599422725561372\n",
      "train loss:0.9198854782988836\n",
      "train loss:0.925346471564387\n",
      "train loss:0.9963679910073938\n",
      "train loss:0.8347663070586144\n",
      "train loss:0.9603832933107658\n",
      "train loss:1.032203216892291\n",
      "train loss:1.029293774249754\n",
      "train loss:0.8746465107709747\n",
      "train loss:1.0250004882279824\n",
      "train loss:1.098485674249406\n",
      "train loss:1.0802794611685471\n",
      "train loss:0.9232636452866159\n",
      "train loss:0.9066085677901232\n",
      "train loss:1.1118797568533119\n",
      "train loss:0.9063298767335488\n",
      "train loss:1.0054948137408286\n",
      "train loss:1.0250518771887729\n",
      "train loss:1.0053281417331437\n",
      "train loss:0.8789077442155524\n",
      "train loss:0.7851353590186629\n",
      "train loss:0.9387011926230275\n",
      "train loss:1.1881834833681864\n",
      "train loss:0.9712575706326294\n",
      "train loss:0.8298285050429997\n",
      "train loss:1.0933045569316397\n",
      "train loss:1.052034103427579\n",
      "train loss:0.9874944540163049\n",
      "train loss:1.0324119087028272\n",
      "train loss:0.7328759978108736\n",
      "train loss:0.7410578235510252\n",
      "train loss:1.0002429262015102\n",
      "train loss:1.0192685022845909\n",
      "train loss:0.9243149267316797\n",
      "train loss:0.9400266370836967\n",
      "train loss:0.8382285764218868\n",
      "train loss:0.9811705562944821\n",
      "train loss:0.9402065368731369\n",
      "train loss:0.9535713649942406\n",
      "train loss:0.9340957670873665\n",
      "train loss:1.1257769002464069\n",
      "train loss:0.8462493461349577\n",
      "train loss:0.8870902440215123\n",
      "train loss:0.9921171021554239\n",
      "train loss:1.108986622684597\n",
      "train loss:0.9664988207455827\n",
      "train loss:0.8046597535360589\n",
      "train loss:0.8864206751989491\n",
      "train loss:0.9727257600384909\n",
      "train loss:1.0340733339138568\n",
      "train loss:1.0750076797092456\n",
      "train loss:0.7812465079167081\n",
      "train loss:0.9105692456372555\n",
      "train loss:1.0736905227520004\n",
      "train loss:0.8580311424261651\n",
      "train loss:0.9383879655113363\n",
      "train loss:0.8286727565792072\n",
      "train loss:0.9663826681964958\n",
      "train loss:0.8788340303121304\n",
      "train loss:1.0014483167194181\n",
      "train loss:1.1539294642398028\n",
      "train loss:0.9665906153986938\n",
      "train loss:0.8793806805000005\n",
      "train loss:0.7682119418838531\n",
      "train loss:0.8225366840329351\n",
      "train loss:0.9199741529146838\n",
      "train loss:0.8804757298194407\n",
      "train loss:1.03243700987855\n",
      "train loss:1.111351806033285\n",
      "train loss:0.9266557177732083\n",
      "train loss:0.9384388551673477\n",
      "train loss:0.8657404681199771\n",
      "train loss:0.9299777501455411\n",
      "train loss:0.8862508291170975\n",
      "train loss:0.9067879834841104\n",
      "train loss:0.9207301803651089\n",
      "train loss:0.9325127248138425\n",
      "train loss:1.0064975100140081\n",
      "train loss:1.0176435274387998\n",
      "train loss:0.8949200386694619\n",
      "train loss:0.932465600492921\n",
      "train loss:1.229131898584956\n",
      "train loss:1.014918232474086\n",
      "train loss:0.8896513804913389\n",
      "train loss:1.0620177769008492\n",
      "train loss:0.7542861623645333\n",
      "train loss:1.0361358329830948\n",
      "train loss:1.0261016505727731\n",
      "train loss:1.0870979030417813\n",
      "train loss:0.8474120595019144\n",
      "train loss:1.0263563438540344\n",
      "train loss:1.0007755592604075\n",
      "train loss:1.046732677259326\n",
      "train loss:0.968668549792158\n",
      "train loss:0.9531194864957986\n",
      "train loss:1.062295289371235\n",
      "train loss:1.0136513344999514\n",
      "train loss:0.9106175707472502\n",
      "train loss:0.7835312432466367\n",
      "train loss:0.8611790309862671\n",
      "train loss:1.041611271702038\n",
      "train loss:1.0380693883034207\n",
      "train loss:0.8523738582943038\n",
      "train loss:0.984050366981151\n",
      "train loss:0.8947583345363093\n",
      "train loss:0.9734164439593652\n",
      "train loss:0.8719639864527697\n",
      "train loss:0.9439918879085522\n",
      "train loss:0.9112370556470156\n",
      "train loss:0.8560760593849721\n",
      "train loss:0.8006011791080838\n",
      "train loss:0.8155964705724204\n",
      "train loss:0.9183459579233051\n",
      "train loss:0.9191058068389953\n",
      "train loss:0.9151642893623446\n",
      "train loss:0.9453422810935929\n",
      "train loss:0.77832954802897\n",
      "train loss:0.7786211124104867\n",
      "train loss:1.0387327408920277\n",
      "train loss:0.8312298827866322\n",
      "train loss:0.9905474888792517\n",
      "train loss:0.81932889160691\n",
      "train loss:0.9316080042152365\n",
      "train loss:0.9094563737267805\n",
      "train loss:0.9061668799108767\n",
      "train loss:0.9240129450723709\n",
      "train loss:0.8728710433071634\n",
      "train loss:0.9343697371957653\n",
      "train loss:0.9122996303086043\n",
      "train loss:0.9514249180865499\n",
      "train loss:0.9355856843957785\n",
      "train loss:0.8848835034885785\n",
      "train loss:0.9371422917777115\n",
      "train loss:0.999721628776598\n",
      "train loss:1.0870740397610126\n",
      "train loss:1.103447416043076\n",
      "train loss:0.9176950358935675\n",
      "train loss:1.0112107910530836\n",
      "train loss:0.9220471394669147\n",
      "train loss:0.9299407683205334\n",
      "train loss:0.9849297752015249\n",
      "train loss:0.9407875032926246\n",
      "train loss:1.030057532258348\n",
      "train loss:1.11490862544019\n",
      "train loss:1.001587339527319\n",
      "train loss:0.8834801718426105\n",
      "train loss:0.9364794563680761\n",
      "train loss:0.9964997028318026\n",
      "train loss:0.8963211964262052\n",
      "train loss:0.9688690465000116\n",
      "train loss:0.8608141161992252\n",
      "train loss:0.9286554972834418\n",
      "train loss:0.9078763742000122\n",
      "train loss:0.9626286585508498\n",
      "train loss:0.9497323765264813\n",
      "train loss:0.904269774547135\n",
      "train loss:0.9072600741475033\n",
      "train loss:0.8692301007486651\n",
      "train loss:1.029314439761596\n",
      "train loss:0.8349281969252083\n",
      "train loss:0.7338088335709619\n",
      "train loss:1.0122057867744898\n",
      "train loss:0.8733875502227367\n",
      "train loss:0.7140186020468885\n",
      "train loss:0.8477891983113356\n",
      "train loss:1.0299755308738716\n",
      "train loss:0.9338663926942219\n",
      "train loss:1.0370559855836419\n",
      "train loss:0.9365695302064121\n",
      "train loss:0.8189919161282662\n",
      "train loss:1.0887672872062277\n",
      "train loss:1.0151776296137398\n",
      "train loss:1.1140584813041914\n",
      "train loss:0.9064096856910357\n",
      "train loss:1.0604292546505023\n",
      "train loss:0.9507603488123079\n",
      "train loss:0.9941261429772841\n",
      "train loss:1.014868053877582\n",
      "train loss:0.9860703735653609\n",
      "train loss:0.9896642448266557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.858690437149722\n",
      "train loss:0.8397460272777515\n",
      "train loss:0.9878840866104757\n",
      "train loss:1.0609746870017434\n",
      "train loss:0.8567996049674715\n",
      "train loss:1.0791633889795083\n",
      "train loss:1.1115007002124326\n",
      "train loss:1.0275924327169426\n",
      "train loss:0.9673046895594176\n",
      "train loss:0.8854682928870726\n",
      "train loss:0.8111631892984558\n",
      "train loss:0.756643534470103\n",
      "train loss:0.9359937707922076\n",
      "train loss:1.0717201323677488\n",
      "train loss:1.0552362198374743\n",
      "train loss:1.1582327342083767\n",
      "train loss:0.9711902000078797\n",
      "train loss:0.9586092202414457\n",
      "train loss:0.9341625622244482\n",
      "train loss:0.9631849643379183\n",
      "train loss:1.159511794326627\n",
      "train loss:1.0319990571817113\n",
      "train loss:0.8922628615019563\n",
      "train loss:0.9289069317517854\n",
      "train loss:0.9529812068223983\n",
      "train loss:1.0037405837406026\n",
      "train loss:0.9856295075672031\n",
      "train loss:0.8616498090539951\n",
      "train loss:1.0655426679882474\n",
      "train loss:0.9752000497355658\n",
      "train loss:0.9543005818659489\n",
      "train loss:1.0163928660718908\n",
      "train loss:0.8769397584111387\n",
      "train loss:1.0034842261994874\n",
      "train loss:0.8897727795208925\n",
      "train loss:1.084472923784005\n",
      "train loss:0.8539013350953387\n",
      "train loss:0.7771108423434205\n",
      "train loss:0.8589858506321824\n",
      "train loss:0.9322315664182684\n",
      "train loss:0.9191547394802161\n",
      "train loss:0.8954895037959254\n",
      "train loss:1.1453222523377178\n",
      "train loss:0.7683291463630707\n",
      "train loss:1.000946302686861\n",
      "train loss:1.034778526796807\n",
      "train loss:0.9894602090366622\n",
      "train loss:0.8693444972602998\n",
      "train loss:1.032770771169463\n",
      "train loss:0.9731207659457701\n",
      "train loss:0.9589694135678154\n",
      "train loss:0.8619180644410879\n",
      "train loss:0.898563767616441\n",
      "train loss:1.0000023476705078\n",
      "train loss:1.1264006922103444\n",
      "train loss:0.9944219991695903\n",
      "train loss:0.6695488682430512\n",
      "train loss:0.8576183098599466\n",
      "train loss:0.8501506335965553\n",
      "train loss:1.1492077610129472\n",
      "train loss:0.9493995351029507\n",
      "train loss:1.02439953519398\n",
      "train loss:1.0213515612266264\n",
      "train loss:0.99383875413196\n",
      "train loss:1.0037008770295712\n",
      "train loss:0.8430407658375885\n",
      "train loss:0.7614400401997347\n",
      "train loss:0.9517543808894259\n",
      "train loss:0.8590489051178252\n",
      "train loss:0.9265587851947821\n",
      "train loss:1.0325378144443578\n",
      "train loss:1.00312409651405\n",
      "train loss:1.0904020206794853\n",
      "train loss:0.964526656799141\n",
      "train loss:0.8392282230043666\n",
      "train loss:0.8635967982145408\n",
      "train loss:0.9045285203307533\n",
      "train loss:1.025607589742435\n",
      "train loss:0.9612517978156977\n",
      "train loss:1.0314665265216616\n",
      "train loss:1.0026300994570199\n",
      "train loss:0.9274800442331833\n",
      "train loss:0.930992092737991\n",
      "train loss:0.992300664631522\n",
      "train loss:1.0176774952954875\n",
      "train loss:0.7742438470575316\n",
      "train loss:1.0308273115495703\n",
      "train loss:1.0065704761776226\n",
      "train loss:0.8293812793911282\n",
      "train loss:0.8610372550477173\n",
      "train loss:0.9819615499618383\n",
      "train loss:1.0650914856712124\n",
      "train loss:1.1065418816051789\n",
      "train loss:0.8003136542620563\n",
      "train loss:0.8060656680396047\n",
      "train loss:0.9113948637957289\n",
      "train loss:0.9585150735333707\n",
      "train loss:1.063799602438551\n",
      "train loss:0.8673097459223602\n",
      "train loss:1.1191940418196444\n",
      "train loss:0.9243997082832454\n",
      "train loss:1.1320177232976285\n",
      "train loss:1.0643593070836086\n",
      "train loss:0.9435825529033735\n",
      "train loss:0.8330384272487739\n",
      "train loss:0.97572026096841\n",
      "train loss:0.9379704450700944\n",
      "train loss:0.8378294155053178\n",
      "train loss:1.0483792753671484\n",
      "train loss:0.9395039065053087\n",
      "train loss:0.9405905751198472\n",
      "train loss:0.988378173301015\n",
      "train loss:1.1088247826696922\n",
      "train loss:0.9408790700191758\n",
      "train loss:0.8889983859806406\n",
      "train loss:0.8889848317592315\n",
      "train loss:0.9191014443094536\n",
      "train loss:0.9290672144251123\n",
      "train loss:1.0065076917089244\n",
      "train loss:0.9927994976480339\n",
      "train loss:0.9728820434799378\n",
      "train loss:0.8768499438683064\n",
      "train loss:0.9987694581831397\n",
      "train loss:0.9036255378137815\n",
      "train loss:1.008347798459445\n",
      "train loss:0.9760666751292157\n",
      "train loss:1.0418386964489743\n",
      "train loss:0.8857963788963271\n",
      "train loss:0.8773013016537147\n",
      "train loss:0.9168916833898919\n",
      "train loss:0.9046613870544599\n",
      "train loss:1.0740635087795862\n",
      "train loss:0.9314269314527143\n",
      "train loss:0.9910031569055278\n",
      "train loss:0.8942770555222289\n",
      "train loss:0.7820105636733807\n",
      "train loss:1.0149427618038322\n",
      "train loss:0.9746528914242312\n",
      "train loss:0.9009389778188276\n",
      "train loss:1.026382533329748\n",
      "train loss:0.8456710362951074\n",
      "train loss:1.0949487541108818\n",
      "train loss:1.0048702960108316\n",
      "train loss:0.9482492570437856\n",
      "train loss:0.8672263623284435\n",
      "train loss:0.8326650143592945\n",
      "train loss:0.8251075484330864\n",
      "train loss:0.9266724739487137\n",
      "train loss:0.9336057790571307\n",
      "train loss:0.8881424023983747\n",
      "train loss:1.0133376944237482\n",
      "train loss:1.1296024395522384\n",
      "train loss:1.0221932055805036\n",
      "train loss:1.070651116565657\n",
      "=== epoch:4, train acc:0.978, test acc:0.981 ===\n",
      "train loss:0.9202837598771262\n",
      "train loss:1.1089607351496722\n",
      "train loss:0.8759739144034491\n",
      "train loss:0.7910333057876605\n",
      "train loss:1.0229157697639193\n",
      "train loss:0.9316314229415839\n",
      "train loss:0.8820685326671974\n",
      "train loss:0.7925902480365568\n",
      "train loss:0.8560455248431629\n",
      "train loss:0.8223839553006412\n",
      "train loss:0.8993548515215413\n",
      "train loss:0.950842704506222\n",
      "train loss:1.0635038481817407\n",
      "train loss:0.8865062932317315\n",
      "train loss:0.9611092683972546\n",
      "train loss:0.8880894252743956\n",
      "train loss:1.2177880135076464\n",
      "train loss:1.009898484731004\n",
      "train loss:0.7916735693539594\n",
      "train loss:1.0309955974922635\n",
      "train loss:1.009102797494724\n",
      "train loss:1.0305566865355436\n",
      "train loss:0.8747186296635487\n",
      "train loss:1.0224910073675375\n",
      "train loss:0.91258632065689\n",
      "train loss:0.9236843497876864\n",
      "train loss:1.0240799196789343\n",
      "train loss:0.9725466327276824\n",
      "train loss:0.9558524220875859\n",
      "train loss:1.0005910291488596\n",
      "train loss:0.7249530957268007\n",
      "train loss:0.9840798377144574\n",
      "train loss:0.7571669842517238\n",
      "train loss:0.8667188166793316\n",
      "train loss:0.9355566107006277\n",
      "train loss:0.8589594634287107\n",
      "train loss:0.9541692824120531\n",
      "train loss:1.1031218087809098\n",
      "train loss:0.9117682276731991\n",
      "train loss:0.9892908460078164\n",
      "train loss:0.9690725488484365\n",
      "train loss:0.9591660837723949\n",
      "train loss:0.9161906876075284\n",
      "train loss:0.9875597497314201\n",
      "train loss:0.9208500948394419\n",
      "train loss:1.0020788806501952\n",
      "train loss:1.0427150102977756\n",
      "train loss:0.9033680239384106\n",
      "train loss:0.7708310461758329\n",
      "train loss:0.8868981279076057\n",
      "train loss:1.0153109030754046\n",
      "train loss:0.9729258956422907\n",
      "train loss:1.102257451428723\n",
      "train loss:0.8817891271568756\n",
      "train loss:1.0512731610576491\n",
      "train loss:0.9715678244326454\n",
      "train loss:0.9035875093451675\n",
      "train loss:0.8369076194726981\n",
      "train loss:0.9185959438838264\n",
      "train loss:0.9315916254607356\n",
      "train loss:0.8130355335000654\n",
      "train loss:1.1210601544118324\n",
      "train loss:0.9123752665120703\n",
      "train loss:0.9441679099949744\n",
      "train loss:0.9043416964875529\n",
      "train loss:0.8130491445974102\n",
      "train loss:0.9018939746026452\n",
      "train loss:0.9178481933859856\n",
      "train loss:0.9000112900928476\n",
      "train loss:1.0874493283352331\n",
      "train loss:0.9832192750886856\n",
      "train loss:1.0006844264040042\n",
      "train loss:0.8021647866048665\n",
      "train loss:0.8692379793791577\n",
      "train loss:0.9779270337456617\n",
      "train loss:1.1387692864553265\n",
      "train loss:1.1074889455551693\n",
      "train loss:1.0499202297363748\n",
      "train loss:0.9353888277060203\n",
      "train loss:1.0803105328935394\n",
      "train loss:1.0160568174937625\n",
      "train loss:1.044825151181415\n",
      "train loss:1.022252014659402\n",
      "train loss:1.0173317129103796\n",
      "train loss:0.9371785757983073\n",
      "train loss:0.8284736007602822\n",
      "train loss:0.797241736213594\n",
      "train loss:0.934058795924713\n",
      "train loss:0.908653292295346\n",
      "train loss:0.8105585386861681\n",
      "train loss:0.8860363401090631\n",
      "train loss:0.928074841147342\n",
      "train loss:0.8213949462360925\n",
      "train loss:0.9449562042933105\n",
      "train loss:0.9068137237585124\n",
      "train loss:0.7595715170515582\n",
      "train loss:0.9348886217873705\n",
      "train loss:0.9726280002561332\n",
      "train loss:0.8225414904261819\n",
      "train loss:0.9428623963610268\n",
      "train loss:0.9524780116889544\n",
      "train loss:0.7554515082236241\n",
      "train loss:0.9694487814122198\n",
      "train loss:1.0124806695917155\n",
      "train loss:0.9746327765736162\n",
      "train loss:1.1733134993136218\n",
      "train loss:0.9790402161826828\n",
      "train loss:0.8533408549476037\n",
      "train loss:1.0757655267583497\n",
      "train loss:0.9731747739909078\n",
      "train loss:0.7968039126591999\n",
      "train loss:0.8740799887689025\n",
      "train loss:0.9506043789021514\n",
      "train loss:1.0432184025587388\n",
      "train loss:0.9543916379728594\n",
      "train loss:1.1069721037931366\n",
      "train loss:0.8768844928916332\n",
      "train loss:1.0059800597113224\n",
      "train loss:1.0068126103757944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.886261083990778\n",
      "train loss:0.7547851205431616\n",
      "train loss:1.0265710545319104\n",
      "train loss:0.8477605083416598\n",
      "train loss:0.793601438023676\n",
      "train loss:1.1240570050583367\n",
      "train loss:0.9014209012571008\n",
      "train loss:0.9611696154072065\n",
      "train loss:0.9128742137121527\n",
      "train loss:1.0940664195354908\n",
      "train loss:1.014326886053879\n",
      "train loss:0.9169703520374594\n",
      "train loss:0.9265463157703541\n",
      "train loss:1.0221977660576176\n",
      "train loss:0.8754092746066441\n",
      "train loss:1.0038930363173102\n",
      "train loss:0.7357155427192883\n",
      "train loss:0.9623443434570512\n",
      "train loss:0.8564551061855744\n",
      "train loss:0.7765420243541005\n",
      "train loss:0.8572356888257847\n",
      "train loss:0.9676186336063176\n",
      "train loss:1.0048108816217969\n",
      "train loss:0.8236591861920626\n",
      "train loss:0.9693151640468184\n",
      "train loss:0.9091383128750222\n",
      "train loss:0.9376631983916071\n",
      "train loss:0.9850139282625721\n",
      "train loss:0.9849474398714042\n",
      "train loss:0.9330128510136894\n",
      "train loss:0.9238832127170714\n",
      "train loss:0.9632665826103206\n",
      "train loss:0.9147057337031189\n",
      "train loss:0.8522354981453784\n",
      "train loss:0.7522610064635559\n",
      "train loss:1.0150918637887525\n",
      "train loss:1.1645210228053238\n",
      "train loss:1.0353285801841385\n",
      "train loss:1.0593690226891455\n",
      "train loss:0.7298315523000727\n",
      "train loss:0.865906707776559\n",
      "train loss:0.8366387629613357\n",
      "train loss:0.9471806526902695\n",
      "train loss:0.984515103543197\n",
      "train loss:0.851109898716414\n",
      "train loss:0.8476127641307791\n",
      "train loss:1.0820619527744664\n",
      "train loss:0.8680046004273682\n",
      "train loss:0.8965890789769767\n",
      "train loss:0.9097703441029584\n",
      "train loss:0.8497056017936261\n",
      "train loss:0.9266146931798143\n",
      "train loss:0.8875210746040879\n",
      "train loss:0.9832606346271922\n",
      "train loss:0.7133083853085992\n",
      "train loss:0.980569653175385\n",
      "train loss:0.9541829256575342\n",
      "train loss:0.9466095718249377\n",
      "train loss:0.9838476405655148\n",
      "train loss:0.9921174320695179\n",
      "train loss:1.0560895390849816\n",
      "train loss:1.0202853240220404\n",
      "train loss:0.751572487335985\n",
      "train loss:0.9721481334706122\n",
      "train loss:0.8430939938774248\n",
      "train loss:0.8637466615982707\n",
      "train loss:0.8157779064970349\n",
      "train loss:0.78440626942282\n",
      "train loss:0.8985069616036233\n",
      "train loss:0.8109739041271571\n",
      "train loss:0.8247706352767767\n",
      "train loss:1.0213850940442546\n",
      "train loss:0.9536328487556546\n",
      "train loss:0.8805213598115712\n",
      "train loss:0.8956342801365718\n",
      "train loss:1.0888258035459435\n",
      "train loss:0.9177649751341392\n",
      "train loss:0.9903466605065355\n",
      "train loss:0.8360264931600073\n",
      "train loss:0.8853351747900895\n",
      "train loss:0.9973636447136096\n",
      "train loss:0.853257312379219\n",
      "train loss:1.1228972291325683\n",
      "train loss:1.0238541932071956\n",
      "train loss:0.934596290512829\n",
      "train loss:1.0118828454656243\n",
      "train loss:0.9909298162629955\n",
      "train loss:1.025757690968342\n",
      "train loss:0.8745402184257813\n",
      "train loss:0.9556489609840004\n",
      "train loss:0.8509923513162767\n",
      "train loss:1.0443130573189758\n",
      "train loss:1.0685279493205295\n",
      "train loss:1.010921792885386\n",
      "train loss:0.9179569779136756\n",
      "train loss:0.9256089609140642\n",
      "train loss:0.9025330723124216\n",
      "train loss:0.9303186977705755\n",
      "train loss:0.894324549048971\n",
      "train loss:0.9463769990965588\n",
      "train loss:1.057549209148279\n",
      "train loss:0.9445990430683185\n",
      "train loss:0.8871376082475061\n",
      "train loss:0.8448389592478069\n",
      "train loss:0.9559999599565471\n",
      "train loss:0.9734241209874231\n",
      "train loss:0.8809558939799126\n",
      "train loss:1.0074492099534569\n",
      "train loss:1.0259942748601638\n",
      "train loss:0.837675391853676\n",
      "train loss:1.045071287803691\n",
      "train loss:0.8538524304428919\n",
      "train loss:0.9584995604238172\n",
      "train loss:0.9579486625549377\n",
      "train loss:0.9576071012654431\n",
      "train loss:0.8766311865748182\n",
      "train loss:0.9757500664900368\n",
      "train loss:0.9540519198465712\n",
      "train loss:0.9256758064065118\n",
      "train loss:0.9793811915303056\n",
      "train loss:1.0353669195660098\n",
      "train loss:0.9690489465600517\n",
      "train loss:0.9916498710082731\n",
      "train loss:0.9760924154837924\n",
      "train loss:0.9594989447570241\n",
      "train loss:0.9735150841346428\n",
      "train loss:1.010442448335752\n",
      "train loss:1.0267875364062002\n",
      "train loss:1.008764378415769\n",
      "train loss:0.8603534313623363\n",
      "train loss:0.9183677535965067\n",
      "train loss:0.9059556714317034\n",
      "train loss:1.101167455100071\n",
      "train loss:0.8628924723259352\n",
      "train loss:0.9804694612994023\n",
      "train loss:0.8817763496208886\n",
      "train loss:0.863193222322341\n",
      "train loss:0.9689875985609602\n",
      "train loss:1.037512814880276\n",
      "train loss:0.9118250942933911\n",
      "train loss:0.9317575025578914\n",
      "train loss:0.991246730842546\n",
      "train loss:1.0918480616701995\n",
      "train loss:0.9365534798209616\n",
      "train loss:0.9497026292119528\n",
      "train loss:0.9547834042457508\n",
      "train loss:0.857781917646975\n",
      "train loss:0.8790080564327024\n",
      "train loss:0.9640511179076653\n",
      "train loss:0.859611858157044\n",
      "train loss:0.8009746368210798\n",
      "train loss:0.974400971541714\n",
      "train loss:0.9578818114067253\n",
      "train loss:0.8609533125305336\n",
      "train loss:0.83690823404064\n",
      "train loss:0.9488356436803125\n",
      "train loss:1.0076960474113408\n",
      "train loss:0.8748065301549864\n",
      "train loss:0.8329370936334642\n",
      "train loss:0.9741082403080259\n",
      "train loss:1.0037395616868077\n",
      "train loss:0.8790327949391231\n",
      "train loss:0.8941307523221396\n",
      "train loss:0.8434505642554427\n",
      "train loss:0.7354325188162792\n",
      "train loss:1.0000117557839545\n",
      "train loss:1.0782639904242355\n",
      "train loss:0.8616190353806616\n",
      "train loss:0.9794875697554326\n",
      "train loss:0.7951428947250768\n",
      "train loss:0.856150603975204\n",
      "train loss:0.8347823559915086\n",
      "train loss:0.8491435247421681\n",
      "train loss:0.8040469404995225\n",
      "train loss:0.9115211879455942\n",
      "train loss:0.8113602407198323\n",
      "train loss:1.0601398531418043\n",
      "train loss:0.8383770236077247\n",
      "train loss:0.8861135847900389\n",
      "train loss:0.9235157801926224\n",
      "train loss:1.0229877460080359\n",
      "train loss:1.0120962252258159\n",
      "train loss:0.9150182305537807\n",
      "train loss:0.7774045355335026\n",
      "train loss:1.070782895872641\n",
      "train loss:0.7563480304933443\n",
      "train loss:0.856020438602578\n",
      "train loss:1.0199667259061456\n",
      "train loss:1.0520430049465928\n",
      "train loss:0.9064477416248684\n",
      "train loss:0.9187541317830936\n",
      "train loss:1.0164700708032777\n",
      "train loss:0.9944687213953443\n",
      "train loss:0.8588266085178833\n",
      "train loss:1.0197167899485124\n",
      "train loss:1.0365422221048897\n",
      "train loss:1.0055483987059524\n",
      "train loss:0.8109403335649058\n",
      "train loss:0.883338359337647\n",
      "train loss:0.953119439722072\n",
      "train loss:1.0266337378586972\n",
      "train loss:0.7579414332469926\n",
      "train loss:0.8906295318411446\n",
      "train loss:0.8570065812651226\n",
      "train loss:0.8542322265052608\n",
      "train loss:0.9816708384630016\n",
      "train loss:0.883755537509991\n",
      "train loss:0.8861162231544121\n",
      "train loss:0.9067740548354802\n",
      "train loss:0.7970330929186364\n",
      "train loss:1.2761418719595634\n",
      "train loss:0.9988756905989667\n",
      "train loss:0.9220443621359006\n",
      "train loss:1.0096860602187265\n",
      "train loss:0.8575827243033115\n",
      "train loss:1.0550613392799226\n",
      "train loss:0.9715575308896531\n",
      "train loss:0.8347500970004069\n",
      "train loss:0.7158928553863088\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8f8e22a0c8b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m                   \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_param\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                   evaluate_sample_num_per_epoch=1000)\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# 매개변수 보관\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\deep-learning-from-scratch-master\\common\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\deep-learning-from-scratch-master\\common\\trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mt_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-bb42d9dae0be>\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[0mtmp_layers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtmp_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[0mdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;31m# 결과 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\deep-learning-from-scratch-master\\common\\layers.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dout)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[0mdcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdmax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdmax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdmax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdmax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m         \u001b[0mdx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol2im\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\deep-learning-from-scratch-master\\common\\util.py\u001b[0m in \u001b[0;36mcol2im\u001b[1;34m(col, input_shape, filter_h, filter_w, stride, pad)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0mx_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mout_w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             \u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_max\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx_max\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mW\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append('C:\\deep-learning-from-scratch-master')  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()  \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보관\n",
    "network.save_params(\"deep_convnet_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOCO8z/C0oJNCTxSSE00FH7",
   "name": "Ch8.딥러닝.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
