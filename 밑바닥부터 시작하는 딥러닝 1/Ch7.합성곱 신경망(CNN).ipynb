{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJfS2MqWK6Cm"
   },
   "source": [
    "# 합성곱/풀링 계층 구현하기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tnlj1NALLByS"
   },
   "source": [
    "4차원 배열 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 955,
     "status": "ok",
     "timestamp": 1606052283222,
     "user": {
      "displayName": "‍구병모[ 학부재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "10190861927292343283"
     },
     "user_tz": -540
    },
    "id": "ArkALKSdLD2R",
    "outputId": "381b01c4-487c-4102-b17f-21ce2d2f6043"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.rand(10, 1, 28, 28)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 920,
     "status": "ok",
     "timestamp": 1606052291917,
     "user": {
      "displayName": "‍구병모[ 학부재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "10190861927292343283"
     },
     "user_tz": -540
    },
    "id": "TYwLjfl3LJUP",
    "outputId": "3c98967b-3a4f-456d-a883-85e0a8cc8a4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 942,
     "status": "ok",
     "timestamp": 1606052320699,
     "user": {
      "displayName": "‍구병모[ 학부재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "10190861927292343283"
     },
     "user_tz": -540
    },
    "id": "lVrZQYYTLSGK",
    "outputId": "2c2c9a5b-5d95-4f31-f152-47e1cbe7012f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,0] # x[0][0] --> 첫 번째 데이터의 첫 채널의 공간 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Efu_x6ItLVoU"
   },
   "source": [
    "im2col 함수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_BhlcDOAMl7U"
   },
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    col : 2차원 배열\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqBA-ItvPGnA"
   },
   "source": [
    "col2im -> 합성공 계층의 역전파 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3KR4rSfPKfc"
   },
   "outputs": [],
   "source": [
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    col : 2차원 배열(입력 데이터)\n",
    "    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    img : 변환된 이미지들\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 909,
     "status": "ok",
     "timestamp": 1606052801651,
     "user": {
      "displayName": "‍구병모[ 학부재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "10190861927292343283"
     },
     "user_tz": -540
    },
    "id": "uEHkHnzsMzx7",
    "outputId": "0404802b-e273-4979-febf-52fc4467f746"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape)\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7)\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHHSDUwKNKxY"
   },
   "source": [
    "합성곱 계층 Convolution 클래스 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6g_tS-0Ne9i"
   },
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride  = stride\n",
    "        self.pad = pad\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad) # 입력데이터 전개\n",
    "        col_W = self.W.reshape(FN, -1).T # 필터 전개\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBamTFRDP2Vb"
   },
   "source": [
    "풀링 계층 Pooling 클래스 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jy4eMFBNP5kf"
   },
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        # 전개(1)_입력 데이터\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "\n",
    "        # 최댓값(2)\n",
    "        out = np.max(col, axis=1)\n",
    "\n",
    "        # 성형(3)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7qsVNALR3MU"
   },
   "source": [
    "# CNN 구현하기 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwYv48gXR5Tx"
   },
   "source": [
    "SimpleConvNet 클래스 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IF4FCvc7R9an"
   },
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim = (1, 28, 28),\n",
    "                 conv_param = {'filter_num': 30, 'filter_size' : 5,\n",
    "                               'pad' : 0, 'stride' : 1},\n",
    "                 hidden_size = 100, output_size = 10, weight_init_std = 0.01):\n",
    "        # 합성곱 계측의 하이퍼파라미터\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 학습에 필요한 매개변수\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0],\n",
    "                                                              filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # CNN을 구성하는 계층\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'],\n",
    "                                           self.params['b1'],\n",
    "                                           conv_param['stride'],\n",
    "                                           conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "    \n",
    "    # 추론 수행\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    # 손실 함수의 값\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "    # 오차역전파법\n",
    "    def  gradient(self, x, t):\n",
    "        # 순전파 \n",
    "        self.loss(x, t)\n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1477146,
     "status": "ok",
     "timestamp": 1606059166224,
     "user": {
      "displayName": "‍구병모[ 학부재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "10190861927292343283"
     },
     "user_tz": -540
    },
    "id": "H3uBxmToWZwz",
    "outputId": "f9f8451a-7d98-4fc8-b15d-3752d6f8e085"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
      "train loss:0.0024555903462361597\n",
      "train loss:0.0017607802009268732\n",
      "train loss:0.003517030154892375\n",
      "train loss:0.008561236847548018\n",
      "train loss:0.011034248144149088\n",
      "train loss:0.004165713588344988\n",
      "train loss:0.003226278457158376\n",
      "train loss:0.0033914069331628265\n",
      "train loss:0.013451610784245288\n",
      "train loss:0.004211781180438558\n",
      "train loss:0.005440470490608934\n",
      "train loss:0.0037358257518868153\n",
      "train loss:0.007164149011411241\n",
      "train loss:0.0007834157373368586\n",
      "train loss:0.00503452314797897\n",
      "train loss:0.0007654622628077325\n",
      "train loss:0.0013034816079471841\n",
      "train loss:0.0024024826899650582\n",
      "train loss:0.001984440249710094\n",
      "train loss:0.0019988943586664327\n",
      "train loss:0.002087571450730426\n",
      "train loss:0.0050359692857249345\n",
      "train loss:0.0018707704561551693\n",
      "train loss:0.005294258932475285\n",
      "train loss:0.002971313813934589\n",
      "train loss:0.0016695486282393398\n",
      "train loss:0.003873774045269253\n",
      "train loss:0.0011923542631198242\n",
      "train loss:0.006033383731041988\n",
      "train loss:0.023842600477943367\n",
      "train loss:0.0014036623571762618\n",
      "train loss:0.0017473310156902247\n",
      "train loss:0.00025002352686935553\n",
      "train loss:0.025005482430752\n",
      "train loss:0.0018410160691233043\n",
      "train loss:0.00314030579801638\n",
      "train loss:0.0019616867899524596\n",
      "train loss:0.0011154963005265376\n",
      "train loss:0.008289217534054864\n",
      "train loss:0.002606903086041192\n",
      "train loss:0.01020121827471638\n",
      "train loss:0.009036683874841134\n",
      "train loss:0.0038057545821504903\n",
      "train loss:0.016448125539148706\n",
      "train loss:0.000515343465723215\n",
      "train loss:0.0014969600692961399\n",
      "train loss:0.0030041999502130688\n",
      "train loss:0.004330336487223305\n",
      "train loss:0.006384342818446281\n",
      "train loss:0.00645655360724354\n",
      "train loss:0.019951567472379975\n",
      "train loss:0.003084685662014439\n",
      "train loss:0.00397869704205284\n",
      "train loss:0.005093002754276547\n",
      "train loss:0.02348563871213758\n",
      "train loss:0.005240457192343089\n",
      "train loss:0.002237655746790154\n",
      "train loss:0.0017456098140750211\n",
      "train loss:0.005061798333133135\n",
      "train loss:0.001739937007184334\n",
      "train loss:0.001766120432545121\n",
      "train loss:0.015481453464549853\n",
      "train loss:0.0024455161521846317\n",
      "train loss:0.0021927079574646726\n",
      "train loss:0.004793083230339396\n",
      "train loss:0.006721757842694732\n",
      "train loss:0.007163033877059283\n",
      "train loss:0.013897505590084298\n",
      "train loss:0.0016626418060752268\n",
      "train loss:0.0005712626135801889\n",
      "train loss:0.0016180197904953617\n",
      "train loss:0.00793971289850869\n",
      "train loss:0.002696459917831618\n",
      "train loss:0.013201935556177822\n",
      "train loss:0.0036912487154536866\n",
      "train loss:0.0011715180077302649\n",
      "train loss:0.0032888495930338975\n",
      "train loss:0.0024855204237966954\n",
      "train loss:0.003928239714216342\n",
      "train loss:0.012461848183733255\n",
      "train loss:0.02389934645636757\n",
      "train loss:0.005055853872274588\n",
      "train loss:0.002184978823801088\n",
      "train loss:0.007464083484828783\n",
      "train loss:0.012708024069622159\n",
      "train loss:0.0011124313272297135\n",
      "train loss:0.00655104025663555\n",
      "train loss:0.003407937575279372\n",
      "train loss:0.0078787716117918\n",
      "train loss:0.017874676915176774\n",
      "train loss:0.007836196523012443\n",
      "train loss:0.014818249252786403\n",
      "train loss:0.002711004726428517\n",
      "train loss:0.006064001876503637\n",
      "train loss:0.022173250015580508\n",
      "train loss:0.006657403457210721\n",
      "train loss:0.005203293569759649\n",
      "train loss:0.004573134019637627\n",
      "train loss:0.0005521324872010806\n",
      "train loss:0.0013558462739633906\n",
      "train loss:0.00242993839945387\n",
      "train loss:0.0017742736197650004\n",
      "train loss:0.0007324765591875496\n",
      "train loss:0.00589421378067156\n",
      "train loss:0.0028895528423115283\n",
      "train loss:0.0017282381514434726\n",
      "train loss:0.007125144113554772\n",
      "train loss:0.0035178145201988976\n",
      "train loss:0.0008928735260206504\n",
      "train loss:0.010748557986974024\n",
      "train loss:0.0020136017097272417\n",
      "train loss:0.022236897551529227\n",
      "train loss:0.0003114147547908287\n",
      "train loss:0.0002368681646318277\n",
      "train loss:0.007108761549803328\n",
      "train loss:0.0008929257665006471\n",
      "train loss:0.0030439129291509655\n",
      "train loss:0.0012559637454190623\n",
      "train loss:0.007470303231494112\n",
      "train loss:0.0416065199663468\n",
      "train loss:0.0025774933591409138\n",
      "train loss:0.0027636694096221355\n",
      "train loss:0.023577848909630043\n",
      "train loss:0.004872094803240026\n",
      "train loss:0.0012066172926288067\n",
      "train loss:0.0018701352205881447\n",
      "train loss:0.0022794012697585696\n",
      "train loss:0.020478777111407484\n",
      "train loss:0.0003618822153891028\n",
      "train loss:0.0015949418564158643\n",
      "train loss:0.0024569887695674838\n",
      "train loss:0.010487600754388005\n",
      "train loss:0.0014310573379384443\n",
      "train loss:0.008177160683654799\n",
      "train loss:0.0048440803492762075\n",
      "train loss:0.009486658265568252\n",
      "train loss:0.004676649908667163\n",
      "train loss:0.0007588885283920433\n",
      "train loss:0.0016048899003841355\n",
      "train loss:0.004962170814585815\n",
      "train loss:0.0023814991337133586\n",
      "train loss:0.007540100491433257\n",
      "train loss:0.021925274187196888\n",
      "train loss:0.0019714468681603214\n",
      "train loss:0.0014813093629443804\n",
      "train loss:0.005295170645213459\n",
      "train loss:0.006127407617549521\n",
      "train loss:0.004090235788416678\n",
      "train loss:0.005485255966573209\n",
      "train loss:0.001310999118145906\n",
      "train loss:0.00046555202235740335\n",
      "train loss:0.003179396546591499\n",
      "train loss:0.017480906713213527\n",
      "train loss:0.0035892774841190358\n",
      "train loss:0.004975022470089649\n",
      "train loss:0.0001631695149808475\n",
      "train loss:0.0062724965435005985\n",
      "train loss:0.001048533077086614\n",
      "train loss:0.00022036469592769006\n",
      "train loss:0.003375623412080999\n",
      "train loss:0.0043294034422260335\n",
      "train loss:0.0037351093411465836\n",
      "train loss:0.0010531638234853534\n",
      "train loss:0.0008591013035119585\n",
      "train loss:0.0005815608676805165\n",
      "train loss:0.0005448597161425018\n",
      "train loss:0.003297427290022158\n",
      "train loss:0.001464846352284396\n",
      "train loss:0.0013826790696808125\n",
      "train loss:0.001344846044678014\n",
      "train loss:0.003373220719757461\n",
      "train loss:0.004324039532208716\n",
      "train loss:0.004433236333323955\n",
      "train loss:0.0018728338357352433\n",
      "train loss:0.004388310149990762\n",
      "train loss:0.00040583792086436267\n",
      "train loss:0.009629923196027687\n",
      "train loss:0.0006883769729065563\n",
      "train loss:0.0016867249137918331\n",
      "train loss:0.0012620158374954437\n",
      "train loss:0.0025646184935539974\n",
      "train loss:0.004719544074426258\n",
      "train loss:0.0012151462135948183\n",
      "train loss:0.00206738819947663\n",
      "train loss:0.00016412804226581166\n",
      "train loss:0.0038718169307226874\n",
      "train loss:0.0011758441827891125\n",
      "train loss:0.0030556612872228955\n",
      "train loss:0.002668506969479811\n",
      "train loss:0.005438442275367304\n",
      "=== epoch:13, train acc:1.0, test acc:0.986 ===\n",
      "train loss:0.0055610009316204925\n",
      "train loss:0.0028656555514522952\n",
      "train loss:0.00025597300331773036\n",
      "train loss:0.0033767419749518375\n",
      "train loss:0.0005258270410394381\n",
      "train loss:0.0012293887045605257\n",
      "train loss:0.0019185268214276506\n",
      "train loss:0.001742072824250873\n",
      "train loss:0.0030283638698895216\n",
      "train loss:0.012850653100088965\n",
      "train loss:0.00893898844988792\n",
      "train loss:0.0009249873216764297\n",
      "train loss:0.0023505199723072244\n",
      "train loss:0.00014324360861849245\n",
      "train loss:0.0015881590999734933\n",
      "train loss:0.0009827090001258768\n",
      "train loss:0.0005143696335541981\n",
      "train loss:0.0025524437553836044\n",
      "train loss:0.0020856588431093868\n",
      "train loss:0.0029830215473900324\n",
      "train loss:0.0010660118600617084\n",
      "train loss:0.000818577048582599\n",
      "train loss:0.001590356502452574\n",
      "train loss:0.0011736753692871517\n",
      "train loss:0.00013750305269722864\n",
      "train loss:0.002387660334538775\n",
      "train loss:0.022585442217804567\n",
      "train loss:0.0017535482858540702\n",
      "train loss:0.0015448132986539934\n",
      "train loss:0.003533563991906361\n",
      "train loss:0.006472317397716606\n",
      "train loss:0.0010595821523174196\n",
      "train loss:0.00248524720519292\n",
      "train loss:0.005929273307600994\n",
      "train loss:0.005343121734973684\n",
      "train loss:0.0004974950407464757\n",
      "train loss:0.006713594969824162\n",
      "train loss:0.0023762420243718606\n",
      "train loss:0.007301199704800065\n",
      "train loss:0.033792289702456166\n",
      "train loss:0.0019070376732658939\n",
      "train loss:0.019274255130677254\n",
      "train loss:0.007780771006022845\n",
      "train loss:0.00047298621780377865\n",
      "train loss:0.005388238079138788\n",
      "train loss:0.0035575702684236318\n",
      "train loss:0.003374944826216297\n",
      "train loss:0.0030013368787510635\n",
      "train loss:0.0038762518954589538\n",
      "train loss:0.0005084681300652058\n",
      "train loss:0.0001867883533267469\n",
      "train loss:0.002263354344919087\n",
      "train loss:0.0003385564011424078\n",
      "train loss:0.013254080743754644\n",
      "train loss:0.0022621455186309872\n",
      "train loss:0.003244244646636522\n",
      "train loss:0.0030796612016320405\n",
      "train loss:0.0014588258234128574\n",
      "train loss:0.0014659920160751874\n",
      "train loss:0.009608612893977598\n",
      "train loss:0.004418666602746067\n",
      "train loss:0.003829645524560909\n",
      "train loss:0.0008531570758574065\n",
      "train loss:0.0028449739560058125\n",
      "train loss:0.000987415975712829\n",
      "train loss:0.001526782577709124\n",
      "train loss:0.0008291410184618337\n",
      "train loss:0.001023562024628928\n",
      "train loss:0.026280895913970016\n",
      "train loss:0.0033599544831496154\n",
      "train loss:0.010557998416321912\n",
      "train loss:0.0021883409869618707\n",
      "train loss:0.0033680153622799138\n",
      "train loss:0.0038285381908714816\n",
      "train loss:0.005162105352442739\n",
      "train loss:0.00703787315096128\n",
      "train loss:0.001551540955730895\n",
      "train loss:0.040537653501714566\n",
      "train loss:0.004221638308676646\n",
      "train loss:0.0027281094391153537\n",
      "train loss:0.000529361596017611\n",
      "train loss:0.00519415838796378\n",
      "train loss:0.0059892688347347855\n",
      "train loss:0.005181378912466529\n",
      "train loss:0.003778615271768684\n",
      "train loss:0.0022509218249074426\n",
      "train loss:0.002895468634662979\n",
      "train loss:0.0034509150995035436\n",
      "train loss:0.00379998841030398\n",
      "train loss:0.0003930654502649654\n",
      "train loss:0.0016483403647078476\n",
      "train loss:0.0001917301975302654\n",
      "train loss:0.007599631667530734\n",
      "train loss:0.005760242726884409\n",
      "train loss:0.00478861598868057\n",
      "train loss:0.004256361501887455\n",
      "train loss:0.0045317786917025175\n",
      "train loss:0.03022626743658057\n",
      "train loss:0.0027284058543759256\n",
      "train loss:0.0002299693855812029\n",
      "train loss:0.00812413457239679\n",
      "train loss:0.001342926264431565\n",
      "train loss:0.003522385463141118\n",
      "train loss:0.0007222299009347435\n",
      "train loss:0.0030008773794017356\n",
      "train loss:0.0009578620040431651\n",
      "train loss:0.004757747091250058\n",
      "train loss:0.00404258476577438\n",
      "train loss:0.0003620828826845015\n",
      "train loss:0.0013349235724751973\n",
      "train loss:0.010723930351008715\n",
      "train loss:0.016106466527398593\n",
      "train loss:0.014546322223600576\n",
      "train loss:0.01181980265521219\n",
      "train loss:0.0021738737330694088\n",
      "train loss:0.0003226863071447271\n",
      "train loss:0.003183016461917621\n",
      "train loss:0.0028857505630820444\n",
      "train loss:0.002230067366839607\n",
      "train loss:0.0016715916061442442\n",
      "train loss:0.004580014923637361\n",
      "train loss:0.004211079210944744\n",
      "train loss:0.000758043701139849\n",
      "train loss:0.003688146414645183\n",
      "train loss:0.003469219701566765\n",
      "train loss:0.02006681355835487\n",
      "train loss:0.010789029742834107\n",
      "train loss:0.0008997986595490908\n",
      "train loss:0.0024305954877337443\n",
      "train loss:0.0033917576871187478\n",
      "train loss:0.010408867735114824\n",
      "train loss:0.0004757313569955267\n",
      "train loss:0.0033652203650757413\n",
      "train loss:0.005592203677703093\n",
      "train loss:0.000994086316844522\n",
      "train loss:0.0042518360170059185\n",
      "train loss:0.024110651912548763\n",
      "train loss:0.0008654730620625653\n",
      "train loss:0.0002704477195583038\n",
      "train loss:0.00029567000646626485\n",
      "train loss:0.0021741076288754203\n",
      "train loss:0.00037239957207628455\n",
      "train loss:0.004168415766754227\n",
      "train loss:0.0033534840583143866\n",
      "train loss:0.00040019877506450015\n",
      "train loss:0.0012580990871678994\n",
      "train loss:0.008620099226419497\n",
      "train loss:0.006413566326905012\n",
      "train loss:0.0009584785328322293\n",
      "train loss:0.00560911190501057\n",
      "train loss:0.00548948453764405\n",
      "train loss:0.029144838990861482\n",
      "train loss:0.005058623129371372\n",
      "train loss:0.004801795363605685\n",
      "train loss:0.0016506101027071553\n",
      "train loss:0.001532889843527171\n",
      "train loss:0.011903475275574147\n",
      "train loss:0.0014087872814042778\n",
      "train loss:0.0069777112302967645\n",
      "train loss:0.001746159508173004\n",
      "train loss:0.002767753418641471\n",
      "train loss:0.011451749447495066\n",
      "train loss:0.002171915038503252\n",
      "train loss:0.0022381889767932885\n",
      "train loss:0.007826844746582531\n",
      "train loss:0.01922195717837452\n",
      "train loss:0.004872170606174116\n",
      "train loss:0.010045995483665695\n",
      "train loss:0.0005412902979441132\n",
      "train loss:0.00871543485338192\n",
      "train loss:0.010720509651862825\n",
      "train loss:0.003839007034202335\n",
      "train loss:0.002871595896678969\n",
      "train loss:0.0003921039020655886\n",
      "train loss:0.004373489032316768\n",
      "train loss:0.001782669857141404\n",
      "train loss:0.0015494265282086745\n",
      "train loss:0.0004719444588341541\n",
      "train loss:0.00027076755489381643\n",
      "train loss:0.00043072405780313956\n",
      "train loss:0.0027922657835044457\n",
      "train loss:0.0007569892069452741\n",
      "train loss:0.01047848519130503\n",
      "train loss:0.018831055644174255\n",
      "train loss:0.01574614357015042\n",
      "train loss:0.03439231297221577\n",
      "train loss:0.0036184142205156655\n",
      "train loss:0.00036816688187493447\n",
      "train loss:0.00041954437582125914\n",
      "train loss:0.000737853216788907\n",
      "train loss:0.0004476317318043516\n",
      "train loss:0.0037226084248996576\n",
      "train loss:0.00019087846320857603\n",
      "train loss:0.0010573358590802871\n",
      "train loss:0.0003896202632241378\n",
      "train loss:0.0009759128952708529\n",
      "train loss:0.008443010317571777\n",
      "train loss:0.001426078768681697\n",
      "train loss:0.004642513466264543\n",
      "train loss:0.006437196090106354\n",
      "train loss:0.0004572891108140087\n",
      "train loss:0.04157255992289945\n",
      "train loss:0.005294997838718087\n",
      "train loss:0.0007322201887713684\n",
      "train loss:0.0033325601792540986\n",
      "train loss:0.0035634896433087092\n",
      "train loss:0.0020312338593293216\n",
      "train loss:0.01562847642378874\n",
      "train loss:0.0054362838112499015\n",
      "train loss:0.001712892942810606\n",
      "train loss:0.0018878500203887197\n",
      "train loss:0.0036063596165944417\n",
      "train loss:0.0033138948465584873\n",
      "train loss:0.01047325687579899\n",
      "train loss:0.006876960365080687\n",
      "train loss:0.0026581583733269807\n",
      "train loss:0.0061597991171153665\n",
      "train loss:0.0013674494258362833\n",
      "train loss:0.006101451096132459\n",
      "train loss:0.011603049749864112\n",
      "train loss:0.0024528936375964243\n",
      "train loss:0.0036277883998025087\n",
      "train loss:0.00032041464514324643\n",
      "train loss:0.0008783485726305625\n",
      "train loss:0.0013034835294807477\n",
      "train loss:0.016227979630660288\n",
      "train loss:0.003072951839881772\n",
      "train loss:0.002856652590488726\n",
      "train loss:0.0070719105046977435\n",
      "train loss:0.004442654013499778\n",
      "train loss:0.0044069061657052875\n",
      "train loss:0.00557949820132331\n",
      "train loss:0.028128030646726424\n",
      "train loss:0.000979320318835656\n",
      "train loss:0.011260591773253767\n",
      "train loss:0.01692619588502612\n",
      "train loss:0.004410086863233317\n",
      "train loss:0.01941607591068685\n",
      "train loss:0.005971663697249812\n",
      "train loss:0.0039637921970869\n",
      "train loss:0.05208598229497561\n",
      "train loss:0.05611336479410396\n",
      "train loss:0.0035800367496688535\n",
      "train loss:0.0025173212612218956\n",
      "train loss:0.005841985806158526\n",
      "train loss:0.0013878862663954353\n",
      "train loss:0.0012369551433696888\n",
      "train loss:0.008747013474111201\n",
      "train loss:0.020650852056782975\n",
      "train loss:0.003666341793699201\n",
      "train loss:0.00016041987022598337\n",
      "train loss:0.0018252238865658532\n",
      "train loss:0.0008797384553008165\n",
      "train loss:0.060717702232660396\n",
      "train loss:0.0017086289835166956\n",
      "train loss:0.0011336939523784802\n",
      "train loss:0.001524334352212239\n",
      "train loss:0.000883803755897953\n",
      "train loss:0.007930966176775961\n",
      "train loss:0.01331337075972051\n",
      "train loss:0.0043265733236441785\n",
      "train loss:0.0226703060538903\n",
      "train loss:0.0064717548326381465\n",
      "train loss:0.007234259405666156\n",
      "train loss:0.005204136159657583\n",
      "train loss:0.005646414887066334\n",
      "train loss:0.0015055596760228252\n",
      "train loss:0.00656170075245812\n",
      "train loss:0.0022419262843742915\n",
      "train loss:0.0023746217682770977\n",
      "train loss:0.036156251928666124\n",
      "train loss:0.006815891181102541\n",
      "train loss:0.0008342942311478685\n",
      "train loss:0.007244689890979078\n",
      "train loss:0.00032982623548093686\n",
      "train loss:0.006183692227774063\n",
      "train loss:0.006170204951217843\n",
      "train loss:0.0006925877346698756\n",
      "train loss:0.0005884012776722658\n",
      "train loss:0.0006147894802496918\n",
      "train loss:0.007636017714428083\n",
      "train loss:0.000370328009300026\n",
      "train loss:0.0006651318221262618\n",
      "train loss:0.0009284364139893411\n",
      "train loss:0.001741463556634841\n",
      "train loss:0.005808359742364404\n",
      "train loss:0.0003556524874540136\n",
      "train loss:0.005444261608635722\n",
      "train loss:0.00616781002138117\n",
      "train loss:0.0016442126610772395\n",
      "train loss:0.0009403819322968089\n",
      "train loss:0.0019923979878404955\n",
      "train loss:0.005321097921070731\n",
      "train loss:0.013737753593181681\n",
      "train loss:0.007086941632503681\n",
      "train loss:0.00459930079807275\n",
      "train loss:0.00020906702690215754\n",
      "train loss:0.0030351008939742876\n",
      "train loss:0.001211423883445077\n",
      "train loss:0.00013258347198803806\n",
      "train loss:0.002596040035223178\n",
      "train loss:0.005683992246298241\n",
      "train loss:0.005791406301046047\n",
      "train loss:0.0017006578946565873\n",
      "train loss:0.0030146782639331863\n",
      "train loss:0.0014079488926590636\n",
      "train loss:0.0022923512790464437\n",
      "train loss:0.003842429449482504\n",
      "train loss:0.006301107717403598\n",
      "train loss:0.0008385167591422695\n",
      "train loss:0.0030617709089533936\n",
      "train loss:0.00378012554256731\n",
      "train loss:0.007916332626162507\n",
      "train loss:0.006814585331930667\n",
      "train loss:0.0018161222237542806\n",
      "train loss:0.0035582234563677835\n",
      "train loss:0.007431682166118228\n",
      "train loss:0.010979936908821082\n",
      "train loss:0.01897843393540221\n",
      "train loss:0.0003336868374810289\n",
      "train loss:0.0021240743053829716\n",
      "train loss:0.001315788422468836\n",
      "train loss:0.0007572363593119353\n",
      "train loss:0.00230553285439379\n",
      "train loss:0.015937844428545348\n",
      "train loss:0.0028130252708597693\n",
      "train loss:0.007396882391655654\n",
      "train loss:0.0005941562171633658\n",
      "train loss:0.0013800469255486017\n",
      "train loss:0.002125162103873617\n",
      "train loss:0.0021688737872298065\n",
      "train loss:0.1646880863752268\n",
      "train loss:0.0033791993991869134\n",
      "train loss:0.002289379529937953\n",
      "train loss:0.0006604725179128546\n",
      "train loss:0.00019331142456032084\n",
      "train loss:0.0026657904541639425\n",
      "train loss:0.001971780632506811\n",
      "train loss:0.00027567541816695773\n",
      "train loss:0.003519961181976206\n",
      "train loss:0.0004467218763645493\n",
      "train loss:0.005812091478437851\n",
      "train loss:0.0034617893987950966\n",
      "train loss:0.0004607831997816006\n",
      "train loss:0.0036579493116869104\n",
      "train loss:0.00028596257687016574\n",
      "train loss:0.001927260765366085\n",
      "train loss:0.001592984242422047\n",
      "train loss:0.008496663482001666\n",
      "train loss:0.0007180553999855582\n",
      "train loss:0.0012784314891644284\n",
      "train loss:0.003397724726182782\n",
      "train loss:0.001417458203869522\n",
      "train loss:0.0005665614174837091\n",
      "train loss:0.00220419061985115\n",
      "train loss:0.0009667518434169287\n",
      "train loss:0.0015375154700579038\n",
      "train loss:0.008280970705749664\n",
      "train loss:0.001544619954490113\n",
      "train loss:0.004984736805958612\n",
      "train loss:0.0009237858677466116\n",
      "train loss:0.004872854848814833\n",
      "train loss:0.004022352073631006\n",
      "train loss:0.00047397144452475637\n",
      "train loss:0.0038348295216256763\n",
      "train loss:0.002510190858011861\n",
      "train loss:0.0016362492800854474\n",
      "train loss:0.001866547163699743\n",
      "train loss:0.0023143115272810655\n",
      "train loss:0.002786403761633802\n",
      "train loss:0.0005299840389629666\n",
      "train loss:0.001957188833347261\n",
      "train loss:0.00036454931373223635\n",
      "train loss:0.001341507489145604\n",
      "train loss:0.00226252830311934\n",
      "train loss:0.0034489229668252155\n",
      "train loss:0.0018929837523830643\n",
      "train loss:0.0021828419079744548\n",
      "train loss:0.007227082293716444\n",
      "train loss:0.004795794487563928\n",
      "train loss:0.0011072020043116169\n",
      "train loss:0.0011025989019268344\n",
      "train loss:4.037880404415992e-05\n",
      "train loss:0.00042637612563848633\n",
      "train loss:0.000679584178555074\n",
      "train loss:0.0029099669005608546\n",
      "train loss:0.001121606727042074\n",
      "train loss:0.0035206168387157677\n",
      "train loss:0.0026921389450893984\n",
      "train loss:0.005750086154547907\n",
      "train loss:0.0012957122856803872\n",
      "train loss:0.0018600993284663555\n",
      "train loss:0.003526029911123837\n",
      "train loss:0.0012214597301822459\n",
      "train loss:0.0005278386921942705\n",
      "train loss:0.0013739413773378542\n",
      "train loss:0.0016427461453553394\n",
      "train loss:0.0014045758814586614\n",
      "train loss:0.005227548112502618\n",
      "train loss:0.0020329995339410955\n",
      "train loss:0.004157775035139472\n",
      "train loss:0.004344920610532359\n",
      "train loss:0.007303747139396851\n",
      "train loss:0.013564759456109354\n",
      "train loss:0.00259253273826498\n",
      "train loss:0.0011403881373039664\n",
      "train loss:0.003979783595798213\n",
      "train loss:0.00035059585940528276\n",
      "train loss:0.0032588532914843717\n",
      "train loss:0.0036059932946761212\n",
      "train loss:0.0056697303760724245\n",
      "train loss:0.003013399320509652\n",
      "train loss:0.002568372969961124\n",
      "train loss:0.0034622961096255644\n",
      "train loss:0.0012491106808269815\n",
      "train loss:0.005252099195977763\n",
      "train loss:0.02423236596260541\n",
      "train loss:0.0025111563116832825\n",
      "train loss:0.0026153630615224984\n",
      "train loss:0.0025144079609080803\n",
      "train loss:0.0010673235832192205\n",
      "train loss:0.004700269812375042\n",
      "train loss:0.0021809221048156697\n",
      "train loss:0.02020904497160222\n",
      "train loss:0.0015852371822447486\n",
      "train loss:0.0009409183778951848\n",
      "train loss:0.08386593818492463\n",
      "train loss:0.006536240293626869\n",
      "train loss:0.0010484795526726756\n",
      "train loss:0.002205004416819652\n",
      "train loss:0.0020095754124906835\n",
      "train loss:0.0030041219686695914\n",
      "train loss:0.009823287409008126\n",
      "train loss:0.0010811690468360557\n",
      "train loss:0.0002454428342732549\n",
      "train loss:0.0005919115546935222\n",
      "train loss:0.0027894193933566567\n",
      "train loss:0.0003582884318549374\n",
      "train loss:0.03529859349003638\n",
      "train loss:0.0006274237953853383\n",
      "train loss:0.0038388238662090883\n",
      "train loss:0.0017104963502053718\n",
      "train loss:0.0031264446321516205\n",
      "train loss:0.003185706739231375\n",
      "train loss:0.01243975817717605\n",
      "train loss:0.0017743854442091988\n",
      "train loss:0.00428551857671022\n",
      "train loss:0.00031279491713221807\n",
      "train loss:0.0034006628201963536\n",
      "train loss:0.03705419036103492\n",
      "train loss:0.0026953045354725343\n",
      "train loss:0.0006668765386329195\n",
      "train loss:0.0012893177645203358\n",
      "train loss:0.0019461142315049943\n",
      "train loss:0.06319546767957751\n",
      "train loss:0.0009745021274325595\n",
      "train loss:0.0004512827719880028\n",
      "train loss:0.0015324613892704418\n",
      "train loss:0.014700628161396674\n",
      "train loss:0.003475334899935848\n",
      "train loss:0.002172429622890429\n",
      "train loss:0.0006217567950674259\n",
      "train loss:0.002252518884671985\n",
      "train loss:0.0009049409727584076\n",
      "train loss:0.00043572109863090994\n",
      "train loss:0.0011410915514637426\n",
      "train loss:0.00032163837504524937\n",
      "train loss:0.009753678629961994\n",
      "train loss:0.0003759159257224723\n",
      "train loss:0.0003778912668461028\n",
      "train loss:0.0002480410694413525\n",
      "train loss:0.0005469491981319002\n",
      "train loss:0.0042987700528183456\n",
      "train loss:0.0007099122535254052\n",
      "train loss:0.0213638887060549\n",
      "train loss:0.0051291880290508475\n",
      "train loss:0.0016306091479322546\n",
      "train loss:0.009694323597087658\n",
      "train loss:0.006383162856256137\n",
      "train loss:0.0021409468100947564\n",
      "train loss:0.0005992178568759874\n",
      "train loss:0.0013262701474610673\n",
      "train loss:0.00583348903700075\n",
      "train loss:0.01261930253613075\n",
      "train loss:0.0024257003071285793\n",
      "train loss:0.0050372030970595736\n",
      "train loss:0.011194725945610167\n",
      "train loss:0.002191500782172103\n",
      "train loss:0.0009470068548326547\n",
      "train loss:0.0013953159034921754\n",
      "train loss:0.0009657212455101983\n",
      "train loss:0.0013499512979483087\n",
      "train loss:0.001169123791907983\n",
      "train loss:0.005879631543448558\n",
      "train loss:0.00511201759589061\n",
      "train loss:0.0015018810303592536\n",
      "train loss:0.002199387685229939\n",
      "train loss:0.0008002766383016248\n",
      "train loss:0.006849185074772336\n",
      "train loss:0.0014865087786816058\n",
      "train loss:0.0005816927032176915\n",
      "train loss:0.0004485156451427387\n",
      "train loss:0.0013538566322631295\n",
      "train loss:0.0010051039751890552\n",
      "train loss:0.0011115389889240377\n",
      "train loss:0.0022595807170432943\n",
      "train loss:0.0005985087403686071\n",
      "train loss:0.0011537865773515606\n",
      "train loss:0.0003519952829841661\n",
      "train loss:0.0015762917199331873\n",
      "train loss:0.00042176685194923383\n",
      "train loss:0.00259320960605283\n",
      "train loss:0.0016922263085155673\n",
      "train loss:0.004385642761485497\n",
      "train loss:0.003981777907336592\n",
      "train loss:0.0010778485050761057\n",
      "train loss:0.0015028930225488473\n",
      "train loss:0.0034494342414180014\n",
      "train loss:0.002557920321915801\n",
      "train loss:0.00038368666392171865\n",
      "train loss:0.0035610904815869264\n",
      "train loss:0.0031549309586862417\n",
      "train loss:0.0021211550076566856\n",
      "train loss:0.0033436432830190126\n",
      "train loss:0.004687568799129546\n",
      "train loss:0.004131064598531627\n",
      "train loss:0.0006588596202934744\n",
      "train loss:0.012131640001415152\n",
      "train loss:0.01476708613426396\n",
      "train loss:0.0016040162711894945\n",
      "train loss:0.005665670818054139\n",
      "train loss:0.007721310784045774\n",
      "train loss:0.005527857166969674\n",
      "train loss:0.00045806806744323004\n",
      "train loss:0.003283034308812063\n",
      "train loss:0.006018136523808495\n",
      "train loss:0.004775653642970819\n",
      "train loss:0.0034879756299096582\n",
      "train loss:0.005273769632779583\n",
      "train loss:0.0015210128569129934\n",
      "train loss:0.0058233100903832815\n",
      "train loss:0.0020776716681038124\n",
      "train loss:0.0011232089022374452\n",
      "train loss:0.0015824866825264383\n",
      "train loss:0.0040059798682396605\n",
      "train loss:0.0008109822889893645\n",
      "train loss:0.0032873025899532666\n",
      "train loss:0.007906088760563246\n",
      "train loss:0.0050947320353984605\n",
      "train loss:0.0024434583228977237\n",
      "train loss:0.004179058879055905\n",
      "train loss:0.0003197239913144891\n",
      "train loss:0.0015998760017156921\n",
      "train loss:0.0009871802000456027\n",
      "train loss:0.029146826489964674\n",
      "train loss:0.0022474219047470016\n",
      "train loss:0.004728002429065145\n",
      "train loss:0.004833998927009724\n",
      "train loss:0.0012433157873210571\n",
      "train loss:0.0040478577454475025\n",
      "train loss:0.004618172276029065\n",
      "train loss:0.027016441203004206\n",
      "train loss:0.0027836035543938125\n",
      "train loss:0.0011473400347190477\n",
      "train loss:0.0017181525042079474\n",
      "train loss:0.0006527445082743362\n",
      "train loss:0.0013608811281654134\n",
      "train loss:0.0013362243627152715\n",
      "train loss:0.008995683885723485\n",
      "train loss:0.0011763518023856826\n",
      "train loss:0.0061012385013322415\n",
      "train loss:0.0036220013614491897\n",
      "train loss:0.002290821753811112\n",
      "train loss:0.004570634738616429\n",
      "train loss:0.005737769100444618\n",
      "train loss:0.0033204124920668656\n",
      "train loss:0.0021208005991435086\n",
      "train loss:0.014412686969855202\n",
      "train loss:0.00650312196950556\n",
      "train loss:0.004858705738711734\n",
      "train loss:0.0008086777881092358\n",
      "train loss:0.007711576207666995\n",
      "train loss:0.00031731311614954636\n",
      "train loss:0.0022327193040502843\n",
      "train loss:0.0015578926859426637\n",
      "train loss:0.006260808124055574\n",
      "train loss:0.00401007564788093\n",
      "train loss:0.0015626101686865032\n",
      "train loss:0.010058352794908115\n",
      "train loss:0.0018832383972985061\n",
      "train loss:0.0016774709444515234\n",
      "train loss:0.0007635858195922173\n",
      "train loss:0.0005708791350112872\n",
      "train loss:0.005391498474685755\n",
      "train loss:0.013547750795979918\n",
      "train loss:0.0017465812577400833\n",
      "train loss:0.0010638161211804313\n",
      "train loss:0.0014904904177888971\n",
      "train loss:0.005281539208750821\n",
      "train loss:0.01930865322835986\n",
      "=== epoch:14, train acc:0.998, test acc:0.986 ===\n",
      "train loss:0.003200285245487058\n",
      "train loss:0.0011539803959922522\n",
      "train loss:0.0037751290577939184\n",
      "train loss:0.01751690323248957\n",
      "train loss:0.0027282394092702743\n",
      "train loss:0.002967029866427703\n",
      "train loss:0.004614741552969651\n",
      "train loss:0.0008051795819279265\n",
      "train loss:0.0005138540202034042\n",
      "train loss:0.003620741509241334\n",
      "train loss:0.00175899708289795\n",
      "train loss:0.0201981290402542\n",
      "train loss:0.0017821015415782882\n",
      "train loss:0.005063799985602079\n",
      "train loss:0.0011972802122209316\n",
      "train loss:0.0025294683194362847\n",
      "train loss:0.003034238629666637\n",
      "train loss:0.0004880962710312445\n",
      "train loss:0.00595499307833474\n",
      "train loss:0.0073404432089568365\n",
      "train loss:0.0009722304651088051\n",
      "train loss:0.0016626434310797436\n",
      "train loss:0.0015969898470166869\n",
      "train loss:0.004698378210369244\n",
      "train loss:0.0027108703750872583\n",
      "train loss:0.00387242547834082\n",
      "train loss:0.001984570334680608\n",
      "train loss:0.00010696330625172721\n",
      "train loss:0.055812087683704374\n",
      "train loss:0.0010690082996747702\n",
      "train loss:0.0069342501994880505\n",
      "train loss:0.0008605620470463607\n",
      "train loss:0.0061124466236260715\n",
      "train loss:0.00040374554282713004\n",
      "train loss:0.0022378090269586713\n",
      "train loss:0.005038283297133279\n",
      "train loss:0.0021177169548351913\n",
      "train loss:0.007232593660690773\n",
      "train loss:0.006195621345392805\n",
      "train loss:0.05036943734576183\n",
      "train loss:0.00628010709146507\n",
      "train loss:0.007599132114942342\n",
      "train loss:0.005838140961764462\n",
      "train loss:0.006623394668793716\n",
      "train loss:0.0032833110609610124\n",
      "train loss:0.0025948796458702076\n",
      "train loss:0.0011048625613501958\n",
      "train loss:0.0017020372209405167\n",
      "train loss:0.0024960990452358644\n",
      "train loss:0.000726334239408408\n",
      "train loss:0.0014036756938094453\n",
      "train loss:0.0005974071841668547\n",
      "train loss:0.0009318862386010894\n",
      "train loss:0.00044882918660713457\n",
      "train loss:0.005671413956560528\n",
      "train loss:0.0013187993439893095\n",
      "train loss:0.0030331652695944544\n",
      "train loss:0.022271871375289905\n",
      "train loss:0.0008120063780125722\n",
      "train loss:0.002120881902478101\n",
      "train loss:0.00021871463083455482\n",
      "train loss:0.0025277919195458632\n",
      "train loss:0.00628033626812824\n",
      "train loss:0.00724182630527946\n",
      "train loss:0.001719818031862816\n",
      "train loss:0.0007124679080318624\n",
      "train loss:0.0007415466803848652\n",
      "train loss:0.00146773970346868\n",
      "train loss:0.0003368239897926695\n",
      "train loss:0.0004936095773951912\n",
      "train loss:0.005299593156606119\n",
      "train loss:0.0007829912773104546\n",
      "train loss:0.03067662798181882\n",
      "train loss:0.014521282238690732\n",
      "train loss:0.0005645097841805465\n",
      "train loss:0.004044399763308771\n",
      "train loss:0.0002710945544864029\n",
      "train loss:0.002266425619047352\n",
      "train loss:0.0038050774024453145\n",
      "train loss:0.0013593917182475657\n",
      "train loss:0.0010798936910843438\n",
      "train loss:0.0012723774409338908\n",
      "train loss:0.0033623617436853897\n",
      "train loss:0.0025616856464270787\n",
      "train loss:0.00025448658905451215\n",
      "train loss:0.00851852310008435\n",
      "train loss:0.0032018416316025506\n",
      "train loss:0.006541489784515608\n",
      "train loss:0.004175967134418458\n",
      "train loss:0.0010515361089183757\n",
      "train loss:0.01302446351379298\n",
      "train loss:0.01323351466116353\n",
      "train loss:0.0011164147308379041\n",
      "train loss:0.0014235364259453194\n",
      "train loss:0.0026169698782538974\n",
      "train loss:0.013971939883951748\n",
      "train loss:0.003328312345957547\n",
      "train loss:0.0020893614054503305\n",
      "train loss:0.0002283355268567668\n",
      "train loss:0.008232771250210153\n",
      "train loss:0.0022011659725330954\n",
      "train loss:0.028335314027088596\n",
      "train loss:0.0035260422754300426\n",
      "train loss:0.0014944830368591366\n",
      "train loss:0.0038169359443142904\n",
      "train loss:0.002304645798652053\n",
      "train loss:0.0011148553993428766\n",
      "train loss:0.002222874408133373\n",
      "train loss:0.004582988946277272\n",
      "train loss:0.0004594298285528382\n",
      "train loss:0.008357669357160724\n",
      "train loss:0.001576596676606452\n",
      "train loss:0.0003645535831135058\n",
      "train loss:0.0010466828902263828\n",
      "train loss:0.005027363191647965\n",
      "train loss:0.0009065724414246139\n",
      "train loss:0.0011970758992344975\n",
      "train loss:0.0027532151612214045\n",
      "train loss:0.0004028355847417723\n",
      "train loss:0.0060728284027954235\n",
      "train loss:0.001855014814079472\n",
      "train loss:0.00022909211170039947\n",
      "train loss:0.0005167706461486236\n",
      "train loss:0.003655192671034066\n",
      "train loss:0.004731941782520805\n",
      "train loss:0.0077595380712389365\n",
      "train loss:0.0011817702948800242\n",
      "train loss:0.0025921124202479035\n",
      "train loss:0.0019389924063350804\n",
      "train loss:0.0043451995089838465\n",
      "train loss:0.0007001664970762281\n",
      "train loss:0.005287321975338725\n",
      "train loss:0.005965269223768206\n",
      "train loss:0.0006845288494579455\n",
      "train loss:0.00019815669166469595\n",
      "train loss:0.001166139500377384\n",
      "train loss:0.0016805637138533323\n",
      "train loss:0.00608939862550527\n",
      "train loss:0.0009351267544011197\n",
      "train loss:0.0009259554696267234\n",
      "train loss:0.005481210998100621\n",
      "train loss:0.00372024739636755\n",
      "train loss:0.008118036980251547\n",
      "train loss:0.0004549558951249838\n",
      "train loss:0.0042372033159908135\n",
      "train loss:0.0011462596795303503\n",
      "train loss:0.0023561314747242035\n",
      "train loss:0.0027950524945820152\n",
      "train loss:0.002323104896969176\n",
      "train loss:0.001701632620191154\n",
      "train loss:0.0002841496419097305\n",
      "train loss:0.001782839441053026\n",
      "train loss:0.0018020847030801588\n",
      "train loss:0.0023423425744395357\n",
      "train loss:5.146278378015145e-05\n",
      "train loss:0.0031133314855135226\n",
      "train loss:0.00023150492230837648\n",
      "train loss:0.0017621863529354187\n",
      "train loss:0.009296070163164315\n",
      "train loss:0.0006484143114811215\n",
      "train loss:0.006695474533659356\n",
      "train loss:0.008760595443177576\n",
      "train loss:0.0034465938298003963\n",
      "train loss:0.023442891784566017\n",
      "train loss:0.0008493222319327054\n",
      "train loss:0.004226722362010128\n",
      "train loss:0.04136039566560724\n",
      "train loss:0.012948337463327398\n",
      "train loss:0.0001555587497160664\n",
      "train loss:0.00044960043628117\n",
      "train loss:0.004772781064559708\n",
      "train loss:0.0016049732854475105\n",
      "train loss:0.0030596246843317843\n",
      "train loss:0.0008360340204787166\n",
      "train loss:0.0012680493786095528\n",
      "train loss:0.0011204549476568817\n",
      "train loss:0.004034227455858335\n",
      "train loss:0.008300787854941842\n",
      "train loss:0.0006217147812569619\n",
      "train loss:0.0022214776646093367\n",
      "train loss:0.0005340672932734842\n",
      "train loss:0.002174100128511044\n",
      "train loss:0.000434526535809351\n",
      "train loss:0.00027523978995722456\n",
      "train loss:0.0002326897035937187\n",
      "train loss:0.0047817416132108345\n",
      "train loss:0.013637904043489473\n",
      "train loss:0.0007840809016441934\n",
      "train loss:0.00020527936794757426\n",
      "train loss:0.011055749801595799\n",
      "train loss:0.007081628261339926\n",
      "train loss:0.0009601938980197612\n",
      "train loss:0.0025750562897727558\n",
      "train loss:0.008322402650552755\n",
      "train loss:0.028773575572349438\n",
      "train loss:0.0038867872854657843\n",
      "train loss:0.004863435217443809\n",
      "train loss:0.013882671164911299\n",
      "train loss:0.003952179008128262\n",
      "train loss:0.0012606086086244907\n",
      "train loss:0.005727814588117385\n",
      "train loss:0.006045542275953012\n",
      "train loss:0.00700243230835318\n",
      "train loss:0.014517419876386337\n",
      "train loss:0.0008031436820155885\n",
      "train loss:0.006629644922069654\n",
      "train loss:0.0006516946990122977\n",
      "train loss:0.003456672816357844\n",
      "train loss:0.0005310813141439664\n",
      "train loss:0.0017172723871376225\n",
      "train loss:0.0027310213004182998\n",
      "train loss:0.003594594374408565\n",
      "train loss:0.0009791431134933038\n",
      "train loss:0.004845627436078424\n",
      "train loss:0.0026642359695114425\n",
      "train loss:0.0006490222008552535\n",
      "train loss:0.0011303830726702372\n",
      "train loss:0.0018936950128583092\n",
      "train loss:0.0003199561457660198\n",
      "train loss:0.001596170925875608\n",
      "train loss:0.0018324830045944784\n",
      "train loss:0.002655653500894064\n",
      "train loss:0.0006040251662644564\n",
      "train loss:0.01383026570445205\n",
      "train loss:0.00018407115886354475\n",
      "train loss:0.0021845161291113076\n",
      "train loss:0.003082216675864788\n",
      "train loss:0.0008983743919857494\n",
      "train loss:0.0033666927655473817\n",
      "train loss:0.015780923235754356\n",
      "train loss:0.0006121695626123937\n",
      "train loss:0.003087054557286225\n",
      "train loss:0.0013035670859304874\n",
      "train loss:0.0017704787493718263\n",
      "train loss:0.0021398298372474513\n",
      "train loss:0.003866887799799447\n",
      "train loss:0.0017959416244128163\n",
      "train loss:0.012487993253614639\n",
      "train loss:0.0011921216172477867\n",
      "train loss:0.00166463200989303\n",
      "train loss:0.0006170906876212655\n",
      "train loss:0.001558844665798556\n",
      "train loss:0.0008745124865068259\n",
      "train loss:0.0008791675766900095\n",
      "train loss:0.005487284485763848\n",
      "train loss:0.0031840023005051247\n",
      "train loss:0.00447895769365083\n",
      "train loss:0.007737070237599425\n",
      "train loss:0.013536449589917357\n",
      "train loss:0.005404876587181044\n",
      "train loss:0.002890149699865342\n",
      "train loss:0.0010432476755606861\n",
      "train loss:0.006006478985875765\n",
      "train loss:0.001148204088030778\n",
      "train loss:0.006877835139788349\n",
      "train loss:0.0018719289152767007\n",
      "train loss:7.581380788293027e-05\n",
      "train loss:0.0039372266963240265\n",
      "train loss:0.002066823041452497\n",
      "train loss:0.002217245793125153\n",
      "train loss:0.0029731482585714764\n",
      "train loss:0.0008996369269013896\n",
      "train loss:0.0006885379653269404\n",
      "train loss:0.0014847154564293697\n",
      "train loss:0.006161687049242079\n",
      "train loss:0.003084331181646733\n",
      "train loss:0.0006237411609451362\n",
      "train loss:0.0013971556509901164\n",
      "train loss:0.001229696538035812\n",
      "train loss:0.0040920514462883684\n",
      "train loss:0.0013533564184901373\n",
      "train loss:0.0013350069738166839\n",
      "train loss:0.0008362772067590751\n",
      "train loss:0.0663382766199847\n",
      "train loss:0.004209935524899636\n",
      "train loss:0.0002923207363903796\n",
      "train loss:0.0017778267096775944\n",
      "train loss:0.007769796029218652\n",
      "train loss:0.011189593716048002\n",
      "train loss:0.0015955119217567423\n",
      "train loss:0.0032139520627839607\n",
      "train loss:0.004348772641923553\n",
      "train loss:0.0012851506312888878\n",
      "train loss:0.0008457990080911928\n",
      "train loss:0.0009224913448087482\n",
      "train loss:0.0009421084251059014\n",
      "train loss:0.0008291439611519259\n",
      "train loss:0.002972504756259687\n",
      "train loss:0.0012323396620978618\n",
      "train loss:0.0026771569239230773\n",
      "train loss:0.0016098071869020143\n",
      "train loss:0.0005556273830413567\n",
      "train loss:0.002434720462758499\n",
      "train loss:0.005369405525097932\n",
      "train loss:0.0023619777522635023\n",
      "train loss:0.0006415728519378257\n",
      "train loss:0.00034050965344264647\n",
      "train loss:0.03380795686182008\n",
      "train loss:0.00029762401054067496\n",
      "train loss:0.005974258735879285\n",
      "train loss:0.0003623825530187324\n",
      "train loss:0.0005316056056361851\n",
      "train loss:0.0007864008807458565\n",
      "train loss:0.005181911188194633\n",
      "train loss:0.0049393091856892194\n",
      "train loss:0.002604718931023215\n",
      "train loss:0.00018832123933100944\n",
      "train loss:0.003973014117105446\n",
      "train loss:0.002139453190466621\n",
      "train loss:0.004335066311817748\n",
      "train loss:0.0015669460711665285\n",
      "train loss:0.0019051631691982174\n",
      "train loss:0.0017333783264492375\n",
      "train loss:0.0009573443321849257\n",
      "train loss:0.0010149761967195641\n",
      "train loss:0.0015286521174511867\n",
      "train loss:0.0005845128247896413\n",
      "train loss:0.0006818916156667868\n",
      "train loss:0.04209105756042921\n",
      "train loss:0.004952935473653262\n",
      "train loss:0.0006029160035488715\n",
      "train loss:0.00027976752095386077\n",
      "train loss:0.00180054395167434\n",
      "train loss:0.0004361170998152286\n",
      "train loss:0.0020463952402324103\n",
      "train loss:0.0035519263467234784\n",
      "train loss:0.0002249042231827915\n",
      "train loss:0.0015192585037388903\n",
      "train loss:0.0017006115636340857\n",
      "train loss:0.0005953539911175065\n",
      "train loss:0.0020392315018550426\n",
      "train loss:0.0005644308293355859\n",
      "train loss:0.0010175786529327904\n",
      "train loss:0.0018571413846446767\n",
      "train loss:0.008871236926650537\n",
      "train loss:0.00015912948456068678\n",
      "train loss:0.0012451520201504863\n",
      "train loss:0.005970283832437902\n",
      "train loss:0.0006183812307899328\n",
      "train loss:0.0005158914299800295\n",
      "train loss:0.0004431561903491656\n",
      "train loss:0.00017003396714573775\n",
      "train loss:0.004220369113802273\n",
      "train loss:0.0008737069869265411\n",
      "train loss:0.0016884600260986262\n",
      "train loss:0.001544605229105226\n",
      "train loss:0.002850228757507196\n",
      "train loss:0.0005913028081730666\n",
      "train loss:0.0006246500502635846\n",
      "train loss:0.0041183244052839935\n",
      "train loss:0.0002587574969788092\n",
      "train loss:0.0018360272644728321\n",
      "train loss:0.001316142791527015\n",
      "train loss:0.0028597449805186997\n",
      "train loss:0.0015978060117387176\n",
      "train loss:0.0006016675264100845\n",
      "train loss:0.002112682269054186\n",
      "train loss:0.0003381128532630076\n",
      "train loss:0.02152090243217263\n",
      "train loss:0.002017570922982263\n",
      "train loss:0.00011905069187733616\n",
      "train loss:0.0018613092463181519\n",
      "train loss:0.0005422588229738888\n",
      "train loss:0.00047466758724105907\n",
      "train loss:0.00016099853756574082\n",
      "train loss:0.0008569073951651857\n",
      "train loss:0.0017841931408074088\n",
      "train loss:0.00532832095471317\n",
      "train loss:0.0008229133150096452\n",
      "train loss:0.00012026881527171725\n",
      "train loss:0.0012179467392028952\n",
      "train loss:0.005011429500229079\n",
      "train loss:0.005395569540081365\n",
      "train loss:0.003911470798474144\n",
      "train loss:0.001305868998984608\n",
      "train loss:0.0014741256718168471\n",
      "train loss:0.0015547253118045393\n",
      "train loss:0.0003566594082412898\n",
      "train loss:0.037900345132675116\n",
      "train loss:0.013863814847888335\n",
      "train loss:0.00010130838072518071\n",
      "train loss:0.0015100844335090978\n",
      "train loss:0.0051209415749423805\n",
      "train loss:0.0015892516626346892\n",
      "train loss:0.00028060808158188036\n",
      "train loss:0.000528125086301876\n",
      "train loss:0.004457755602866329\n",
      "train loss:0.004395954229409192\n",
      "train loss:0.020467770898993917\n",
      "train loss:0.0036711461775528124\n",
      "train loss:0.0013667719831112787\n",
      "train loss:0.0002677606970445376\n",
      "train loss:0.008081808378615617\n",
      "train loss:0.0039521050313820975\n",
      "train loss:0.00031444139171716776\n",
      "train loss:0.0026205602922334216\n",
      "train loss:0.042262395466776057\n",
      "train loss:0.004531859002049928\n",
      "train loss:0.0017646977531570387\n",
      "train loss:0.0012299256579682309\n",
      "train loss:0.011914653282632167\n",
      "train loss:0.003421820017695543\n",
      "train loss:0.009524953039686613\n",
      "train loss:0.002670224328361176\n",
      "train loss:0.002567306382030797\n",
      "train loss:0.0004274732381798369\n",
      "train loss:0.0011677116925433283\n",
      "train loss:0.000711512855961639\n",
      "train loss:0.004876239850611099\n",
      "train loss:0.004819123420632138\n",
      "train loss:0.0011794800401050765\n",
      "train loss:0.000904595547182416\n",
      "train loss:0.004795349724077613\n",
      "train loss:0.000892395757694407\n",
      "train loss:0.004717861734067131\n",
      "train loss:0.0009185518650845649\n",
      "train loss:0.0011575545172631053\n",
      "train loss:0.0028401605197751535\n",
      "train loss:0.0019794099166834636\n",
      "train loss:0.002295108227083476\n",
      "train loss:0.003099186687377708\n",
      "train loss:0.0030562917807670587\n",
      "train loss:0.00215658739941026\n",
      "train loss:0.0006847057247345758\n",
      "train loss:0.00035431741892867554\n",
      "train loss:0.0034417724848766124\n",
      "train loss:0.001206962154979558\n",
      "train loss:0.0036326089242597337\n",
      "train loss:0.0008560455059373924\n",
      "train loss:0.0009879667504936207\n",
      "train loss:0.003345970193640704\n",
      "train loss:0.00042876439533697875\n",
      "train loss:0.0012212473100199522\n",
      "train loss:0.0009334653261870497\n",
      "train loss:0.0005937103888302177\n",
      "train loss:0.005975487886726686\n",
      "train loss:0.0031998600787903862\n",
      "train loss:0.0012672257827960533\n",
      "train loss:0.0016093592346738685\n",
      "train loss:0.00021863646281337784\n",
      "train loss:0.0009540509858283977\n",
      "train loss:0.00010850743388077249\n",
      "train loss:0.00045498909900305986\n",
      "train loss:0.0006205182853204428\n",
      "train loss:0.00019311614305461266\n",
      "train loss:0.01625795797098897\n",
      "train loss:0.0004897528805457172\n",
      "train loss:0.0016493312612994423\n",
      "train loss:0.0006106564978802133\n",
      "train loss:0.0005955024840848484\n",
      "train loss:0.002994257093425734\n",
      "train loss:0.001526618987347346\n",
      "train loss:0.0002256941610511803\n",
      "train loss:4.904090839455276e-05\n",
      "train loss:0.001034078128196915\n",
      "train loss:0.000160211183469264\n",
      "train loss:0.003250879801838048\n",
      "train loss:0.0004798084779076376\n",
      "train loss:0.0018047773891703034\n",
      "train loss:0.00025650746512272593\n",
      "train loss:0.0024053771269112708\n",
      "train loss:0.0003818060888213269\n",
      "train loss:0.0014157698090887655\n",
      "train loss:0.0017809723056585812\n",
      "train loss:0.00046985633134558004\n",
      "train loss:0.0002624800071908766\n",
      "train loss:0.0007673637970123952\n",
      "train loss:0.00022021385768397977\n",
      "train loss:0.0005607979103930699\n",
      "train loss:0.0016039094776816724\n",
      "train loss:0.0021569131190022125\n",
      "train loss:0.0028299996956528558\n",
      "train loss:0.0012648520092567708\n",
      "train loss:0.001286671514395646\n",
      "train loss:0.0002930454465997444\n",
      "train loss:0.01476834809206658\n",
      "train loss:0.0005672450471237471\n",
      "train loss:7.128811154804246e-05\n",
      "train loss:0.0017596314130127765\n",
      "train loss:0.0031703384183876305\n",
      "train loss:0.0009217967229620988\n",
      "train loss:0.0006618520416184239\n",
      "train loss:0.004144695812081722\n",
      "train loss:0.002218270838127675\n",
      "train loss:0.002923382421718386\n",
      "train loss:0.00037708999343206555\n",
      "train loss:0.00084472670172233\n",
      "train loss:0.00045916858619558825\n",
      "train loss:0.0016456742683175788\n",
      "train loss:0.002693752607297575\n",
      "train loss:0.001612675514803063\n",
      "train loss:0.0038125427360227253\n",
      "train loss:0.00059778063135172\n",
      "train loss:0.0002897745060639952\n",
      "train loss:0.0006991288950357216\n",
      "train loss:0.004724062711795966\n",
      "train loss:0.0011752881173180255\n",
      "train loss:0.0009227919023273857\n",
      "train loss:0.0003782584235330589\n",
      "train loss:0.00037147060340640193\n",
      "train loss:0.004068365861918505\n",
      "train loss:0.0007870550645236427\n",
      "train loss:0.0004145038826136945\n",
      "train loss:0.0005835429405118861\n",
      "train loss:0.00048471367026902177\n",
      "train loss:0.02624363012618885\n",
      "train loss:0.0005278269042844668\n",
      "train loss:0.0021975038992089117\n",
      "train loss:0.017653541262773947\n",
      "train loss:0.008558100852453288\n",
      "train loss:0.0016544863390677209\n",
      "train loss:0.002139436634301479\n",
      "train loss:0.001519013805209373\n",
      "train loss:0.0005040375661239674\n",
      "train loss:0.0028956494688244312\n",
      "train loss:0.0016655937586462736\n",
      "train loss:0.0015388085691081558\n",
      "train loss:0.00028801238360853196\n",
      "train loss:0.0022156673426856595\n",
      "train loss:0.0010790023519339535\n",
      "train loss:0.0009329280491988916\n",
      "train loss:0.00018459610980002078\n",
      "train loss:0.00772260002194987\n",
      "train loss:0.0005727149563011947\n",
      "train loss:0.0069476692288912645\n",
      "train loss:0.0009099805007087273\n",
      "train loss:0.0027056002204746132\n",
      "train loss:0.0030090903190606185\n",
      "train loss:0.00017790301796968272\n",
      "train loss:0.0011252730742074605\n",
      "train loss:0.00153840409140817\n",
      "train loss:0.0005098949541493533\n",
      "train loss:0.001957581629153514\n",
      "train loss:0.0034857138834928363\n",
      "train loss:0.0016455605959983582\n",
      "train loss:0.00040364517253235017\n",
      "train loss:0.0005111405276139247\n",
      "train loss:0.002191054411507261\n",
      "train loss:0.0006593711434417387\n",
      "train loss:0.0032045058523135594\n",
      "train loss:0.0015277517814232184\n",
      "train loss:0.0006175554109580337\n",
      "train loss:0.0022972826553194024\n",
      "train loss:0.0002495579850680288\n",
      "train loss:0.0014170225760082558\n",
      "train loss:0.004316950599661437\n",
      "train loss:0.0006415260649945667\n",
      "train loss:0.0010080346923145413\n",
      "train loss:0.0002656447136307145\n",
      "train loss:0.003940492682192934\n",
      "train loss:0.0001267569212983537\n",
      "train loss:0.0008582597991790024\n",
      "train loss:0.00017770381448771488\n",
      "train loss:0.0026285658804625204\n",
      "train loss:0.002256373285629029\n",
      "train loss:0.0009990356371780186\n",
      "train loss:0.0014548281143312582\n",
      "train loss:0.0013295511162534227\n",
      "train loss:9.327129678187532e-05\n",
      "train loss:0.0022137999001723523\n",
      "train loss:0.00017388112922211526\n",
      "train loss:0.0036414900903073903\n",
      "train loss:0.001572081018825757\n",
      "train loss:0.0007339185877217624\n",
      "train loss:0.000317722956703725\n",
      "train loss:0.01022699863812781\n",
      "train loss:0.0007348908786489991\n",
      "train loss:0.001244615237399497\n",
      "train loss:0.0013006651983804593\n",
      "train loss:0.004680185004559322\n",
      "train loss:0.0019220903636453363\n",
      "train loss:0.0003258427631718787\n",
      "train loss:0.00341293346365894\n",
      "train loss:0.005643966486896917\n",
      "train loss:0.0006309746981341415\n",
      "train loss:0.004787568155842263\n",
      "train loss:0.0024463403947811513\n",
      "train loss:0.0018601769673715774\n",
      "train loss:0.001639941431122999\n",
      "train loss:0.0035349817331100214\n",
      "train loss:0.0005733884651592723\n",
      "train loss:0.0053528309862960945\n",
      "train loss:0.0018331408190384478\n",
      "train loss:0.003949556891883888\n",
      "train loss:0.0010695457801046795\n",
      "train loss:0.003538336395603438\n",
      "train loss:0.0031164684635928224\n",
      "train loss:0.0012341832886227013\n",
      "train loss:0.0030069745162357338\n",
      "train loss:0.0033288636110786972\n",
      "train loss:0.0027887745380162933\n",
      "train loss:0.0007160120197704932\n",
      "train loss:0.003679369221223262\n",
      "train loss:0.0004101204867776888\n",
      "train loss:0.0006434915420797981\n",
      "train loss:0.0004103037057529813\n",
      "train loss:0.0005560574832348352\n",
      "train loss:0.00031233141114453135\n",
      "train loss:0.006267026187480064\n",
      "train loss:0.0007180399105574365\n",
      "=== epoch:15, train acc:0.996, test acc:0.989 ===\n",
      "train loss:0.004091007605271401\n",
      "train loss:0.0002341665495703481\n",
      "train loss:0.0026335221505626246\n",
      "train loss:0.029497652047635818\n",
      "train loss:0.004038854087538016\n",
      "train loss:0.00034458542109917854\n",
      "train loss:0.003980473945427971\n",
      "train loss:0.0066797192659000856\n",
      "train loss:0.002676821875156524\n",
      "train loss:0.00025009717694948286\n",
      "train loss:0.0006749444943973378\n",
      "train loss:0.00015687922247694712\n",
      "train loss:0.00011173549369840106\n",
      "train loss:0.0002741643953952905\n",
      "train loss:0.00034991555852442827\n",
      "train loss:0.0011331160021352767\n",
      "train loss:0.00045322251070540026\n",
      "train loss:0.0002957788674810931\n",
      "train loss:0.0003498054304256507\n",
      "train loss:0.002149492534067218\n",
      "train loss:0.002814107361342572\n",
      "train loss:0.0012223583161591694\n",
      "train loss:0.0002943342273162296\n",
      "train loss:0.0012464130650783798\n",
      "train loss:0.00037557720174797573\n",
      "train loss:0.0002442904343089648\n",
      "train loss:0.0023503652325110934\n",
      "train loss:0.01429109182450546\n",
      "train loss:0.0003715509177994802\n",
      "train loss:0.0009040419774465662\n",
      "train loss:0.00036840033145107086\n",
      "train loss:0.0005390450765939288\n",
      "train loss:0.001817629599951835\n",
      "train loss:0.002707006259096302\n",
      "train loss:0.005258337269000196\n",
      "train loss:0.0015019957183757188\n",
      "train loss:0.002327065234351531\n",
      "train loss:0.012302885714873263\n",
      "train loss:0.0004904432336300716\n",
      "train loss:0.0005279072440478389\n",
      "train loss:0.0043422410716010855\n",
      "train loss:0.0010550000221603401\n",
      "train loss:0.000698922683370793\n",
      "train loss:0.001876443969670965\n",
      "train loss:0.004611906722007043\n",
      "train loss:0.002458812133192866\n",
      "train loss:0.00043191894194136957\n",
      "train loss:0.0035097749558176267\n",
      "train loss:0.0018722414460419867\n",
      "train loss:0.002370354566027026\n",
      "train loss:0.0008464642775735116\n",
      "train loss:0.0017772996072079696\n",
      "train loss:0.0003394144474303171\n",
      "train loss:0.0017171760544102339\n",
      "train loss:0.007576387730244957\n",
      "train loss:0.0023544423489083198\n",
      "train loss:0.0005815892289724011\n",
      "train loss:0.002358179767850056\n",
      "train loss:6.174983025781216e-05\n",
      "train loss:0.004479484054731516\n",
      "train loss:0.0005323792235895138\n",
      "train loss:0.000619755258891711\n",
      "train loss:0.0015402450968137053\n",
      "train loss:0.00904983383221344\n",
      "train loss:0.003640926348096977\n",
      "train loss:0.0008467193310921792\n",
      "train loss:0.0028013084887805856\n",
      "train loss:0.0003064065466458523\n",
      "train loss:0.0018227209274082206\n",
      "train loss:0.008420338503742537\n",
      "train loss:0.0001263433186081871\n",
      "train loss:0.004468049405130261\n",
      "train loss:0.005943070525810379\n",
      "train loss:0.0022520325637979473\n",
      "train loss:0.0007537159078213978\n",
      "train loss:0.0006941857313234902\n",
      "train loss:0.00217888965179244\n",
      "train loss:0.001290525258970462\n",
      "train loss:0.001383595776718733\n",
      "train loss:0.00063481080023528\n",
      "train loss:0.00027508806390702116\n",
      "train loss:0.0005372042399013507\n",
      "train loss:0.04075002366771881\n",
      "train loss:0.008590275324090742\n",
      "train loss:0.001194423886279737\n",
      "train loss:0.0015943611541706327\n",
      "train loss:0.0009044396590628345\n",
      "train loss:0.0012660590236912633\n",
      "train loss:0.000408708740203114\n",
      "train loss:0.0001810137792086808\n",
      "train loss:0.00015224758523577767\n",
      "train loss:0.0058458582095886604\n",
      "train loss:0.0002617719704312283\n",
      "train loss:0.008050755504572632\n",
      "train loss:0.0005950691000364409\n",
      "train loss:0.002800267566025389\n",
      "train loss:0.0014451656590035462\n",
      "train loss:0.0010913247866350378\n",
      "train loss:0.0009488928587420731\n",
      "train loss:0.0007546286771663135\n",
      "train loss:0.004847276065974447\n",
      "train loss:0.003090299292320018\n",
      "train loss:0.002108554941279804\n",
      "train loss:0.003213818145524718\n",
      "train loss:0.0036533258887559285\n",
      "train loss:0.002470970187001393\n",
      "train loss:0.0004066644420546439\n",
      "train loss:0.0008796595570869162\n",
      "train loss:0.0012163420091403643\n",
      "train loss:0.002832589663368384\n",
      "train loss:0.013233225157809026\n",
      "train loss:0.004367045849634746\n",
      "train loss:0.00020651009437283092\n",
      "train loss:0.002497333169216628\n",
      "train loss:0.0037745045524006644\n",
      "train loss:0.017158059087778284\n",
      "train loss:0.007551142081272024\n",
      "train loss:0.0019368124874334647\n",
      "train loss:0.008155269795696432\n",
      "train loss:0.002531637003472356\n",
      "train loss:0.0032487885909393225\n",
      "train loss:0.0008913889619080146\n",
      "train loss:0.001081330360373309\n",
      "train loss:0.0030429935830690448\n",
      "train loss:0.002179540361417282\n",
      "train loss:0.002229560030802884\n",
      "train loss:0.00022147323776026455\n",
      "train loss:0.0016329495437378758\n",
      "train loss:0.0016347482649487198\n",
      "train loss:0.004265190602632328\n",
      "train loss:0.003782353878553312\n",
      "train loss:0.0012232618085712978\n",
      "train loss:0.0008584721367184763\n",
      "train loss:0.00028875581193441545\n",
      "train loss:0.0004703983540743469\n",
      "train loss:0.0016978915133251653\n",
      "train loss:0.0025269079195962716\n",
      "train loss:0.006004554976235246\n",
      "train loss:0.023074607300618576\n",
      "train loss:0.0013832081851248676\n",
      "train loss:0.0018507666497896455\n",
      "train loss:0.0006764560428154772\n",
      "train loss:0.0009436869530736555\n",
      "train loss:0.004058665223107526\n",
      "train loss:0.004236813037921602\n",
      "train loss:0.00013005097918460933\n",
      "train loss:0.0003791603268559122\n",
      "train loss:0.020479051460085936\n",
      "train loss:0.00034724851862630106\n",
      "train loss:0.0002834942092118153\n",
      "train loss:0.0013786390930076978\n",
      "train loss:0.00043474208277554646\n",
      "train loss:0.004823700693738435\n",
      "train loss:0.001527835280114668\n",
      "train loss:0.0005387315988123196\n",
      "train loss:0.001755285246950239\n",
      "train loss:0.00044744466176958235\n",
      "train loss:0.0015065342661224953\n",
      "train loss:0.002168064199273096\n",
      "train loss:0.0026535228245943704\n",
      "train loss:0.0016423163850066097\n",
      "train loss:0.0012255717697194244\n",
      "train loss:0.0033423182031296094\n",
      "train loss:0.002139139244164165\n",
      "train loss:0.011820105898465107\n",
      "train loss:0.0023694336089776784\n",
      "train loss:0.018735256074389365\n",
      "train loss:0.0017749423935415143\n",
      "train loss:0.0016875078661971925\n",
      "train loss:0.018086425891005\n",
      "train loss:0.0001359515480727847\n",
      "train loss:0.0017288861111200632\n",
      "train loss:0.0028977724687946167\n",
      "train loss:0.0033893760510470493\n",
      "train loss:0.0005039333683127013\n",
      "train loss:0.003824458698457723\n",
      "train loss:0.015026228380344786\n",
      "train loss:0.001206027456968883\n",
      "train loss:0.002621640837346197\n",
      "train loss:0.0006926467473896893\n",
      "train loss:0.003713314830170142\n",
      "train loss:0.00021716385420425912\n",
      "train loss:0.024186002589984015\n",
      "train loss:0.0020939618626353105\n",
      "train loss:0.002442322560344434\n",
      "train loss:0.00025635834870187986\n",
      "train loss:0.004163073895962108\n",
      "train loss:0.0013223777171078803\n",
      "train loss:0.0004772399593233459\n",
      "train loss:0.0004068544962434952\n",
      "train loss:0.0030447567355301663\n",
      "train loss:0.0003993121156706859\n",
      "train loss:0.0030859776822428405\n",
      "train loss:0.017754023895942283\n",
      "train loss:0.0017088614755092857\n",
      "train loss:0.00030530748677802977\n",
      "train loss:0.0040383273759971835\n",
      "train loss:0.003381504078796048\n",
      "train loss:0.0012070786444903594\n",
      "train loss:0.0006672972655184868\n",
      "train loss:0.0022129253903339723\n",
      "train loss:0.0013185075724898624\n",
      "train loss:0.004149999556207421\n",
      "train loss:0.003409794855846998\n",
      "train loss:0.006894819843865981\n",
      "train loss:0.0003619578353892249\n",
      "train loss:0.0015169071907441447\n",
      "train loss:0.0029362247177027096\n",
      "train loss:0.0014605438776576236\n",
      "train loss:0.0001451776159628948\n",
      "train loss:0.0015236867371209155\n",
      "train loss:0.0005122831781362041\n",
      "train loss:0.0012832589180195974\n",
      "train loss:0.0005352215520120541\n",
      "train loss:0.003096765758316631\n",
      "train loss:0.0016156413578554216\n",
      "train loss:0.00023990099098481494\n",
      "train loss:0.0027980008621256674\n",
      "train loss:0.0012405918499922319\n",
      "train loss:0.005131101221862897\n",
      "train loss:0.0020089361127956307\n",
      "train loss:0.00016169344005691248\n",
      "train loss:0.0034410173790833014\n",
      "train loss:0.0007864330727619017\n",
      "train loss:7.21377680553933e-05\n",
      "train loss:0.0006125961843821458\n",
      "train loss:0.0015715314883372002\n",
      "train loss:0.0010334297615963465\n",
      "train loss:0.0017950894132926624\n",
      "train loss:0.0021483809921698576\n",
      "train loss:0.0006053987467674457\n",
      "train loss:0.0005758828976597281\n",
      "train loss:0.0012248600239743337\n",
      "train loss:0.004301449899933418\n",
      "train loss:0.0017800519289813477\n",
      "train loss:0.0012466840910906741\n",
      "train loss:0.00047820414730021313\n",
      "train loss:0.00038677285006908876\n",
      "train loss:0.0034121312916962966\n",
      "train loss:0.0009798951250574606\n",
      "train loss:0.003162826367671638\n",
      "train loss:0.0030372657175404246\n",
      "train loss:0.0018929580967388168\n",
      "train loss:0.020822442009617356\n",
      "train loss:0.0019166224766459159\n",
      "train loss:0.00048472912894364644\n",
      "train loss:0.0006407402117196669\n",
      "train loss:0.004059950115026709\n",
      "train loss:0.013627221026087231\n",
      "train loss:0.003742099962711546\n",
      "train loss:0.00011833540227910163\n",
      "train loss:0.0006234160141165046\n",
      "train loss:0.003265259613058141\n",
      "train loss:0.08696171700587396\n",
      "train loss:0.00035064606526402214\n",
      "train loss:0.0017378284011205852\n",
      "train loss:0.00026292488419731264\n",
      "train loss:0.0036719878814062755\n",
      "train loss:0.0010343304162781098\n",
      "train loss:0.0012816187409640416\n",
      "train loss:0.0001701755904796504\n",
      "train loss:0.002217090888045132\n",
      "train loss:0.002338832859113891\n",
      "train loss:0.001628335991030134\n",
      "train loss:0.002011189039001548\n",
      "train loss:0.003336263074530434\n",
      "train loss:0.00037279027095692726\n",
      "train loss:0.0004886454813980097\n",
      "train loss:0.012383876821773271\n",
      "train loss:0.0033562331537983214\n",
      "train loss:0.00028850838456299813\n",
      "train loss:0.0006811334646682235\n",
      "train loss:0.0004564179838426709\n",
      "train loss:0.0007611437789944156\n",
      "train loss:0.0008173567479560897\n",
      "train loss:0.001094111281160814\n",
      "train loss:0.0013203018168065747\n",
      "train loss:0.00027990530997554095\n",
      "train loss:3.083831694823659e-05\n",
      "train loss:4.2439073023139935e-05\n",
      "train loss:0.0005135724520045102\n",
      "train loss:0.0019529196556492881\n",
      "train loss:0.0008692497320097706\n",
      "train loss:0.0007466161942320219\n",
      "train loss:0.0025719757342512533\n",
      "train loss:0.004546176810312258\n",
      "train loss:0.0011415589543272266\n",
      "train loss:0.002094061191104394\n",
      "train loss:8.960475006132761e-05\n",
      "train loss:0.0012396037710713681\n",
      "train loss:0.00018551132309522155\n",
      "train loss:0.0007063101453502733\n",
      "train loss:0.00011127031942594655\n",
      "train loss:0.003602679322168878\n",
      "train loss:0.00030356295894987524\n",
      "train loss:0.002650382584259957\n",
      "train loss:0.006030418635013671\n",
      "train loss:0.0024082699542954497\n",
      "train loss:0.0013133197203047716\n",
      "train loss:0.00022006182763102458\n",
      "train loss:0.0004600560585215285\n",
      "train loss:0.0034217101864337292\n",
      "train loss:0.0015113814304796936\n",
      "train loss:0.0012798434105681543\n",
      "train loss:0.00351666446039079\n",
      "train loss:0.002639591746644417\n",
      "train loss:0.004251375432197457\n",
      "train loss:0.008889466439792966\n",
      "train loss:0.003817109888866405\n",
      "train loss:0.0030169622682937776\n",
      "train loss:0.0007267339724286568\n",
      "train loss:0.020004524415988243\n",
      "train loss:0.0019483099366697254\n",
      "train loss:9.89828773004203e-05\n",
      "train loss:0.0007986777303191927\n",
      "train loss:0.0012511926403420432\n",
      "train loss:0.0001856825858150023\n",
      "train loss:0.0007328654444971651\n",
      "train loss:0.004259589783108422\n",
      "train loss:0.0008539634472104926\n",
      "train loss:0.0008460291118071485\n",
      "train loss:0.008309025688737788\n",
      "train loss:0.0003982383703895824\n",
      "train loss:0.00036916433455219335\n",
      "train loss:0.0009358181520174897\n",
      "train loss:0.003951050281996508\n",
      "train loss:0.0009908710965863498\n",
      "train loss:0.0006268386991828535\n",
      "train loss:0.004104042450989901\n",
      "train loss:0.004134126459852145\n",
      "train loss:0.00406682142933146\n",
      "train loss:0.002397874937238159\n",
      "train loss:0.007238315411932502\n",
      "train loss:0.0005144913471113496\n",
      "train loss:0.005549406869714425\n",
      "train loss:0.00015184779247441664\n",
      "train loss:0.001382757088082759\n",
      "train loss:0.00425720523452773\n",
      "train loss:0.01214301899370402\n",
      "train loss:0.0005064560848105635\n",
      "train loss:0.0019806330247190466\n",
      "train loss:0.0018895527269752466\n",
      "train loss:0.00763493563246156\n",
      "train loss:0.0015883372740117638\n",
      "train loss:0.005162500221009197\n",
      "train loss:0.0009277782980797811\n",
      "train loss:0.0002433471423214497\n",
      "train loss:0.002431052332857077\n",
      "train loss:0.0011678311053283457\n",
      "train loss:0.002180098701420144\n",
      "train loss:0.0009694363146644361\n",
      "train loss:0.0007288286527263154\n",
      "train loss:0.0004774986825711336\n",
      "train loss:0.0001578766252695923\n",
      "train loss:0.0033186181422686723\n",
      "train loss:0.0003980074844901644\n",
      "train loss:0.00021996733468583406\n",
      "train loss:0.00196325258811504\n",
      "train loss:0.0004667201542545407\n",
      "train loss:0.0023534096207658618\n",
      "train loss:0.0005527230551749951\n",
      "train loss:0.006878825825461359\n",
      "train loss:0.0021827345104550554\n",
      "train loss:0.001876097658968279\n",
      "train loss:0.0006709117279094054\n",
      "train loss:0.00021573089884565155\n",
      "train loss:0.0013876561197492974\n",
      "train loss:0.000871135075485302\n",
      "train loss:0.0006732775678283552\n",
      "train loss:0.004470744219497942\n",
      "train loss:0.0001143455601303537\n",
      "train loss:0.005510595424638806\n",
      "train loss:0.0017484982725503497\n",
      "train loss:0.0005548356930994106\n",
      "train loss:0.0007508619597520604\n",
      "train loss:0.0006593770871542288\n",
      "train loss:0.009346673787062403\n",
      "train loss:0.0021060459132567636\n",
      "train loss:0.0035432660917533053\n",
      "train loss:0.0003863032254346576\n",
      "train loss:0.00016143457035951806\n",
      "train loss:0.001955550874975501\n",
      "train loss:0.003014375344504473\n",
      "train loss:0.00579651003167974\n",
      "train loss:0.001090356173103462\n",
      "train loss:0.0035732302692988572\n",
      "train loss:0.0005977216721869381\n",
      "train loss:0.0030554137306211125\n",
      "train loss:0.0005640663658668663\n",
      "train loss:0.0005341497434776647\n",
      "train loss:0.01049027767276123\n",
      "train loss:0.000669376304323378\n",
      "train loss:0.0044568031268369885\n",
      "train loss:0.01639588077246037\n",
      "train loss:7.833768871343382e-05\n",
      "train loss:0.00050778681441728\n",
      "train loss:0.00013286372073134984\n",
      "train loss:0.0001047779262461808\n",
      "train loss:0.0029565097158556863\n",
      "train loss:0.00013517254504391208\n",
      "train loss:0.0016469360112508957\n",
      "train loss:0.0039126896916405265\n",
      "train loss:0.005016361141125505\n",
      "train loss:0.013257758618310921\n",
      "train loss:0.0016137124016175162\n",
      "train loss:0.002005797605556113\n",
      "train loss:0.0001297853106995518\n",
      "train loss:9.885729683352634e-05\n",
      "train loss:0.0011937409995201231\n",
      "train loss:0.0116675386328524\n",
      "train loss:5.621903229367911e-05\n",
      "train loss:0.001898028302938931\n",
      "train loss:0.0003691886617212442\n",
      "train loss:0.0003920778174160791\n",
      "train loss:0.04164052430630694\n",
      "train loss:0.0026344679180851298\n",
      "train loss:0.008839987593662849\n",
      "train loss:0.0009178566527677079\n",
      "train loss:0.00033158089825945894\n",
      "train loss:0.0012917385800771874\n",
      "train loss:0.0031598001819283534\n",
      "train loss:0.000625921021869489\n",
      "train loss:0.002617903828331932\n",
      "train loss:0.003291547375116461\n",
      "train loss:0.0009467267182166318\n",
      "train loss:0.0007767700612713443\n",
      "train loss:0.00015244100807164832\n",
      "train loss:0.0009183534409014188\n",
      "train loss:0.004231322193817887\n",
      "train loss:0.0010437510775602155\n",
      "train loss:0.0011521644661825357\n",
      "train loss:0.0013227662921437523\n",
      "train loss:0.0036254564179403507\n",
      "train loss:0.001533097882491341\n",
      "train loss:0.000279694781372588\n",
      "train loss:0.0018376192637286756\n",
      "train loss:0.0009813902382859504\n",
      "train loss:0.00048802223126961225\n",
      "train loss:0.0010045654098174638\n",
      "train loss:0.0012106543634697955\n",
      "train loss:0.002783401166366317\n",
      "train loss:0.002662759057368172\n",
      "train loss:0.0035333698940187663\n",
      "train loss:0.005814656165917851\n",
      "train loss:0.0019863876756878226\n",
      "train loss:0.0030070492531949415\n",
      "train loss:0.0036954193570200525\n",
      "train loss:0.005807693771361839\n",
      "train loss:0.0013671376349577735\n",
      "train loss:0.0005471369615881631\n",
      "train loss:0.002310911624648816\n",
      "train loss:0.0004060276203254676\n",
      "train loss:0.0011487362905762627\n",
      "train loss:0.0005594601502833534\n",
      "train loss:0.005067209806546929\n",
      "train loss:0.0004607139437969181\n",
      "train loss:0.002245403354032656\n",
      "train loss:0.00041536847107126587\n",
      "train loss:0.0007488113940682066\n",
      "train loss:4.734506350356207e-05\n",
      "train loss:0.0023644482973169843\n",
      "train loss:7.998648033917396e-05\n",
      "train loss:0.0049318435962636305\n",
      "train loss:0.0004696552552297066\n",
      "train loss:0.000794338396229988\n",
      "train loss:0.0006389505259485336\n",
      "train loss:0.006798823401197607\n",
      "train loss:0.0002899693056299597\n",
      "train loss:0.0004437272164146143\n",
      "train loss:0.0016580608583213758\n",
      "train loss:0.00035260316378454094\n",
      "train loss:0.004361410732587775\n",
      "train loss:0.00042370543814291824\n",
      "train loss:0.0006262242129625637\n",
      "train loss:0.006361261201928132\n",
      "train loss:0.001410386814138981\n",
      "train loss:0.001127453146132609\n",
      "train loss:0.0011280789417278062\n",
      "train loss:0.007599077407750112\n",
      "train loss:0.0012699156353714958\n",
      "train loss:6.574464459822748e-05\n",
      "train loss:0.0017925435054674344\n",
      "train loss:0.005583481845729231\n",
      "train loss:0.003716184992280244\n",
      "train loss:0.004199865715581757\n",
      "train loss:0.0002245293041675268\n",
      "train loss:0.0014083158102564447\n",
      "train loss:0.0024498707515854174\n",
      "train loss:0.0015084013956872591\n",
      "train loss:0.0023125751195382464\n",
      "train loss:0.0009767958358941594\n",
      "train loss:0.0009946796577899712\n",
      "train loss:0.003768705963413074\n",
      "train loss:0.00023360794507537236\n",
      "train loss:0.0003979104760929549\n",
      "train loss:0.0005281734953252303\n",
      "train loss:0.021013730701040477\n",
      "train loss:0.008071299062980758\n",
      "train loss:0.0010465321589694203\n",
      "train loss:0.0011384172314427017\n",
      "train loss:0.0027629411579505915\n",
      "train loss:0.00023408414674162766\n",
      "train loss:0.0008491145469864558\n",
      "train loss:0.0029105391709686296\n",
      "train loss:0.000465678305513449\n",
      "train loss:0.0039241834689655196\n",
      "train loss:0.002749491246683772\n",
      "train loss:0.0036979573538241327\n",
      "train loss:0.004029850356444483\n",
      "train loss:0.0010095518586459387\n",
      "train loss:0.0007171274689987098\n",
      "train loss:0.003349398426235323\n",
      "train loss:0.0019059545097596061\n",
      "train loss:0.002296827135793859\n",
      "train loss:0.006066838308309711\n",
      "train loss:0.00043217840934705654\n",
      "train loss:0.0018847985031423223\n",
      "train loss:0.0013290126719012146\n",
      "train loss:0.002120325671396051\n",
      "train loss:0.003143115824661571\n",
      "train loss:0.000176719817484404\n",
      "train loss:0.002358159676453744\n",
      "train loss:0.00018208937141056582\n",
      "train loss:0.0021074359159386573\n",
      "train loss:0.0012981886033851392\n",
      "train loss:0.001138052289798221\n",
      "train loss:3.704231133116458e-05\n",
      "train loss:0.00013568990202480948\n",
      "train loss:0.0002651072443129105\n",
      "train loss:0.0004016651982755176\n",
      "train loss:0.0006440620737006987\n",
      "train loss:0.0018644446569932552\n",
      "train loss:0.0092708413132301\n",
      "train loss:0.0005012316057999506\n",
      "train loss:2.8730720778420304e-05\n",
      "train loss:0.002605964037589783\n",
      "train loss:0.0006400023554053633\n",
      "train loss:0.0004841221719797915\n",
      "train loss:0.008381497341187755\n",
      "train loss:0.0007357987503738586\n",
      "train loss:0.00031737706892846497\n",
      "train loss:0.0008364760928950706\n",
      "train loss:0.00036218057443075283\n",
      "train loss:0.0014368389754246796\n",
      "train loss:0.003231755059700171\n",
      "train loss:0.0021489550181725435\n",
      "train loss:0.003299736030174271\n",
      "train loss:0.00026339914773683296\n",
      "train loss:0.0001397957382138592\n",
      "train loss:0.004784662830440102\n",
      "train loss:0.003011839805693459\n",
      "train loss:0.0004223252170933031\n",
      "train loss:0.005718953891820244\n",
      "train loss:0.0008533273192495941\n",
      "train loss:0.0085599143292957\n",
      "train loss:0.005446219029123511\n",
      "train loss:0.0010681308982250384\n",
      "train loss:0.00014155654985210882\n",
      "train loss:0.001227743177132527\n",
      "train loss:0.0031620701999728086\n",
      "train loss:0.0029468625971108063\n",
      "train loss:0.008329332967428237\n",
      "train loss:0.0024076868942081928\n",
      "train loss:0.003906401661447464\n",
      "train loss:0.002921157635321453\n",
      "train loss:0.0033101960794947494\n",
      "train loss:0.000680862676209874\n",
      "train loss:0.01886359312468998\n",
      "train loss:0.002474342558180689\n",
      "train loss:0.004298337141510427\n",
      "train loss:6.684663461158473e-05\n",
      "train loss:0.0006379513251867096\n",
      "train loss:0.008370472199230898\n",
      "train loss:0.000179886565955343\n",
      "train loss:0.004989816101564833\n",
      "train loss:0.00392125568823033\n",
      "train loss:0.00848681490906063\n",
      "train loss:0.00035031199111050623\n",
      "train loss:0.00040219764091053425\n",
      "train loss:0.008578519055518086\n",
      "train loss:0.0004975909890484032\n",
      "train loss:0.007890404115163265\n",
      "train loss:0.0005412474617264292\n",
      "train loss:0.00020392631644953509\n",
      "train loss:0.006745659209062287\n",
      "train loss:0.0006141789930914243\n",
      "train loss:0.0020192848361084105\n",
      "train loss:0.0001887574888081249\n",
      "train loss:0.0002756387274977225\n",
      "train loss:0.0010371164897436556\n",
      "train loss:0.0010348325261632743\n",
      "train loss:0.010198225805216472\n",
      "train loss:0.003446446722406068\n",
      "train loss:0.0010116916193425017\n",
      "train loss:0.0024467444246819665\n",
      "train loss:0.0016052181242488053\n",
      "train loss:0.0012690431782469709\n",
      "train loss:0.0008849172097104994\n",
      "train loss:0.0005612171373279216\n",
      "train loss:0.00467734719304394\n",
      "=== epoch:16, train acc:0.998, test acc:0.983 ===\n",
      "train loss:0.011136072268543287\n",
      "train loss:0.008464828322101426\n",
      "train loss:0.0020942313408439375\n",
      "train loss:0.002935976618822919\n",
      "train loss:0.0007394922896417\n",
      "train loss:0.0010148016457959582\n",
      "train loss:0.008536138772889848\n",
      "train loss:0.0024224281428874236\n",
      "train loss:0.0005151844990633204\n",
      "train loss:0.0002484089895082735\n",
      "train loss:5.7238379047165844e-05\n",
      "train loss:0.0005632301992568617\n",
      "train loss:0.001489248567899018\n",
      "train loss:0.0005069076827086402\n",
      "train loss:0.001538276779981507\n",
      "train loss:0.0013073333809975674\n",
      "train loss:0.0024994926697563067\n",
      "train loss:0.0004554921499793381\n",
      "train loss:0.001342087624060469\n",
      "train loss:0.001505028385881812\n",
      "train loss:0.0035415370830217453\n",
      "train loss:0.002278779396983767\n",
      "train loss:0.0026024485977533657\n",
      "train loss:0.0003439171458008557\n",
      "train loss:0.00043533155957383026\n",
      "train loss:0.005148011941003263\n",
      "train loss:0.005153598271638098\n",
      "train loss:5.8795058915478035e-05\n",
      "train loss:0.0033342735259318723\n",
      "train loss:0.0037049500876425888\n",
      "train loss:0.0009731630238928943\n",
      "train loss:0.0021494825131151093\n",
      "train loss:0.00015202863223112568\n",
      "train loss:0.006549882613228686\n",
      "train loss:0.002812271044160254\n",
      "train loss:0.0005263269075868209\n",
      "train loss:0.0019572648143518605\n",
      "train loss:0.00013743580945792106\n",
      "train loss:0.015892841275752963\n",
      "train loss:0.00041308617016182106\n",
      "train loss:0.003761832826050498\n",
      "train loss:0.002269197587273592\n",
      "train loss:0.0023547008500657747\n",
      "train loss:0.00018469319197599073\n",
      "train loss:0.0006486750641599946\n",
      "train loss:0.01182566632625488\n",
      "train loss:0.006240477462224311\n",
      "train loss:0.0006119653021531967\n",
      "train loss:0.01187059522189378\n",
      "train loss:0.00506088883892319\n",
      "train loss:0.0053651012819907725\n",
      "train loss:0.000680946362179934\n",
      "train loss:0.00011493220124716125\n",
      "train loss:0.0020189158206959223\n",
      "train loss:0.001284477823515142\n",
      "train loss:0.008372319279416719\n",
      "train loss:0.0016691963216089556\n",
      "train loss:0.0005623389265466788\n",
      "train loss:0.00883407485275211\n",
      "train loss:0.003920958367479221\n",
      "train loss:0.00047403681292461087\n",
      "train loss:0.006631856281651802\n",
      "train loss:0.009224038599043778\n",
      "train loss:0.008110610386180267\n",
      "train loss:0.000699833316803679\n",
      "train loss:0.0024104465180873435\n",
      "train loss:0.00041576890195130006\n",
      "train loss:0.001111569093868752\n",
      "train loss:0.001206695559195709\n",
      "train loss:0.002840446521859437\n",
      "train loss:0.03375712384676479\n",
      "train loss:0.00325960592735314\n",
      "train loss:0.0005495543961602998\n",
      "train loss:0.0012976955664985002\n",
      "train loss:0.00010323372813598497\n",
      "train loss:0.0014507311469125275\n",
      "train loss:0.0033087139917747765\n",
      "train loss:0.001160427423470005\n",
      "train loss:0.009612040696172277\n",
      "train loss:0.005413452581194517\n",
      "train loss:0.0017707416394517877\n",
      "train loss:0.000642032654364987\n",
      "train loss:0.001694745286333064\n",
      "train loss:0.0062731286772243836\n",
      "train loss:0.0024360348660428803\n",
      "train loss:0.0003538756663671924\n",
      "train loss:0.0109125155693836\n",
      "train loss:0.001185696157979279\n",
      "train loss:0.002705426081718492\n",
      "train loss:0.00188498928885869\n",
      "train loss:0.0004024253907833196\n",
      "train loss:0.0027669518865119785\n",
      "train loss:0.00024043602815106396\n",
      "train loss:0.0010435789414749593\n",
      "train loss:0.00210322870152988\n",
      "train loss:0.0010851915197138502\n",
      "train loss:0.004239102656331192\n",
      "train loss:0.00040315906095058767\n",
      "train loss:0.0015794056472168633\n",
      "train loss:0.03516499299223961\n",
      "train loss:0.0019023810311174269\n",
      "train loss:0.05306925558831463\n",
      "train loss:0.09903517666100639\n",
      "train loss:0.0016632168330980282\n",
      "train loss:0.0005120586170284067\n",
      "train loss:0.0006146363105497379\n",
      "train loss:0.0033424198531909156\n",
      "train loss:0.0024176297374992038\n",
      "train loss:0.0021935556807369384\n",
      "train loss:0.0028797993848130475\n",
      "train loss:0.000410459583716442\n",
      "train loss:0.0021997333856771813\n",
      "train loss:0.013124919187643453\n",
      "train loss:0.0013215288452713943\n",
      "train loss:0.0042371799386676755\n",
      "train loss:0.002034877899320878\n",
      "train loss:0.0003677546735915193\n",
      "train loss:0.00039770481955637727\n",
      "train loss:0.001969073544472355\n",
      "train loss:0.0010371885370293627\n",
      "train loss:0.00045316327824981904\n",
      "train loss:0.0025019227229894597\n",
      "train loss:0.00019400642214755653\n",
      "train loss:0.0003851583882104962\n",
      "train loss:0.0008054390490695233\n",
      "train loss:0.0020003594262322983\n",
      "train loss:0.001877755661116102\n",
      "train loss:0.002844026799504302\n",
      "train loss:0.0010585744359098612\n",
      "train loss:0.0018919531928273915\n",
      "train loss:0.0005058319865599945\n",
      "train loss:0.00143056833186762\n",
      "train loss:0.0009184504276004061\n",
      "train loss:0.005888116707633578\n",
      "train loss:0.004811294189911097\n",
      "train loss:0.0017561482253963543\n",
      "train loss:0.005762199550806817\n",
      "train loss:0.002577142538267281\n",
      "train loss:0.0005738648609527353\n",
      "train loss:0.0010135237301319552\n",
      "train loss:0.0030330304615329017\n",
      "train loss:0.004446883099264124\n",
      "train loss:0.00014109750849903457\n",
      "train loss:0.010645511240764573\n",
      "train loss:0.006649555572612321\n",
      "train loss:0.0003703982822135986\n",
      "train loss:0.001020682838935118\n",
      "train loss:0.0008438765021683535\n",
      "train loss:0.0012251793298006406\n",
      "train loss:0.001954081555145411\n",
      "train loss:0.00019405955355090393\n",
      "train loss:0.0008678025862810226\n",
      "train loss:0.01037851949456655\n",
      "train loss:0.004589636405777788\n",
      "train loss:0.0014949119707730635\n",
      "train loss:0.0036714832510411706\n",
      "train loss:0.000718064089975511\n",
      "train loss:0.0006876583376585432\n",
      "train loss:0.0019826284201674028\n",
      "train loss:0.0004802705621642479\n",
      "train loss:0.017739158665219083\n",
      "train loss:0.001974049862903669\n",
      "train loss:0.0033695343774895236\n",
      "train loss:0.00038318782909850925\n",
      "train loss:5.255314497951756e-05\n",
      "train loss:0.001377778507883575\n",
      "train loss:0.0006053864334176927\n",
      "train loss:0.00655484637847311\n",
      "train loss:0.002506602344938641\n",
      "train loss:0.0011038338178872482\n",
      "train loss:0.0006439965515364427\n",
      "train loss:0.0007792894135555488\n",
      "train loss:0.015684004992589026\n",
      "train loss:0.0007693950454294972\n",
      "train loss:0.0031722210931081213\n",
      "train loss:0.0009372404452200008\n",
      "train loss:0.0010085472222283714\n",
      "train loss:0.002079770116050547\n",
      "train loss:0.002368268691551435\n",
      "train loss:0.0019205944169340692\n",
      "train loss:0.001988585511521614\n",
      "train loss:0.00018665792290120743\n",
      "train loss:0.0003919525356152586\n",
      "train loss:0.00023347805102463744\n",
      "train loss:0.0011961102668819987\n",
      "train loss:0.001006277855113623\n",
      "train loss:0.03689916279762714\n",
      "train loss:0.0002613108009567077\n",
      "train loss:0.0007639535119993776\n",
      "train loss:0.005028157253410808\n",
      "train loss:0.034810759237272164\n",
      "train loss:0.0028959939209011135\n",
      "train loss:0.0013277282548300776\n",
      "train loss:0.001792247426805767\n",
      "train loss:0.002191954783642826\n",
      "train loss:0.0001917529077249897\n",
      "train loss:0.0026087953784731848\n",
      "train loss:0.002356333858358759\n",
      "train loss:0.0007498303094592731\n",
      "train loss:0.00391931050965991\n",
      "train loss:0.0014025845133726065\n",
      "train loss:0.0011726338478869368\n",
      "train loss:0.0005975421729320562\n",
      "train loss:0.005158919771526389\n",
      "train loss:0.0008341967945355522\n",
      "train loss:0.00108973741942138\n",
      "train loss:0.002261980906201162\n",
      "train loss:0.00019091861455130044\n",
      "train loss:0.0003627969527351284\n",
      "train loss:0.002281730521131358\n",
      "train loss:0.0010923684769361317\n",
      "train loss:0.0017303785901687041\n",
      "train loss:0.0001981064910383346\n",
      "train loss:0.00995854194186008\n",
      "train loss:0.0002263972775531056\n",
      "train loss:0.0026328248274037763\n",
      "train loss:0.0024410401975952967\n",
      "train loss:0.0033459416413626013\n",
      "train loss:0.00028297192213620705\n",
      "train loss:0.00023313540359311123\n",
      "train loss:0.0010413916225310528\n",
      "train loss:0.0016530580927277174\n",
      "train loss:0.0007020885372818035\n",
      "train loss:0.0003587524034783734\n",
      "train loss:0.002989390315484296\n",
      "train loss:0.0028855606385561464\n",
      "train loss:0.0022498786157995184\n",
      "train loss:0.0025464830933122078\n",
      "train loss:0.0006295477921077655\n",
      "train loss:0.00020552748630604665\n",
      "train loss:0.00254583500519269\n",
      "train loss:0.003486267595069919\n",
      "train loss:0.0038661175527859993\n",
      "train loss:0.0038290345028068285\n",
      "train loss:0.0026803715674651378\n",
      "train loss:0.0005523111120797605\n",
      "train loss:0.0025544143079324027\n",
      "train loss:0.00011967852011313015\n",
      "train loss:0.00533664401529332\n",
      "train loss:0.0006010528165027767\n",
      "train loss:0.0005770718421868133\n",
      "train loss:0.002227513898128363\n",
      "train loss:7.130667938440783e-05\n",
      "train loss:0.0028262624914735103\n",
      "train loss:0.008148717930999692\n",
      "train loss:0.008541364993833823\n",
      "train loss:0.005960228035804067\n",
      "train loss:8.780917101592253e-05\n",
      "train loss:0.0002219006734786665\n",
      "train loss:4.863777189158112e-05\n",
      "train loss:0.002027065801556296\n",
      "train loss:0.0008352731920575182\n",
      "train loss:0.0007294349390897832\n",
      "train loss:0.001130972193899432\n",
      "train loss:0.0026149497622220902\n",
      "train loss:0.001107678389073704\n",
      "train loss:0.0016208262961329891\n",
      "train loss:0.004629925808892379\n",
      "train loss:0.0026747887997491697\n",
      "train loss:0.0007691021210467739\n",
      "train loss:0.002319135117420323\n",
      "train loss:0.0029695147176363483\n",
      "train loss:0.00889569954305652\n",
      "train loss:0.0038218405044723536\n",
      "train loss:0.0005282966119035309\n",
      "train loss:0.002594229583313462\n",
      "train loss:0.00048810316606498846\n",
      "train loss:0.0012822235236419495\n",
      "train loss:0.003815107084065234\n",
      "train loss:0.0004982414747041156\n",
      "train loss:0.005530429071485738\n",
      "train loss:0.0009229163534055741\n",
      "train loss:0.005115644326207647\n",
      "train loss:0.005463312779253016\n",
      "train loss:0.0003018847309491368\n",
      "train loss:0.00022182590706746398\n",
      "train loss:0.0005160844819656969\n",
      "train loss:0.00237615489111829\n",
      "train loss:0.00039898871284630295\n",
      "train loss:0.001014755460303423\n",
      "train loss:0.0073371626461764435\n",
      "train loss:0.0061757370804167355\n",
      "train loss:0.001427605158925614\n",
      "train loss:0.001367131317640262\n",
      "train loss:0.00042996932232260506\n",
      "train loss:0.002419501215888298\n",
      "train loss:9.254918938095777e-05\n",
      "train loss:0.0006785438940243926\n",
      "train loss:0.0009227989270123602\n",
      "train loss:0.00019800058180184166\n",
      "train loss:0.001126966323303261\n",
      "train loss:0.00230704725370842\n",
      "train loss:0.000594195216637581\n",
      "train loss:0.0003800939109087981\n",
      "train loss:0.0013176663504309751\n",
      "train loss:0.0016753132115161745\n",
      "train loss:0.00012528948749227566\n",
      "train loss:0.0011685019526855092\n",
      "train loss:0.0005454001325621785\n",
      "train loss:0.0012299184319039893\n",
      "train loss:0.001441454959986094\n",
      "train loss:0.0002433819345667201\n",
      "train loss:0.0060740865630805226\n",
      "train loss:0.0016789310497368442\n",
      "train loss:0.0004344693974822845\n",
      "train loss:7.794266734632236e-05\n",
      "train loss:0.0075186584000288695\n",
      "train loss:0.0010725611220721324\n",
      "train loss:0.0017681909620108677\n",
      "train loss:0.0005106585506568874\n",
      "train loss:0.0010453276376159882\n",
      "train loss:0.0013130526704977516\n",
      "train loss:0.0004412643289883309\n",
      "train loss:0.0037773453933821516\n",
      "train loss:0.0028814736640149956\n",
      "train loss:0.000810153884030332\n",
      "train loss:0.00027398169639010787\n",
      "train loss:0.012415589112611647\n",
      "train loss:0.0003714882484471222\n",
      "train loss:0.00047873637130975254\n",
      "train loss:0.00533277280413052\n",
      "train loss:0.0009388985700602866\n",
      "train loss:0.0007612919162456596\n",
      "train loss:0.003624320385497029\n",
      "train loss:0.0029326577731882465\n",
      "train loss:0.0002734528951917529\n",
      "train loss:0.003606068397486206\n",
      "train loss:0.0020028227980231884\n",
      "train loss:0.0019367101856289923\n",
      "train loss:0.00042226913237394044\n",
      "train loss:0.0005778724259518161\n",
      "train loss:0.0021775443729241304\n",
      "train loss:0.0012000497552514938\n",
      "train loss:0.0011025629753104959\n",
      "train loss:0.009279523364158736\n",
      "train loss:0.00123636602233362\n",
      "train loss:0.0031931139531021805\n",
      "train loss:0.0008431045761973276\n",
      "train loss:0.001482677729736499\n",
      "train loss:0.006197904525739794\n",
      "train loss:0.004915900753924164\n",
      "train loss:0.0007425054985956975\n",
      "train loss:0.0019173320005950345\n",
      "train loss:0.0003889166712740652\n",
      "train loss:0.0004065511625376359\n",
      "train loss:0.003574706944398292\n",
      "train loss:0.0009263309853710061\n",
      "train loss:0.00023230095356622846\n",
      "train loss:0.0013839008849350436\n",
      "train loss:0.0007250562295882478\n",
      "train loss:0.0022336789397168284\n",
      "train loss:8.982618650944395e-05\n",
      "train loss:0.0017630089471329788\n",
      "train loss:0.005001329386528844\n",
      "train loss:0.0011794403289778618\n",
      "train loss:0.0023325464973672507\n",
      "train loss:0.0004586002550484177\n",
      "train loss:0.004414380799111809\n",
      "train loss:0.005895102336178304\n",
      "train loss:0.0004691926314628647\n",
      "train loss:0.0006792725841607851\n",
      "train loss:8.560050350795598e-05\n",
      "train loss:0.00025931160255376787\n",
      "train loss:0.00037714564858306714\n",
      "train loss:0.0009152127121857321\n",
      "train loss:0.002737421899892707\n",
      "train loss:0.000738718947967493\n",
      "train loss:0.0021663428675251155\n",
      "train loss:0.0008943472461479922\n",
      "train loss:0.0014539710382671437\n",
      "train loss:0.0002500825706324239\n",
      "train loss:0.0007336352768014389\n",
      "train loss:0.0005237871374524819\n",
      "train loss:0.06640834551020641\n",
      "train loss:0.002811923735452283\n",
      "train loss:0.0009324546486703974\n",
      "train loss:0.01632606405597318\n",
      "train loss:0.0005915172475321608\n",
      "train loss:0.0011021455647114323\n",
      "train loss:0.00024027742595859793\n",
      "train loss:0.015782288595805934\n",
      "train loss:0.002853097867283913\n",
      "train loss:0.0019498715640365399\n",
      "train loss:0.002777290825807387\n",
      "train loss:0.0019562622882936414\n",
      "train loss:0.0030914190296636975\n",
      "train loss:0.001183665473778515\n",
      "train loss:0.00046725547930719\n",
      "train loss:0.0026551005887913313\n",
      "train loss:0.0019956778489683214\n",
      "train loss:0.001443551017620954\n",
      "train loss:0.0016463441125163405\n",
      "train loss:0.0012937271164517633\n",
      "train loss:0.0030542637560331187\n",
      "train loss:0.0031749985151571635\n",
      "train loss:0.00011519112483007141\n",
      "train loss:0.0009589207804538466\n",
      "train loss:0.00021892122598206654\n",
      "train loss:0.001698584900994973\n",
      "train loss:0.0017240967058162568\n",
      "train loss:0.003717932877094662\n",
      "train loss:0.003850858423871357\n",
      "train loss:0.016928264537240337\n",
      "train loss:0.0018153042011355444\n",
      "train loss:0.0017586780906632276\n",
      "train loss:0.0012210195985666622\n",
      "train loss:0.0008408601807262512\n",
      "train loss:0.0005655351960790604\n",
      "train loss:0.0005489470952788326\n",
      "train loss:0.0013257769485563575\n",
      "train loss:0.000696562859002021\n",
      "train loss:0.0003173197193908926\n",
      "train loss:0.00038209908137200056\n",
      "train loss:0.0021269073476494586\n",
      "train loss:0.0003115763616343098\n",
      "train loss:0.002783467301745398\n",
      "train loss:0.0005552149286812333\n",
      "train loss:0.005524835355645468\n",
      "train loss:0.0001454753378621831\n",
      "train loss:0.0017200577008785508\n",
      "train loss:0.002258419559029558\n",
      "train loss:0.0006974759140826564\n",
      "train loss:0.000850140448203867\n",
      "train loss:0.002296596012506635\n",
      "train loss:0.0013836338377174213\n",
      "train loss:8.52345906180502e-05\n",
      "train loss:0.004549395402648673\n",
      "train loss:0.002407265536350536\n",
      "train loss:0.0001805713994931529\n",
      "train loss:0.0003395717884737041\n",
      "train loss:0.002179083223556067\n",
      "train loss:0.0023715824307884007\n",
      "train loss:0.0007628623100355847\n",
      "train loss:0.00010621341308810735\n",
      "train loss:0.0006480308066527729\n",
      "train loss:0.00028825484999829185\n",
      "train loss:0.009747456678242531\n",
      "train loss:0.00402218873241331\n",
      "train loss:0.003098832065737068\n",
      "train loss:0.002253780330889727\n",
      "train loss:0.0018452738469512528\n",
      "train loss:0.0015843476483128744\n",
      "train loss:0.0010281855309495012\n",
      "train loss:0.0009221514872513506\n",
      "train loss:0.001400504941527767\n",
      "train loss:0.0017298085472586334\n",
      "train loss:0.0005060212892070261\n",
      "train loss:0.0018473638491811617\n",
      "train loss:0.0013902433659663865\n",
      "train loss:0.003279001658781594\n",
      "train loss:0.0006169580811779252\n",
      "train loss:0.00035736193577874073\n",
      "train loss:0.0009589710211782474\n",
      "train loss:0.0007319576056169673\n",
      "train loss:0.002171992023388459\n",
      "train loss:0.0011725170086196236\n",
      "train loss:0.001411600593545003\n",
      "train loss:0.00014460464386212505\n",
      "train loss:0.00034398303055680846\n",
      "train loss:0.0008132528520080145\n",
      "train loss:0.0016624131398603414\n",
      "train loss:0.0009635140621866195\n",
      "train loss:0.0005507164886886502\n",
      "train loss:0.0007968541034938452\n",
      "train loss:0.001760313722533328\n",
      "train loss:0.0023656916794600937\n",
      "train loss:0.000122746210792332\n",
      "train loss:0.002530382277614431\n",
      "train loss:0.0001675866910587916\n",
      "train loss:0.002181070906177935\n",
      "train loss:0.0003190427716749318\n",
      "train loss:0.0005475139164761401\n",
      "train loss:0.0011431147447451785\n",
      "train loss:0.0048441303644147235\n",
      "train loss:0.0024319825554247987\n",
      "train loss:0.00020912633453629728\n",
      "train loss:0.0020647666440467656\n",
      "train loss:0.002083923099957738\n",
      "train loss:0.0013736962892052329\n",
      "train loss:0.0006468050757252706\n",
      "train loss:0.0021821851080187243\n",
      "train loss:0.004452342643799392\n",
      "train loss:0.00014975256331450386\n",
      "train loss:0.00581990152812073\n",
      "train loss:0.00013646704904403574\n",
      "train loss:0.0006490432751462434\n",
      "train loss:0.002701862556437429\n",
      "train loss:8.44712931468906e-05\n",
      "train loss:0.0011159080656538865\n",
      "train loss:0.002375115146639253\n",
      "train loss:7.861082666593218e-05\n",
      "train loss:0.0005076351411472209\n",
      "train loss:0.0033428596552253177\n",
      "train loss:0.0009348740043199068\n",
      "train loss:0.0018157993154298748\n",
      "train loss:0.0027767511477273426\n",
      "train loss:0.0008806554268854056\n",
      "train loss:0.0003502737619622254\n",
      "train loss:0.0015035139924959224\n",
      "train loss:0.0001773883710463405\n",
      "train loss:0.003610411882296724\n",
      "train loss:0.009991679960869346\n",
      "train loss:0.0030709535508977657\n",
      "train loss:0.00021846104600772287\n",
      "train loss:0.002496322780701618\n",
      "train loss:0.052274470404765856\n",
      "train loss:0.0003040259586997929\n",
      "train loss:0.00025312758209388624\n",
      "train loss:0.0010238005674480292\n",
      "train loss:0.014380427150227006\n",
      "train loss:6.349774983331013e-05\n",
      "train loss:0.004440774035570284\n",
      "train loss:0.0007880763117203451\n",
      "train loss:0.00019948895819330215\n",
      "train loss:0.010262483215672032\n",
      "train loss:0.0018744871627650334\n",
      "train loss:0.0040038053968738775\n",
      "train loss:0.0029673854657758718\n",
      "train loss:0.0002556086560907021\n",
      "train loss:0.0013674550094299016\n",
      "train loss:0.0016724037339830302\n",
      "train loss:0.0017582301871492943\n",
      "train loss:0.002381897005719996\n",
      "train loss:0.001985215601149345\n",
      "train loss:0.0001893941408323549\n",
      "train loss:0.004956803075106481\n",
      "train loss:0.002538864084549393\n",
      "train loss:0.0002969638171176637\n",
      "train loss:0.005026262255794087\n",
      "train loss:0.0013704544800860405\n",
      "train loss:0.0007355576186024958\n",
      "train loss:0.002406311751629619\n",
      "train loss:0.002139204882872541\n",
      "train loss:0.0004340266640945157\n",
      "train loss:0.07468754975941702\n",
      "train loss:0.0012027077043036583\n",
      "train loss:0.0026206711181858433\n",
      "train loss:0.00182577099254467\n",
      "train loss:0.0006446828451257068\n",
      "train loss:0.0038870513453946823\n",
      "train loss:0.0006370612788408961\n",
      "train loss:0.0013284822251476028\n",
      "train loss:0.0015865572302081116\n",
      "train loss:0.0005835282076718122\n",
      "train loss:0.006026846283174317\n",
      "train loss:2.0939363594917592e-05\n",
      "train loss:0.004269700729216754\n",
      "train loss:0.021082906705108352\n",
      "train loss:0.00012021045111742082\n",
      "train loss:0.00572489071600117\n",
      "train loss:0.0017648084504509672\n",
      "train loss:1.3880069209596947e-05\n",
      "train loss:7.44708456430827e-05\n",
      "train loss:0.0019234496555791025\n",
      "train loss:0.00037059168435878435\n",
      "train loss:0.00015419106066683912\n",
      "train loss:0.00013777231190984835\n",
      "train loss:0.0032849376241550175\n",
      "train loss:0.0009117362778380276\n",
      "train loss:0.003135397123343338\n",
      "train loss:0.002536368363842113\n",
      "train loss:0.0020454105596018814\n",
      "train loss:0.00011166623246583612\n",
      "train loss:0.0007678559985567001\n",
      "train loss:3.983459151474706e-05\n",
      "train loss:0.0022468888424821156\n",
      "train loss:0.011571720295714363\n",
      "train loss:0.001669098860543216\n",
      "train loss:0.00031463430696313617\n",
      "train loss:0.0015664902603061861\n",
      "train loss:0.0020264459588778235\n",
      "train loss:0.00013442822143629787\n",
      "train loss:0.010285493999791704\n",
      "train loss:0.0024102391597525187\n",
      "train loss:0.0012913232597996757\n",
      "train loss:0.002723698743340982\n",
      "train loss:0.006863015681499768\n",
      "train loss:0.0030604660822620765\n",
      "train loss:0.0052613853551183145\n",
      "train loss:0.011495416427572338\n",
      "train loss:0.002131534421948179\n",
      "train loss:0.004935708268209007\n",
      "train loss:0.0014589840269710536\n",
      "train loss:0.001968659422470596\n",
      "train loss:0.00011036922783599002\n",
      "train loss:0.00043388673214543404\n",
      "train loss:0.005835232997211183\n",
      "train loss:0.000414020151762631\n",
      "train loss:0.00033345755283109297\n",
      "train loss:0.0008074165337541285\n",
      "train loss:0.012742874530441834\n",
      "train loss:0.0037306845803227397\n",
      "train loss:0.004215461605934263\n",
      "train loss:0.0011947410391675217\n",
      "train loss:0.02461208394008371\n",
      "train loss:0.019140083119788983\n",
      "train loss:0.004695547210278359\n",
      "train loss:0.002430720580950158\n",
      "train loss:0.0009025771046239299\n",
      "train loss:0.004576236955622373\n",
      "=== epoch:17, train acc:0.994, test acc:0.988 ===\n",
      "train loss:0.00026270217828562787\n",
      "train loss:0.00904653279388425\n",
      "train loss:9.130335237355257e-05\n",
      "train loss:0.0007075961123734255\n",
      "train loss:0.0027806248808646804\n",
      "train loss:0.0006697209105671037\n",
      "train loss:0.0055993431749661\n",
      "train loss:0.0002508371726346475\n",
      "train loss:0.0005869641740649111\n",
      "train loss:0.0032698863124033606\n",
      "train loss:0.0033823520345357383\n",
      "train loss:0.0003578693263664349\n",
      "train loss:0.0021352777841581894\n",
      "train loss:0.0072370798592802646\n",
      "train loss:0.004024253841069306\n",
      "train loss:0.004301156134574188\n",
      "train loss:0.002610449025956466\n",
      "train loss:0.0012378496829188285\n",
      "train loss:0.00041849266606381317\n",
      "train loss:0.0006740456278906146\n",
      "train loss:0.0009217203965143382\n",
      "train loss:0.0010649661674346311\n",
      "train loss:0.001690340112509853\n",
      "train loss:0.00615484362820135\n",
      "train loss:0.008751349641496734\n",
      "train loss:0.004267625273245137\n",
      "train loss:0.0022221437761276424\n",
      "train loss:0.0015055976657731907\n",
      "train loss:0.0012013069187466715\n",
      "train loss:0.0005546855207462226\n",
      "train loss:0.023342788195649552\n",
      "train loss:0.0008371375033097933\n",
      "train loss:0.0014947807784539552\n",
      "train loss:0.005032140399965298\n",
      "train loss:0.009314117051104681\n",
      "train loss:0.0006305709158503969\n",
      "train loss:0.0010043152360109386\n",
      "train loss:0.000492646327791561\n",
      "train loss:0.0025696281250257736\n",
      "train loss:0.00032778239457708367\n",
      "train loss:8.117423783487513e-05\n",
      "train loss:7.082906688828131e-05\n",
      "train loss:0.00036007283671190383\n",
      "train loss:0.00011854744683413544\n",
      "train loss:0.004704027880416672\n",
      "train loss:0.0002803068589166962\n",
      "train loss:0.0031562178491320748\n",
      "train loss:0.005051535812046348\n",
      "train loss:0.00215791567679252\n",
      "train loss:0.0009703078748796863\n",
      "train loss:0.0019332119048677693\n",
      "train loss:0.0026404049680015186\n",
      "train loss:0.0019696603879563734\n",
      "train loss:0.0013470780027135143\n",
      "train loss:0.0032777800623506894\n",
      "train loss:0.0009758134181785001\n",
      "train loss:0.0020227883506214097\n",
      "train loss:0.003865916619375302\n",
      "train loss:0.0003703760392047935\n",
      "train loss:0.0017878465338552166\n",
      "train loss:0.0022206785565732617\n",
      "train loss:0.002988063519769386\n",
      "train loss:0.013584600852115549\n",
      "train loss:0.003151532001491229\n",
      "train loss:0.003235822856364645\n",
      "train loss:0.0002622878138956281\n",
      "train loss:0.00837894009310654\n",
      "train loss:0.001037042317132664\n",
      "train loss:0.0013135784360903294\n",
      "train loss:0.0027091372346835785\n",
      "train loss:0.004709208902312387\n",
      "train loss:0.00043198261185718886\n",
      "train loss:0.0006142421094729626\n",
      "train loss:0.0034169384868344586\n",
      "train loss:0.012256941897493942\n",
      "train loss:0.0005471322518121719\n",
      "train loss:0.0034573163357818943\n",
      "train loss:0.0029959088053773462\n",
      "train loss:0.0012699613894696397\n",
      "train loss:0.00298497027913268\n",
      "train loss:0.0029102793295761975\n",
      "train loss:0.0005421947518439653\n",
      "train loss:0.0004520937644374034\n",
      "train loss:0.0031542923291424836\n",
      "train loss:0.004942243502517388\n",
      "train loss:0.0009069989718328604\n",
      "train loss:8.742527883694363e-05\n",
      "train loss:0.0027135239817553027\n",
      "train loss:0.0019655327026507456\n",
      "train loss:0.0024192962074891236\n",
      "train loss:0.0021371450052104627\n",
      "train loss:0.0008052447396942175\n",
      "train loss:0.002163881187422262\n",
      "train loss:0.0019377032072230669\n",
      "train loss:0.0014962447683372884\n",
      "train loss:0.001827595631891109\n",
      "train loss:0.0013509065789193253\n",
      "train loss:0.0004057924788206075\n",
      "train loss:0.0020316670064658743\n",
      "train loss:0.0030292150390629057\n",
      "train loss:0.003323807076675997\n",
      "train loss:0.0011907072050335525\n",
      "train loss:0.002891740589150294\n",
      "train loss:0.0009125854225974177\n",
      "train loss:0.002052153364671701\n",
      "train loss:0.004452384289627545\n",
      "train loss:0.0017220356393892475\n",
      "train loss:0.000550941532798895\n",
      "train loss:0.002304245657037656\n",
      "train loss:0.003859184457196864\n",
      "train loss:0.0022694221328086405\n",
      "train loss:0.007702257487292263\n",
      "train loss:0.0008002640802647208\n",
      "train loss:0.0007820722630723399\n",
      "train loss:0.0008845174739819306\n",
      "train loss:0.0031791840029044478\n",
      "train loss:0.00047260103433776046\n",
      "train loss:0.0005168622131492049\n",
      "train loss:0.0009302810953738413\n",
      "train loss:0.00022615307876134514\n",
      "train loss:0.004140461989473115\n",
      "train loss:0.0008167171287958226\n",
      "train loss:0.004463736520679703\n",
      "train loss:0.000778331310253064\n",
      "train loss:0.001774994312566225\n",
      "train loss:0.0016691449387576155\n",
      "train loss:0.0013871352205369247\n",
      "train loss:0.0005873990680173053\n",
      "train loss:0.00020261501252773851\n",
      "train loss:0.0003457619569752661\n",
      "train loss:0.0025137504946923087\n",
      "train loss:0.0013688179700531008\n",
      "train loss:0.019013459249177397\n",
      "train loss:0.0014278731107154026\n",
      "train loss:0.0019204873078451279\n",
      "train loss:0.000183212057161137\n",
      "train loss:0.0010234879826546087\n",
      "train loss:0.0013734175457251973\n",
      "train loss:0.0012932400669472025\n",
      "train loss:0.004438431162480444\n",
      "train loss:0.00489725271875388\n",
      "train loss:0.004529632872144139\n",
      "train loss:0.00206992074445898\n",
      "train loss:0.007164353915673223\n",
      "train loss:0.004531672708684822\n",
      "train loss:0.005243714704315672\n",
      "train loss:0.008505477722867232\n",
      "train loss:0.0003724863651569114\n",
      "train loss:0.020691961808719864\n",
      "train loss:0.0019945930907818744\n",
      "train loss:0.0008920382278656979\n",
      "train loss:0.01880383823686126\n",
      "train loss:0.00028188692192639787\n",
      "train loss:0.003988861011850694\n",
      "train loss:0.00034676840637372147\n",
      "train loss:0.00028239761149708885\n",
      "train loss:0.00090768177139164\n",
      "train loss:0.00037762521838400725\n",
      "train loss:0.0019185279237223874\n",
      "train loss:0.0002129688015612621\n",
      "train loss:0.0022664695624099425\n",
      "train loss:0.006819614460782859\n",
      "train loss:0.002624568940011176\n",
      "train loss:8.441664339878008e-06\n",
      "train loss:0.0016214008078210827\n",
      "train loss:0.0006348315054272321\n",
      "train loss:0.0008905893216936593\n",
      "train loss:0.0021479828868051125\n",
      "train loss:0.006021517654273536\n",
      "train loss:0.00017586197337576625\n",
      "train loss:0.003126546463914407\n",
      "train loss:0.00541297555873255\n",
      "train loss:4.664703051381886e-05\n",
      "train loss:0.001082890575617416\n",
      "train loss:0.0015186696181465637\n",
      "train loss:0.002678726879012502\n",
      "train loss:0.0007628077342211454\n",
      "train loss:0.0014640665616810367\n",
      "train loss:0.0002603756805750414\n",
      "train loss:0.000857427725799467\n",
      "train loss:0.00022318064847794746\n",
      "train loss:0.004569532696977962\n",
      "train loss:0.0011699395483390895\n",
      "train loss:0.00011270693994954065\n",
      "train loss:0.000829411246720091\n",
      "train loss:0.0007387821495999619\n",
      "train loss:0.0011332396395727648\n",
      "train loss:0.023938661585442535\n",
      "train loss:0.0014683793810799386\n",
      "train loss:0.0007932776582624275\n",
      "train loss:0.00039774358600701596\n",
      "train loss:0.0005153869717580923\n",
      "train loss:0.0009040322712495449\n",
      "train loss:0.0017068063531181033\n",
      "train loss:2.2717473700254836e-05\n",
      "train loss:0.00017197678310687913\n",
      "train loss:0.004082347013001908\n",
      "train loss:0.0001861736667357086\n",
      "train loss:0.0008547071764885232\n",
      "train loss:0.002248912530251782\n",
      "train loss:0.00016835378423675037\n",
      "train loss:0.0009796963469920377\n",
      "train loss:0.0023759923514042516\n",
      "train loss:0.0003657949424046498\n",
      "train loss:0.0024072807953022953\n",
      "train loss:0.0024501324999905935\n",
      "train loss:0.002959985145206784\n",
      "train loss:0.0003946365347228491\n",
      "train loss:0.0006133120267868195\n",
      "train loss:0.00010462069623176813\n",
      "train loss:0.00042258925338164473\n",
      "train loss:0.0003442712327277997\n",
      "train loss:0.005017844352764386\n",
      "train loss:0.00022552833758088635\n",
      "train loss:0.004699901886691263\n",
      "train loss:0.0003517152435678062\n",
      "train loss:0.0002486727029624339\n",
      "train loss:0.0017491819943159207\n",
      "train loss:0.00031096645982286645\n",
      "train loss:0.0027734250104723963\n",
      "train loss:0.012184332211120813\n",
      "train loss:0.00041546587397948525\n",
      "train loss:0.0005294110020244172\n",
      "train loss:0.00126726049861029\n",
      "train loss:0.014613943095670778\n",
      "train loss:0.0024824371090156378\n",
      "train loss:0.01098558859502184\n",
      "train loss:0.0020235996396854926\n",
      "train loss:0.0006764647391853327\n",
      "train loss:0.0011543040639727152\n",
      "train loss:0.00019060874873661092\n",
      "train loss:0.00021421670827062817\n",
      "train loss:0.006518307879386814\n",
      "train loss:0.00418773028740703\n",
      "train loss:0.010330426109096728\n",
      "train loss:0.004900790153599088\n",
      "train loss:0.0039199234385908135\n",
      "train loss:0.005258218591258802\n",
      "train loss:0.0011387920179838046\n",
      "train loss:0.004991424760399471\n",
      "train loss:0.0003323288429533636\n",
      "train loss:0.0005146623155773523\n",
      "train loss:0.02573401154541823\n",
      "train loss:0.0054616058578700385\n",
      "train loss:0.0007840881737416122\n",
      "train loss:0.0006517636603569593\n",
      "train loss:0.0009456279090359365\n",
      "train loss:0.0014262012658966272\n",
      "train loss:0.00024908481829527806\n",
      "train loss:0.0003481877895424562\n",
      "train loss:0.003206788398423707\n",
      "train loss:0.001283542589248986\n",
      "train loss:0.000543148722702183\n",
      "train loss:0.0004057304458453707\n",
      "train loss:0.0019023247318186265\n",
      "train loss:4.9806410746023576e-05\n",
      "train loss:0.0014041196546723872\n",
      "train loss:0.018078932314692715\n",
      "train loss:0.0015312597437338333\n",
      "train loss:0.0002607075873577921\n",
      "train loss:0.0007669576536043032\n",
      "train loss:0.004206081949184544\n",
      "train loss:0.0013993513631459541\n",
      "train loss:0.0018474573115238402\n",
      "train loss:0.0041161761333749205\n",
      "train loss:0.0016123637403166128\n",
      "train loss:0.0002539541728808048\n",
      "train loss:0.0007930231096630553\n",
      "train loss:0.0012207008271312725\n",
      "train loss:0.00047897997046810457\n",
      "train loss:0.0010723131141772448\n",
      "train loss:0.004984686151587184\n",
      "train loss:0.0010707196637254443\n",
      "train loss:0.006135248151584246\n",
      "train loss:0.0031337842710929546\n",
      "train loss:0.0006074932889167229\n",
      "train loss:0.0010364665087838772\n",
      "train loss:0.0005388675043394632\n",
      "train loss:0.0030929101126191657\n",
      "train loss:0.002106033993088891\n",
      "train loss:0.00010939916736587336\n",
      "train loss:0.0003363545478766171\n",
      "train loss:0.0008837048057518263\n",
      "train loss:0.004658139267026149\n",
      "train loss:0.004111618907726168\n",
      "train loss:4.568039686600531e-05\n",
      "train loss:0.0051848182036734435\n",
      "train loss:0.005919176532006652\n",
      "train loss:0.0019973738932794\n",
      "train loss:0.0006687054197156953\n",
      "train loss:0.00019578545215972868\n",
      "train loss:0.0003167125718079712\n",
      "train loss:0.0031584666243889238\n",
      "train loss:0.000472728275635492\n",
      "train loss:0.003558020978817349\n",
      "train loss:0.015182581558976204\n",
      "train loss:0.00016726922887058\n",
      "train loss:0.0008579412094116781\n",
      "train loss:0.0011252316896877652\n",
      "train loss:0.0048201297545911615\n",
      "train loss:0.0003023380846719909\n",
      "train loss:0.03297675210038014\n",
      "train loss:0.002757367339399036\n",
      "train loss:0.00031434501810533104\n",
      "train loss:0.00397259447759884\n",
      "train loss:0.0072376118598288285\n",
      "train loss:0.0002894146401837233\n",
      "train loss:0.0006333132220879567\n",
      "train loss:0.0003910085233129818\n",
      "train loss:0.00242387816750505\n",
      "train loss:0.004492816767788529\n",
      "train loss:0.005117459343339649\n",
      "train loss:0.0027333634101213158\n",
      "train loss:0.0005405339507363587\n",
      "train loss:0.00250882426738402\n",
      "train loss:0.004666919941932671\n",
      "train loss:0.002991008270718306\n",
      "train loss:0.0002579155558506563\n",
      "train loss:0.0017293895262882263\n",
      "train loss:0.00015989846749549428\n",
      "train loss:0.0001669636586352434\n",
      "train loss:0.0010925691564281053\n",
      "train loss:0.004616009207165972\n",
      "train loss:0.0013399314921379699\n",
      "train loss:0.000533024080928743\n",
      "train loss:0.00021548041529266483\n",
      "train loss:0.03745390373891532\n",
      "train loss:0.00034783466872046116\n",
      "train loss:0.001005658568312551\n",
      "train loss:2.8809545696580403e-05\n",
      "train loss:0.001731376849348178\n",
      "train loss:0.0010319157782096586\n",
      "train loss:0.003382241730361226\n",
      "train loss:0.002121900474054365\n",
      "train loss:0.01088582958369707\n",
      "train loss:0.001240200471630954\n",
      "train loss:0.00014289393041261428\n",
      "train loss:0.004800261345629016\n",
      "train loss:0.0013574732593894744\n",
      "train loss:0.0014368516141756817\n",
      "train loss:0.002384009387119044\n",
      "train loss:0.0008044378032534473\n",
      "train loss:0.0016411874948756271\n",
      "train loss:0.0008986573977157558\n",
      "train loss:0.0034345197197379333\n",
      "train loss:0.0005181728853803453\n",
      "train loss:0.0006601278155327029\n",
      "train loss:0.0012387044714850207\n",
      "train loss:0.0010894934804060044\n",
      "train loss:0.0001849024870957985\n",
      "train loss:0.0038798796250789397\n",
      "train loss:0.0020832857848864204\n",
      "train loss:0.002885733474324045\n",
      "train loss:0.001339393186720065\n",
      "train loss:0.033157781731340014\n",
      "train loss:0.0009354292387729483\n",
      "train loss:0.00012534963041894678\n",
      "train loss:0.003563072985854854\n",
      "train loss:0.00021961425713290937\n",
      "train loss:0.007576140079268457\n",
      "train loss:0.0031643837560860364\n",
      "train loss:0.002770402527124474\n",
      "train loss:0.0026213938926260166\n",
      "train loss:0.004565727426848518\n",
      "train loss:0.00018503613855048426\n",
      "train loss:0.0012561998762660909\n",
      "train loss:0.00040719386657264386\n",
      "train loss:0.0021307206787814785\n",
      "train loss:0.0030057126883392404\n",
      "train loss:0.00034126069983621504\n",
      "train loss:0.003847347902340173\n",
      "train loss:0.00019422271910202578\n",
      "train loss:0.0005274983108986545\n",
      "train loss:0.0022453550760405273\n",
      "train loss:0.000573493931561186\n",
      "train loss:0.0003489679642698387\n",
      "train loss:0.0012549931750226537\n",
      "train loss:0.002684373069185832\n",
      "train loss:8.083770648842717e-05\n",
      "train loss:0.000628585747608192\n",
      "train loss:0.005685556710226391\n",
      "train loss:0.00660992705845097\n",
      "train loss:0.0023582383381477176\n",
      "train loss:0.0011056855706096534\n",
      "train loss:0.001575557983432862\n",
      "train loss:0.011467086209618676\n",
      "train loss:8.733441852376258e-05\n",
      "train loss:0.00178015889789359\n",
      "train loss:0.00128079290961627\n",
      "train loss:0.017759190131174423\n",
      "train loss:0.0020729931192061094\n",
      "train loss:0.0004830482556544791\n",
      "train loss:0.00041796352468814334\n",
      "train loss:0.0027189745869846023\n",
      "train loss:0.00039473112850406866\n",
      "train loss:0.003146814776644044\n",
      "train loss:0.0013636196402852631\n",
      "train loss:0.0009782502992526471\n",
      "train loss:0.0014952647499750695\n",
      "train loss:0.000591373590622917\n",
      "train loss:0.0009795603159660454\n",
      "train loss:0.0023800294057658338\n",
      "train loss:0.007714629796250152\n",
      "train loss:0.0017894002660422779\n",
      "train loss:0.0006776039428766797\n",
      "train loss:0.0011664845110330925\n",
      "train loss:0.001523245968980476\n",
      "train loss:0.0032962658736205015\n",
      "train loss:0.0008524830752258141\n",
      "train loss:0.0014541624993469509\n",
      "train loss:0.0006759170329487778\n",
      "train loss:0.00035580000856357387\n",
      "train loss:0.003793938444164095\n",
      "train loss:0.00023507713380703605\n",
      "train loss:0.0006221556823855551\n",
      "train loss:0.0030796455686607997\n",
      "train loss:0.0028055286455225376\n",
      "train loss:0.002374670708118834\n",
      "train loss:0.02444163669273794\n",
      "train loss:0.00018134655425205873\n",
      "train loss:0.0053257012163792925\n",
      "train loss:0.00018618839979539904\n",
      "train loss:0.0016300232260348174\n",
      "train loss:0.001906256558489784\n",
      "train loss:0.004375867992616863\n",
      "train loss:0.0013810211505988288\n",
      "train loss:0.003590445704739022\n",
      "train loss:0.0034410438856396245\n",
      "train loss:0.0006209160252714867\n",
      "train loss:0.002404766357499418\n",
      "train loss:0.0005357407701639044\n",
      "train loss:0.0001665640715028158\n",
      "train loss:0.002516710982407977\n",
      "train loss:0.0015681208589141795\n",
      "train loss:0.0007061305726656632\n",
      "train loss:0.014531976631519571\n",
      "train loss:0.004204204359614735\n",
      "train loss:0.0004603976838043746\n",
      "train loss:0.0005854028946916982\n",
      "train loss:0.0002545443855235992\n",
      "train loss:0.0002815694688514474\n",
      "train loss:0.0014667146041812019\n",
      "train loss:0.0028613987783269075\n",
      "train loss:1.4209906634021245e-05\n",
      "train loss:0.00025977943348965224\n",
      "train loss:0.0017800874332181147\n",
      "train loss:0.0019208888687678768\n",
      "train loss:0.0001883948749380778\n",
      "train loss:0.001157250059319572\n",
      "train loss:0.0008728755400558819\n",
      "train loss:0.0017373163944316519\n",
      "train loss:0.0034449442030578536\n",
      "train loss:0.0001481530416944683\n",
      "train loss:0.000306368138669865\n",
      "train loss:0.0006608823982429907\n",
      "train loss:0.0006972809827566787\n",
      "train loss:0.0001827510530583177\n",
      "train loss:0.010527507014543542\n",
      "train loss:0.0005438552387337221\n",
      "train loss:0.01886569580846384\n",
      "train loss:0.0015533198368687631\n",
      "train loss:8.609941015155746e-05\n",
      "train loss:0.00020716395686897575\n",
      "train loss:0.0005533426622488567\n",
      "train loss:0.0005104107096836794\n",
      "train loss:0.001025581673290533\n",
      "train loss:0.0017465763699639906\n",
      "train loss:0.004270320789969068\n",
      "train loss:0.0006688116803970073\n",
      "train loss:0.0016342141512884231\n",
      "train loss:0.002782644159954935\n",
      "train loss:0.013308313619670078\n",
      "train loss:0.04166640041495418\n",
      "train loss:0.021680394744897247\n",
      "train loss:0.000877375607808215\n",
      "train loss:0.00047071718550164454\n",
      "train loss:0.0001806150463766555\n",
      "train loss:0.010249184000553468\n",
      "train loss:0.009865641146780534\n",
      "train loss:0.001181155488957377\n",
      "train loss:0.0005111897629288593\n",
      "train loss:0.0011533585255139999\n",
      "train loss:1.3435449777888204e-05\n",
      "train loss:0.003396468227853068\n",
      "train loss:0.000467757765245755\n",
      "train loss:0.004964489553509417\n",
      "train loss:0.005344890155922464\n",
      "train loss:0.0049774192052185135\n",
      "train loss:0.01110089664975113\n",
      "train loss:0.0011375269856580632\n",
      "train loss:0.0006613531355056869\n",
      "train loss:0.0007177516778910282\n",
      "train loss:7.77910692893023e-05\n",
      "train loss:0.0002442857708916335\n",
      "train loss:0.0013527522025130905\n",
      "train loss:0.0003134935928701304\n",
      "train loss:0.001747630389383261\n",
      "train loss:0.004368039896094373\n",
      "train loss:0.0042427246098965775\n",
      "train loss:0.004558520465643722\n",
      "train loss:0.0033357258352509833\n",
      "train loss:0.001959453661771278\n",
      "train loss:0.00036836053645819056\n",
      "train loss:0.0007871460966573859\n",
      "train loss:0.00033770191800063404\n",
      "train loss:0.0028803089581871484\n",
      "train loss:0.0002579150271059348\n",
      "train loss:0.0007267188581196737\n",
      "train loss:1.9198595587104315e-05\n",
      "train loss:0.002261947342288557\n",
      "train loss:0.0006760321906000309\n",
      "train loss:0.0020404787969289374\n",
      "train loss:0.0073759360852214476\n",
      "train loss:0.003131277411825607\n",
      "train loss:9.238570972418097e-05\n",
      "train loss:0.0058006463931126896\n",
      "train loss:0.00479113556953956\n",
      "train loss:0.001076191661532684\n",
      "train loss:0.0009349626408099397\n",
      "train loss:0.0012136672568471897\n",
      "train loss:0.00040197642260608117\n",
      "train loss:3.500415566202159e-05\n",
      "train loss:0.004093084244297394\n",
      "train loss:0.00013624383210142493\n",
      "train loss:0.0004653436315809042\n",
      "train loss:0.006906818161073715\n",
      "train loss:0.001523477200538394\n",
      "train loss:0.0037207404674567063\n",
      "train loss:0.0007345447974558678\n",
      "train loss:0.0011325621113582147\n",
      "train loss:5.933459543855808e-05\n",
      "train loss:0.0002467881469793085\n",
      "train loss:0.0003948954722674787\n",
      "train loss:0.0005353316032210914\n",
      "train loss:0.0015823627804510889\n",
      "train loss:5.600726805664177e-05\n",
      "train loss:0.0005723426327094152\n",
      "train loss:0.00028116298670676746\n",
      "train loss:0.0045644737637511815\n",
      "train loss:0.0004729613872793697\n",
      "train loss:9.694382814927745e-05\n",
      "train loss:0.0009261307607960489\n",
      "train loss:0.0011913177035562156\n",
      "train loss:0.001433786105954337\n",
      "train loss:0.0008786866725962123\n",
      "train loss:0.0010540497439602231\n",
      "train loss:0.002092623683636952\n",
      "train loss:0.00015249078949809355\n",
      "train loss:0.00040584396146750904\n",
      "train loss:0.005004249836367376\n",
      "train loss:0.00010349366660546247\n",
      "train loss:0.0012725453056017888\n",
      "train loss:0.00026405148634585905\n",
      "train loss:0.0016048777997817933\n",
      "train loss:0.003316279839851731\n",
      "train loss:0.0015319800892183227\n",
      "train loss:0.0010613742997568002\n",
      "train loss:0.00016423885827313874\n",
      "train loss:0.0010632955749456743\n",
      "train loss:8.00366233714509e-05\n",
      "train loss:0.0007832814420213226\n",
      "train loss:4.946751114084119e-05\n",
      "train loss:0.002977090091373797\n",
      "train loss:0.00016060830894417064\n",
      "train loss:0.0007081917960903904\n",
      "train loss:7.228881559642695e-05\n",
      "train loss:0.0015645507331082152\n",
      "train loss:0.0005358526754737642\n",
      "train loss:0.0017572945092243042\n",
      "train loss:9.37699047768394e-05\n",
      "train loss:0.0005369618187416495\n",
      "train loss:0.0036220839384549248\n",
      "train loss:0.00015402051932677705\n",
      "train loss:0.0014693783460257709\n",
      "train loss:0.00027872211084065964\n",
      "train loss:0.00011496894092960711\n",
      "train loss:0.003495090264918466\n",
      "train loss:0.00011622175915139113\n",
      "train loss:0.0033409078330280373\n",
      "train loss:4.2135143761093025e-05\n",
      "train loss:0.0015624180888208039\n",
      "train loss:0.00017427294356852692\n",
      "train loss:0.0002495327825364475\n",
      "train loss:0.0026566270442754387\n",
      "train loss:0.00010181372984911164\n",
      "train loss:0.0011590070166699257\n",
      "train loss:0.00014630376378075817\n",
      "train loss:0.0022076425157016495\n",
      "train loss:0.004160967786088158\n",
      "train loss:0.001564761847630981\n",
      "train loss:5.1657430660572616e-05\n",
      "train loss:0.003396399681472122\n",
      "train loss:0.004681734665763545\n",
      "train loss:0.00013343004458231603\n",
      "train loss:6.939775866665718e-05\n",
      "train loss:0.00024630988826117924\n",
      "train loss:0.00019564421630746003\n",
      "train loss:0.0002766287462672987\n",
      "train loss:5.68657644789606e-05\n",
      "train loss:0.0022179391980646068\n",
      "=== epoch:18, train acc:0.997, test acc:0.983 ===\n",
      "train loss:0.0012657540476545119\n",
      "train loss:0.0010632839055364192\n",
      "train loss:8.961808544896927e-05\n",
      "train loss:0.0027219352433686794\n",
      "train loss:0.00022753838545899307\n",
      "train loss:0.00015985498607471707\n",
      "train loss:0.003496570809961124\n",
      "train loss:0.0004544256090712745\n",
      "train loss:0.0026599699288877986\n",
      "train loss:0.0007398746027575395\n",
      "train loss:0.0007648247176982048\n",
      "train loss:0.0015953366598443492\n",
      "train loss:0.00018635604992021365\n",
      "train loss:0.00020508901058715693\n",
      "train loss:0.0012769102660617024\n",
      "train loss:0.0041272248062910735\n",
      "train loss:0.000675798637644873\n",
      "train loss:0.0010315582480177072\n",
      "train loss:0.002491031931219912\n",
      "train loss:0.00028176803312739986\n",
      "train loss:0.0009455178050796268\n",
      "train loss:0.001014598031502656\n",
      "train loss:0.0009309673129250952\n",
      "train loss:0.0010003112592909531\n",
      "train loss:0.004026241722964028\n",
      "train loss:0.0017837843636263164\n",
      "train loss:0.00015970832798564108\n",
      "train loss:0.0061089565917262605\n",
      "train loss:0.0010143149665596647\n",
      "train loss:0.0012272103574304464\n",
      "train loss:0.00016777846835254402\n",
      "train loss:0.00014524201024687604\n",
      "train loss:0.0021563475078764625\n",
      "train loss:0.0022757367462083334\n",
      "train loss:0.00695138551211156\n",
      "train loss:0.004495436849513767\n",
      "train loss:0.000270024401660713\n",
      "train loss:0.0013554126817741413\n",
      "train loss:0.0039447575425611454\n",
      "train loss:0.000864390934012535\n",
      "train loss:0.00011530886738779022\n",
      "train loss:0.0015792449434073506\n",
      "train loss:0.00011904439987967745\n",
      "train loss:9.376916252132922e-06\n",
      "train loss:6.16938251094119e-05\n",
      "train loss:0.0007059624800467883\n",
      "train loss:0.0017537648562286861\n",
      "train loss:0.0001622599146834332\n",
      "train loss:0.0017812600203708665\n",
      "train loss:0.00032347606492293964\n",
      "train loss:0.00021578558093340576\n",
      "train loss:0.0006575467595373645\n",
      "train loss:0.0002825627905258324\n",
      "train loss:0.007118220973657935\n",
      "train loss:0.0019004472608497897\n",
      "train loss:0.0011970520103167434\n",
      "train loss:4.495672552851276e-05\n",
      "train loss:0.0019451088389133041\n",
      "train loss:3.208199588759266e-05\n",
      "train loss:0.0023740252622047957\n",
      "train loss:7.385318083823336e-05\n",
      "train loss:0.001796472630288276\n",
      "train loss:0.0016805543392185143\n",
      "train loss:0.0008080888596419196\n",
      "train loss:0.0030569852439123867\n",
      "train loss:0.00036587252236195826\n",
      "train loss:0.0013256030309714356\n",
      "train loss:0.0001445502361830536\n",
      "train loss:0.0009404532196706872\n",
      "train loss:0.000587004103557014\n",
      "train loss:0.0003781863861758828\n",
      "train loss:0.0019326398116057395\n",
      "train loss:0.00010439065142791234\n",
      "train loss:0.009806323157214619\n",
      "train loss:0.0010329433821284905\n",
      "train loss:0.001824561815665053\n",
      "train loss:0.0012413529031460773\n",
      "train loss:0.00201607893231936\n",
      "train loss:3.0252697554922744e-05\n",
      "train loss:0.00023432982264084225\n",
      "train loss:0.001620122077678374\n",
      "train loss:0.003930210036242051\n",
      "train loss:0.00015199885842077116\n",
      "train loss:6.632796813136708e-05\n",
      "train loss:6.016928465318574e-05\n",
      "train loss:0.024662186170028216\n",
      "train loss:0.001005878337260535\n",
      "train loss:0.0004758434061836446\n",
      "train loss:0.00280561573717459\n",
      "train loss:0.005948097162618177\n",
      "train loss:0.0055994671270977745\n",
      "train loss:0.0002868933956171521\n",
      "train loss:0.0009065836513153175\n",
      "train loss:0.0009598906715784747\n",
      "train loss:0.03410935692449664\n",
      "train loss:0.0006700844431543998\n",
      "train loss:0.009662609317396756\n",
      "train loss:0.0029202454331712746\n",
      "train loss:0.002627982859335041\n",
      "train loss:0.0019553812002131173\n",
      "train loss:0.0004518049791498464\n",
      "train loss:0.0003993952563643041\n",
      "train loss:0.0017100652412828645\n",
      "train loss:0.002556171137298143\n",
      "train loss:8.879031771282446e-05\n",
      "train loss:0.009326656981601202\n",
      "train loss:0.0025195461835410523\n",
      "train loss:0.00045348431214614485\n",
      "train loss:0.0001714044999677836\n",
      "train loss:0.000700864499804517\n",
      "train loss:0.0020151918694238354\n",
      "train loss:0.0012463866510060999\n",
      "train loss:3.0185193903111324e-05\n",
      "train loss:0.0005167387850392416\n",
      "train loss:0.0009049483877176634\n",
      "train loss:4.854766097180847e-05\n",
      "train loss:0.0002665875324438559\n",
      "train loss:0.0005502042355524698\n",
      "train loss:0.00018990592363150242\n",
      "train loss:0.00013029518163344917\n",
      "train loss:0.0016994261326139844\n",
      "train loss:0.0005023791989220405\n",
      "train loss:0.0023255453596224777\n",
      "train loss:0.001540607186534124\n",
      "train loss:0.0026124077574793225\n",
      "train loss:0.0007444966785622775\n",
      "train loss:0.0028818190991267913\n",
      "train loss:0.0014095260579624939\n",
      "train loss:0.007012102658045178\n",
      "train loss:0.009007608150837208\n",
      "train loss:0.0004856917096653179\n",
      "train loss:0.0013082268076776019\n",
      "train loss:0.027023857526780794\n",
      "train loss:0.0004369650967015108\n",
      "train loss:0.0004213478617650879\n",
      "train loss:0.0004663531179956811\n",
      "train loss:0.0009724174693863826\n",
      "train loss:0.011068169365437017\n",
      "train loss:0.0008381507087531854\n",
      "train loss:0.00024149424439996302\n",
      "train loss:0.001766697407492092\n",
      "train loss:0.00023510722903340977\n",
      "train loss:0.0011781879213377054\n",
      "train loss:0.0023049955493800868\n",
      "train loss:0.0002640559356612683\n",
      "train loss:0.0013431797138899765\n",
      "train loss:0.0005715125206089806\n",
      "train loss:0.0026841415767223566\n",
      "train loss:0.0008933593593223162\n",
      "train loss:0.0021355033951617947\n",
      "train loss:0.0006317938606970476\n",
      "train loss:0.0010042256315586277\n",
      "train loss:0.00719384099936106\n",
      "train loss:0.001211972931038032\n",
      "train loss:0.002992426914774256\n",
      "train loss:0.0033671158138956487\n",
      "train loss:0.001604489454903339\n",
      "train loss:0.0010476869214648674\n",
      "train loss:0.00014090822002625318\n",
      "train loss:0.0051077998101522306\n",
      "train loss:0.0006141247962859625\n",
      "train loss:0.016464696074469008\n",
      "train loss:0.0035178777233227676\n",
      "train loss:0.0005240224402438292\n",
      "train loss:0.00019901822054101562\n",
      "train loss:0.001125801099792894\n",
      "train loss:0.005717506118163509\n",
      "train loss:8.046753095840017e-05\n",
      "train loss:0.000643108189148869\n",
      "train loss:0.0009440668105647574\n",
      "train loss:0.000993916348841625\n",
      "train loss:0.00015326462995939868\n",
      "train loss:0.0014300233670306035\n",
      "train loss:0.00025765497970948515\n",
      "train loss:0.0009253534470320733\n",
      "train loss:0.0014116243742600836\n",
      "train loss:0.0025762952263688587\n",
      "train loss:0.0001839855448766305\n",
      "train loss:0.00021508821353034895\n",
      "train loss:0.00022711087166255955\n",
      "train loss:0.0014362336869984918\n",
      "train loss:0.0017777554334306987\n",
      "train loss:0.0014168864196587394\n",
      "train loss:0.0006197981407769257\n",
      "train loss:0.00017687139718854501\n",
      "train loss:0.00019933019496221548\n",
      "train loss:0.00124060535683223\n",
      "train loss:0.0001414703820095883\n",
      "train loss:0.00014827098476897976\n",
      "train loss:0.00048580419478363795\n",
      "train loss:0.0021051116814796668\n",
      "train loss:0.0013842592785102874\n",
      "train loss:0.0019610711693344935\n",
      "train loss:0.0010744482750621625\n",
      "train loss:0.00240665445751575\n",
      "train loss:0.002315511621247\n",
      "train loss:0.000657685130458183\n",
      "train loss:0.0006314722296415807\n",
      "train loss:0.0008899066981126398\n",
      "train loss:0.0004749057459144028\n",
      "train loss:0.0005062927714623891\n",
      "train loss:2.8702059605691443e-05\n",
      "train loss:0.00021904308559870546\n",
      "train loss:0.0008355905215490316\n",
      "train loss:0.0011999540796581744\n",
      "train loss:0.002000197762402731\n",
      "train loss:0.00017934168869730187\n",
      "train loss:0.00020670256275543135\n",
      "train loss:0.00015127055818992986\n",
      "train loss:4.531223037207462e-05\n",
      "train loss:6.65587034106623e-05\n",
      "train loss:0.0009442675020422778\n",
      "train loss:0.0023245410778376416\n",
      "train loss:2.808117119204211e-05\n",
      "train loss:0.001816480867389493\n",
      "train loss:0.00028435802324360254\n",
      "train loss:0.02300657035268384\n",
      "train loss:0.00016876241188084864\n",
      "train loss:0.0022180135405656894\n",
      "train loss:0.00011886805334251084\n",
      "train loss:0.0001816549620559585\n",
      "train loss:0.002968288736203017\n",
      "train loss:0.0009195997442776893\n",
      "train loss:0.0007981237124171079\n",
      "train loss:0.00013964307988854582\n",
      "train loss:0.00033796038302652174\n",
      "train loss:0.0006493700464960829\n",
      "train loss:0.0007625847746945576\n",
      "train loss:0.0022957436602678762\n",
      "train loss:0.0002790917970996771\n",
      "train loss:0.0003016755024835706\n",
      "train loss:0.000296361119373026\n",
      "train loss:0.00028011322589092805\n",
      "train loss:0.0018098725726065276\n",
      "train loss:0.0001579189753002345\n",
      "train loss:0.0006316990273197879\n",
      "train loss:0.0001252518333181158\n",
      "train loss:2.9671290689915648e-05\n",
      "train loss:0.000165260117869063\n",
      "train loss:0.0009812424376056126\n",
      "train loss:0.000765339894132909\n",
      "train loss:0.00011417333037132297\n",
      "train loss:2.0465011267858257e-05\n",
      "train loss:0.0007755445094510653\n",
      "train loss:0.0002939221026957875\n",
      "train loss:0.00039335977393662147\n",
      "train loss:0.0015847190380948945\n",
      "train loss:0.0004760510044482908\n",
      "train loss:0.0005249993472095372\n",
      "train loss:0.00037067950855660997\n",
      "train loss:5.048571425837815e-05\n",
      "train loss:0.0014855430576012686\n",
      "train loss:0.003031615529151599\n",
      "train loss:0.00031382418449111437\n",
      "train loss:0.0013409309410609641\n",
      "train loss:0.00014896290359663205\n",
      "train loss:0.0014705195044940691\n",
      "train loss:0.0006043363762316384\n",
      "train loss:7.424970878988606e-05\n",
      "train loss:0.0008958098606202715\n",
      "train loss:9.879050945778005e-05\n",
      "train loss:0.0022642011074583933\n",
      "train loss:0.0001719369324908359\n",
      "train loss:6.47397399894717e-05\n",
      "train loss:0.00011420447733127246\n",
      "train loss:0.00029063352578783644\n",
      "train loss:0.001780298254456038\n",
      "train loss:0.006530355453725933\n",
      "train loss:0.0006746238850837222\n",
      "train loss:0.00042531980602781953\n",
      "train loss:0.0004108549572923916\n",
      "train loss:0.002307887822125347\n",
      "train loss:1.0770182405932552e-05\n",
      "train loss:0.0017293646061397885\n",
      "train loss:0.00560907016697668\n",
      "train loss:0.006808264703520584\n",
      "train loss:6.745368723108074e-05\n",
      "train loss:0.00017070236226864266\n",
      "train loss:0.0001684529764173046\n",
      "train loss:0.0028480021973941137\n",
      "train loss:0.0013612558363767426\n",
      "train loss:0.0008159887955042441\n",
      "train loss:0.0017075469379685452\n",
      "train loss:0.0006493377801922956\n",
      "train loss:0.0002002474592611227\n",
      "train loss:0.0007086532330414057\n",
      "train loss:0.0011167566101691995\n",
      "train loss:0.0012881532948549026\n",
      "train loss:0.0001506919795486033\n",
      "train loss:0.00016085185138450993\n",
      "train loss:7.76026385731197e-05\n",
      "train loss:0.001120253794378104\n",
      "train loss:5.216921333059366e-05\n",
      "train loss:0.0002557832995843674\n",
      "train loss:0.000576067113844853\n",
      "train loss:0.00015475512112078638\n",
      "train loss:2.0607881171447784e-05\n",
      "train loss:0.0015476714182724308\n",
      "train loss:4.687626520462641e-05\n",
      "train loss:0.0002297045540165407\n",
      "train loss:0.0009243924659365692\n",
      "train loss:0.0002069464553623469\n",
      "train loss:0.0012627226686099536\n",
      "train loss:0.0006538652420995117\n",
      "train loss:0.0021449983537863058\n",
      "train loss:0.0005171868764449906\n",
      "train loss:0.0009658043957237919\n",
      "train loss:0.0003590454623445513\n",
      "train loss:0.0018135696269298797\n",
      "train loss:8.925325999023359e-05\n",
      "train loss:0.0026825795340710496\n",
      "train loss:0.00041991569953117434\n",
      "train loss:3.495575706004424e-05\n",
      "train loss:0.00037383036548272857\n",
      "train loss:2.305926881566173e-05\n",
      "train loss:0.00025655138903062216\n",
      "train loss:0.00036890633604584784\n",
      "train loss:0.0005646074542430847\n",
      "train loss:0.0011446495238738212\n",
      "train loss:0.0015949268169795734\n",
      "train loss:0.00013403438846048594\n",
      "train loss:0.0005883233972595317\n",
      "train loss:0.0022116622001211684\n",
      "train loss:0.0005910033068896164\n",
      "train loss:0.0004087423974150739\n",
      "train loss:0.0001706277121675517\n",
      "train loss:0.00011484236734598982\n",
      "train loss:0.00010674520956835531\n",
      "train loss:0.002919278776539441\n",
      "train loss:8.182127815323397e-05\n",
      "train loss:0.001295371375679516\n",
      "train loss:0.0020985360324958656\n",
      "train loss:0.0015145401818075482\n",
      "train loss:3.4491576718019185e-05\n",
      "train loss:0.004860023462091694\n",
      "train loss:0.0019487972046803228\n",
      "train loss:6.290896063594563e-05\n",
      "train loss:0.0004290879622731198\n",
      "train loss:0.0016146047519490067\n",
      "train loss:0.00012527617075192127\n",
      "train loss:0.003393718933627897\n",
      "train loss:0.0005723173778871265\n",
      "train loss:0.0005365504859667234\n",
      "train loss:3.209282527859611e-05\n",
      "train loss:0.00017046774932567176\n",
      "train loss:0.0022368231591224914\n",
      "train loss:0.0002795158289518386\n",
      "train loss:0.0015060417675696219\n",
      "train loss:8.531533834650615e-05\n",
      "train loss:0.0012737743956006782\n",
      "train loss:0.0005980290981486753\n",
      "train loss:0.0012241412023223685\n",
      "train loss:0.00037645501457682064\n",
      "train loss:0.00042552806133272486\n",
      "train loss:0.0016770828545483773\n",
      "train loss:0.0022699762913354123\n",
      "train loss:0.00046179465887800366\n",
      "train loss:0.002067114108058087\n",
      "train loss:0.0003267541255523549\n",
      "train loss:0.000895772257482065\n",
      "train loss:0.0016780571336422261\n",
      "train loss:6.0926191097331114e-05\n",
      "train loss:0.0014382379235253673\n",
      "train loss:0.001987492488435943\n",
      "train loss:0.0010462387735796928\n",
      "train loss:0.0017129100895936918\n",
      "train loss:2.5305552166473778e-05\n",
      "train loss:0.00022492717269053918\n",
      "train loss:0.00029421177814106233\n",
      "train loss:0.0002266403577144456\n",
      "train loss:0.00028164275811056\n",
      "train loss:0.0007217253281727192\n",
      "train loss:0.0008899040673770885\n",
      "train loss:0.00022358642551005027\n",
      "train loss:0.0006495767370289533\n",
      "train loss:0.00013519066020450935\n",
      "train loss:0.0001167967937201135\n",
      "train loss:0.0001077423770921552\n",
      "train loss:0.0001464208545625101\n",
      "train loss:0.0012792519152108536\n",
      "train loss:0.0005176042345027411\n",
      "train loss:0.00013389321816973084\n",
      "train loss:0.0009002984766559615\n",
      "train loss:0.00036249722346961806\n",
      "train loss:0.00040897532782050626\n",
      "train loss:0.00022969676666532329\n",
      "train loss:0.0013154982818576217\n",
      "train loss:0.0009063583474732198\n",
      "train loss:0.00019580434289423265\n",
      "train loss:0.0006220891092258414\n",
      "train loss:0.0010056837950091543\n",
      "train loss:0.001055286499801239\n",
      "train loss:0.0002566565945789305\n",
      "train loss:5.3615628221944125e-05\n",
      "train loss:0.0006215204387873406\n",
      "train loss:0.006197541662129344\n",
      "train loss:0.001152327648692953\n",
      "train loss:0.00040436790934261156\n",
      "train loss:0.1621275468363487\n",
      "train loss:8.310205505157341e-05\n",
      "train loss:0.00019070771352083186\n",
      "train loss:0.0006199070279422425\n",
      "train loss:0.003126144654408145\n",
      "train loss:0.0024993933901987886\n",
      "train loss:0.001062624147318868\n",
      "train loss:0.00045334601605702103\n",
      "train loss:0.0006319086544528973\n",
      "train loss:0.0006453336958684788\n",
      "train loss:0.00043791966025831015\n",
      "train loss:0.0008562579028215427\n",
      "train loss:0.00015042632880932813\n",
      "train loss:0.0002458346844622822\n",
      "train loss:0.00029952955486264366\n",
      "train loss:0.0007352895840981966\n",
      "train loss:0.0005054305266658549\n",
      "train loss:0.00012635094569178552\n",
      "train loss:0.001107953498658505\n",
      "train loss:0.00019781695976305987\n",
      "train loss:1.569309243422082e-05\n",
      "train loss:0.0006171952785183657\n",
      "train loss:0.0035851879293438194\n",
      "train loss:0.00011468426019212006\n",
      "train loss:0.0014411567953763048\n",
      "train loss:0.00021822432611635207\n",
      "train loss:0.0006364477406865218\n",
      "train loss:0.00013730778563214704\n",
      "train loss:0.036657798692279565\n",
      "train loss:0.003459514144806084\n",
      "train loss:0.0036386466940996624\n",
      "train loss:0.0012635826380631421\n",
      "train loss:0.0002140686080969747\n",
      "train loss:0.0014842039840543728\n",
      "train loss:0.005846876679953359\n",
      "train loss:0.0006678386837512049\n",
      "train loss:0.0014635196619678795\n",
      "train loss:0.0001491407618936734\n",
      "train loss:9.553961470478312e-05\n",
      "train loss:0.0011484885111183772\n",
      "train loss:0.0014527617716855415\n",
      "train loss:0.002846858707897175\n",
      "train loss:0.0013366718059823815\n",
      "train loss:0.0008240606300718067\n",
      "train loss:0.017714356072427943\n",
      "train loss:0.00042428607870240593\n",
      "train loss:0.0017340538140410125\n",
      "train loss:0.0014120416317212753\n",
      "train loss:0.001455657849744132\n",
      "train loss:3.840102471496885e-05\n",
      "train loss:0.002488190719537133\n",
      "train loss:0.0008265797909561264\n",
      "train loss:0.001756501602981242\n",
      "train loss:0.0009664981105464789\n",
      "train loss:0.003304623284111392\n",
      "train loss:0.005324733191919198\n",
      "train loss:0.0013521604979499385\n",
      "train loss:0.00041180361215631774\n",
      "train loss:0.01617172400884281\n",
      "train loss:0.0005380890922891856\n",
      "train loss:0.0007832536640388726\n",
      "train loss:0.0006086593558224739\n",
      "train loss:0.004573831339912539\n",
      "train loss:3.182606719495129e-05\n",
      "train loss:0.0002657343858671728\n",
      "train loss:0.0044405953575782494\n",
      "train loss:0.0015685436730830614\n",
      "train loss:0.0026489928399163034\n",
      "train loss:0.0003065081626044235\n",
      "train loss:0.0004239862227473994\n",
      "train loss:0.00014454986540532135\n",
      "train loss:0.00023293500479879727\n",
      "train loss:0.0010417103314189693\n",
      "train loss:0.007824167027940997\n",
      "train loss:0.00045182550114768064\n",
      "train loss:0.0010913977122165098\n",
      "train loss:0.001425873939928285\n",
      "train loss:0.0007840423613405515\n",
      "train loss:0.0014747891452783393\n",
      "train loss:0.0002837062948058051\n",
      "train loss:0.000419591949517909\n",
      "train loss:0.002010303759108902\n",
      "train loss:0.0031474923323632303\n",
      "train loss:0.0002565871203453847\n",
      "train loss:0.0021138966703970587\n",
      "train loss:0.004259105556295803\n",
      "train loss:0.0009192564638275084\n",
      "train loss:0.0032482870876614535\n",
      "train loss:0.0004456529151605522\n",
      "train loss:0.0011072364845844253\n",
      "train loss:0.0007389848510331879\n",
      "train loss:0.00030166665056930385\n",
      "train loss:0.0015055451375041622\n",
      "train loss:0.0011363580354863964\n",
      "train loss:0.0021558335152460652\n",
      "train loss:0.00687556931661817\n",
      "train loss:0.0009789384525171981\n",
      "train loss:0.011383033784739531\n",
      "train loss:0.0018081196508214466\n",
      "train loss:0.0015066423174116847\n",
      "train loss:0.004351382342796138\n",
      "train loss:0.006958994343339654\n",
      "train loss:0.0049753816981820635\n",
      "train loss:0.0013133671158338505\n",
      "train loss:0.00021149453938414534\n",
      "train loss:0.0039835294755860545\n",
      "train loss:0.004865185253923199\n",
      "train loss:0.0026017494799236533\n",
      "train loss:0.001306974855297906\n",
      "train loss:0.0033961753917576194\n",
      "train loss:0.0032249241347443057\n",
      "train loss:0.001617545143607087\n",
      "train loss:0.0014507532488486539\n",
      "train loss:0.001115131594820446\n",
      "train loss:0.004944754576321099\n",
      "train loss:0.0013222444135051908\n",
      "train loss:0.00034998065517386677\n",
      "train loss:0.0002519743519922296\n",
      "train loss:0.0015834565476565245\n",
      "train loss:0.00252776755979394\n",
      "train loss:0.0016645646811622786\n",
      "train loss:0.000701927888451555\n",
      "train loss:0.001004914931449452\n",
      "train loss:0.002244400992761169\n",
      "train loss:0.0006902217275608989\n",
      "train loss:0.023401227971235977\n",
      "train loss:0.00036412263953376414\n",
      "train loss:0.0059292280203725985\n",
      "train loss:0.004841774517617847\n",
      "train loss:0.0007695385731023222\n",
      "train loss:0.0012458909442185503\n",
      "train loss:0.0003644100678265665\n",
      "train loss:0.0002707984956760305\n",
      "train loss:0.003883533841254586\n",
      "train loss:0.020726678946880163\n",
      "train loss:0.0016647384765093043\n",
      "train loss:0.0003305372779679041\n",
      "train loss:6.277533319164765e-05\n",
      "train loss:0.0016560801162690422\n",
      "train loss:0.0050342186967910155\n",
      "train loss:0.0027203708133514857\n",
      "train loss:0.0018843125263622036\n",
      "train loss:0.0009508678885172719\n",
      "train loss:0.003686762976495522\n",
      "train loss:0.06212168018082908\n",
      "train loss:0.00701783170443121\n",
      "train loss:0.000386721425286486\n",
      "train loss:0.0026893566003937056\n",
      "train loss:0.0019424912523562782\n",
      "train loss:0.0018834579947911503\n",
      "train loss:0.0007332285520197123\n",
      "train loss:0.0015530355824820506\n",
      "train loss:0.0009243281667745132\n",
      "train loss:0.002554755967619114\n",
      "train loss:0.0021123046568154854\n",
      "train loss:0.0007689488347949261\n",
      "train loss:0.0006612782361216697\n",
      "train loss:0.003173202812551597\n",
      "train loss:0.0021783688863493637\n",
      "train loss:0.001895613132844382\n",
      "train loss:0.004391214109205479\n",
      "train loss:0.0026582091136037483\n",
      "train loss:0.002544215228662363\n",
      "train loss:0.0006597757056405891\n",
      "train loss:0.00044602314105598214\n",
      "train loss:0.0008451926968224733\n",
      "train loss:0.009349688477951342\n",
      "train loss:0.00026422182658834744\n",
      "train loss:0.01670934569895488\n",
      "train loss:0.0028814280586394064\n",
      "train loss:0.0014962657250293812\n",
      "train loss:0.000988089601092766\n",
      "train loss:0.000829685693565872\n",
      "train loss:0.00025121068228407145\n",
      "train loss:0.0003799439546320299\n",
      "train loss:0.0036193015985439936\n",
      "train loss:0.0018329617294356613\n",
      "train loss:0.004358539632884328\n",
      "train loss:0.00503454968899481\n",
      "train loss:0.0007665311421648225\n",
      "train loss:0.0008891921801887577\n",
      "train loss:0.0014887123666317754\n",
      "train loss:0.004904539197199906\n",
      "train loss:0.0019907669586522866\n",
      "train loss:0.007123719872161058\n",
      "train loss:0.0028944813915425293\n",
      "train loss:0.0006675733591986774\n",
      "train loss:0.0010237679054818235\n",
      "train loss:0.002389241348272707\n",
      "train loss:3.559506873681247e-05\n",
      "train loss:0.0033886111016480504\n",
      "train loss:0.0026192020948417796\n",
      "train loss:0.0008374481692254006\n",
      "train loss:0.007423366440354086\n",
      "train loss:0.00024642140792906735\n",
      "train loss:0.0009779722132225166\n",
      "train loss:9.282599782399072e-05\n",
      "train loss:0.0018348559482420786\n",
      "train loss:0.0012692489930168785\n",
      "train loss:4.7850181362134306e-05\n",
      "train loss:0.0011311317007597545\n",
      "train loss:0.00961941514195037\n",
      "=== epoch:19, train acc:0.998, test acc:0.984 ===\n",
      "train loss:0.002002134008866551\n",
      "train loss:0.0003886092703161735\n",
      "train loss:0.0005613956080153254\n",
      "train loss:0.0015209607642436402\n",
      "train loss:0.0011069256336494373\n",
      "train loss:0.0012178281650554292\n",
      "train loss:0.0134309715396543\n",
      "train loss:0.0006867408871886976\n",
      "train loss:6.951194695413336e-06\n",
      "train loss:0.007330111703621998\n",
      "train loss:0.000738387041004967\n",
      "train loss:0.002789592154712567\n",
      "train loss:0.007561010705900221\n",
      "train loss:0.00016551184069726852\n",
      "train loss:0.0030351795257338695\n",
      "train loss:0.0027033837288496097\n",
      "train loss:0.00532856685451184\n",
      "train loss:0.0012701716301507556\n",
      "train loss:0.00027744374866074586\n",
      "train loss:0.0014212762187266652\n",
      "train loss:0.0027331482769206977\n",
      "train loss:0.00020390390550532184\n",
      "train loss:0.0007459722519471014\n",
      "train loss:0.002473056330995033\n",
      "train loss:0.001499546587904307\n",
      "train loss:0.0008858629517998896\n",
      "train loss:0.00021743756961938542\n",
      "train loss:0.0001299513771860516\n",
      "train loss:0.006820941226952013\n",
      "train loss:9.181744321321855e-05\n",
      "train loss:0.00024290902939312853\n",
      "train loss:0.002068681453335815\n",
      "train loss:0.00048825701567129697\n",
      "train loss:0.00022734993882405206\n",
      "train loss:0.0001841879415909036\n",
      "train loss:0.002687813680775191\n",
      "train loss:0.002229098550860049\n",
      "train loss:0.007638243622383793\n",
      "train loss:5.87192483836439e-05\n",
      "train loss:0.00117673963303669\n",
      "train loss:0.004241165259695555\n",
      "train loss:0.001606101771807223\n",
      "train loss:7.371612181130602e-05\n",
      "train loss:0.0008930474269944152\n",
      "train loss:0.0021298313758910388\n",
      "train loss:6.266246782573448e-05\n",
      "train loss:0.0014027684161164286\n",
      "train loss:0.001174351118484445\n",
      "train loss:0.0018868829498775808\n",
      "train loss:0.0013871953464833946\n",
      "train loss:0.0007325259708528595\n",
      "train loss:0.0005657424046331927\n",
      "train loss:0.01829285491146544\n",
      "train loss:0.005331405521096553\n",
      "train loss:0.011838129828135584\n",
      "train loss:0.0009228587703363345\n",
      "train loss:0.004845178316660181\n",
      "train loss:0.0014603273110251507\n",
      "train loss:0.0004854904021437643\n",
      "train loss:0.0017650898489625908\n",
      "train loss:0.0032733107616358882\n",
      "train loss:0.0006659076062188704\n",
      "train loss:0.0031989790467635156\n",
      "train loss:0.006109291820326445\n",
      "train loss:0.0021114508773527924\n",
      "train loss:0.0003609422112892394\n",
      "train loss:8.430200496540284e-05\n",
      "train loss:0.000406123748918814\n",
      "train loss:0.001208500518186537\n",
      "train loss:0.0006342197293707323\n",
      "train loss:0.010131331076147203\n",
      "train loss:0.002191611572190243\n",
      "train loss:6.312703715606638e-05\n",
      "train loss:0.00016443742665734988\n",
      "train loss:0.00028141505476831497\n",
      "train loss:0.0015548847041447066\n",
      "train loss:0.0022081052615119607\n",
      "train loss:0.00039269874363196636\n",
      "train loss:0.00021782898459548895\n",
      "train loss:0.0014852627635478404\n",
      "train loss:0.0028872091808661723\n",
      "train loss:0.0019043973424451722\n",
      "train loss:0.00013946581858427048\n",
      "train loss:0.00022996183283740837\n",
      "train loss:0.0013128576458823753\n",
      "train loss:0.005112075938865671\n",
      "train loss:0.0035399309515634953\n",
      "train loss:0.00019257520299662573\n",
      "train loss:0.00310494786528826\n",
      "train loss:1.6497971283208072e-05\n",
      "train loss:0.0015127175094727088\n",
      "train loss:0.0001909680883718132\n",
      "train loss:0.0011249515715501064\n",
      "train loss:2.42385225732774e-05\n",
      "train loss:0.0007074901884411869\n",
      "train loss:3.409293653832422e-05\n",
      "train loss:0.0015505826486819161\n",
      "train loss:0.0012981968833271115\n",
      "train loss:0.0004539708476023098\n",
      "train loss:0.0007911058603848671\n",
      "train loss:0.0022995165103680197\n",
      "train loss:0.004555484086767173\n",
      "train loss:5.869118076332312e-05\n",
      "train loss:0.0016209205473779026\n",
      "train loss:0.00021084600887499584\n",
      "train loss:0.0015618222845060431\n",
      "train loss:0.0019110803574159954\n",
      "train loss:0.0008430297305800852\n",
      "train loss:0.0017578821276127937\n",
      "train loss:0.0011442025326717715\n",
      "train loss:0.0025088899988753833\n",
      "train loss:0.0009418296006885542\n",
      "train loss:0.0009695349925215179\n",
      "train loss:0.001165119137695838\n",
      "train loss:0.0021117605340909996\n",
      "train loss:0.0007225625523897605\n",
      "train loss:0.0036365325450209885\n",
      "train loss:0.00029857897314153553\n",
      "train loss:3.277722531289179e-05\n",
      "train loss:0.0015473709587201017\n",
      "train loss:0.00013316780987157584\n",
      "train loss:0.00016498630769637413\n",
      "train loss:0.0024845412256617734\n",
      "train loss:0.0002336764319279279\n",
      "train loss:0.003648124217921056\n",
      "train loss:0.0015855900026106246\n",
      "train loss:0.001698352138425212\n",
      "train loss:0.013946432213217208\n",
      "train loss:0.0028494902099376607\n",
      "train loss:0.002793083610794454\n",
      "train loss:0.00026466278293778457\n",
      "train loss:7.400959594984273e-05\n",
      "train loss:0.0002244401481447487\n",
      "train loss:0.0008992801450690066\n",
      "train loss:0.0001043533946264296\n",
      "train loss:0.00011619705603837941\n",
      "train loss:0.0002520246576109433\n",
      "train loss:0.0008312259484577881\n",
      "train loss:0.0007626987741697834\n",
      "train loss:0.0002683369932825473\n",
      "train loss:0.0006457196824807053\n",
      "train loss:0.00031577699106054264\n",
      "train loss:0.0003062680393658073\n",
      "train loss:0.001019556733514299\n",
      "train loss:0.00013758319727433265\n",
      "train loss:0.024231286449095037\n",
      "train loss:0.0025796400062842034\n",
      "train loss:0.00013873314445177843\n",
      "train loss:0.000345574142004472\n",
      "train loss:0.0005612711454630578\n",
      "train loss:0.0006898472595223565\n",
      "train loss:3.985031954814478e-05\n",
      "train loss:0.00030028147419536495\n",
      "train loss:0.0005256284912779819\n",
      "train loss:0.00032948105038757043\n",
      "train loss:0.0003869435038868902\n",
      "train loss:0.0006811562798661854\n",
      "train loss:0.0022295975941246694\n",
      "train loss:0.0017451656761905163\n",
      "train loss:0.0016057310124296886\n",
      "train loss:0.001943669871672244\n",
      "train loss:0.0002974411939308143\n",
      "train loss:0.0005083969893496878\n",
      "train loss:0.0006687373876907542\n",
      "train loss:0.0010114843059725006\n",
      "train loss:0.002467900974469951\n",
      "train loss:0.0009385446500119945\n",
      "train loss:6.830360285964594e-05\n",
      "train loss:0.000497291900939461\n",
      "train loss:0.00021020873529589798\n",
      "train loss:0.0009153213418491926\n",
      "train loss:6.846483314505176e-05\n",
      "train loss:3.723919094069595e-05\n",
      "train loss:0.0017694157666964811\n",
      "train loss:0.0022192100889155413\n",
      "train loss:0.001170666927766443\n",
      "train loss:0.015640285985753913\n",
      "train loss:0.00033761369795770645\n",
      "train loss:0.00031359632564334684\n",
      "train loss:0.004456178800606334\n",
      "train loss:0.0027777629635492847\n",
      "train loss:0.00035434914301187877\n",
      "train loss:0.0010637956890598933\n",
      "train loss:0.00016065638291112201\n",
      "train loss:0.0025911850731602736\n",
      "train loss:0.0009643763039398083\n",
      "train loss:0.00011929124263101996\n",
      "train loss:0.0019401884182961117\n",
      "train loss:0.00031200197539380436\n",
      "train loss:5.04680512849084e-05\n",
      "train loss:0.002398563338288889\n",
      "train loss:8.528827864237067e-05\n",
      "train loss:0.00040369202928057117\n",
      "train loss:0.030732135968660822\n",
      "train loss:0.0002230376889055555\n",
      "train loss:0.002985270778498717\n",
      "train loss:0.001714091679863706\n",
      "train loss:0.00046967754623730116\n",
      "train loss:0.00019471713480391037\n",
      "train loss:0.002791221512155759\n",
      "train loss:0.0018267489496955622\n",
      "train loss:8.994922911991898e-05\n",
      "train loss:0.0005787814160641058\n",
      "train loss:0.001885966148440357\n",
      "train loss:0.0004055039997320164\n",
      "train loss:0.0004008215352611679\n",
      "train loss:6.829886582192384e-05\n",
      "train loss:0.00020143172652368175\n",
      "train loss:6.911017414114163e-05\n",
      "train loss:0.00029301569578026856\n",
      "train loss:0.00029954524420404736\n",
      "train loss:4.685132868622758e-05\n",
      "train loss:0.0005413177136711781\n",
      "train loss:0.0009827775940828636\n",
      "train loss:0.000524749672437882\n",
      "train loss:0.00018390482942449575\n",
      "train loss:0.0005731509185716255\n",
      "train loss:8.208897191943625e-05\n",
      "train loss:0.0005180610567042344\n",
      "train loss:0.0009718660900095669\n",
      "train loss:0.00016800791250995938\n",
      "train loss:0.0007114806410600786\n",
      "train loss:0.0014661145663635397\n",
      "train loss:0.00025876460990758356\n",
      "train loss:0.000748133222791321\n",
      "train loss:0.0025713716191513203\n",
      "train loss:0.0041375586887442166\n",
      "train loss:0.0008842544119674672\n",
      "train loss:0.001093527766435997\n",
      "train loss:0.0007341673820577384\n",
      "train loss:0.001922470266267829\n",
      "train loss:0.00021809324984583902\n",
      "train loss:0.0022504315690713276\n",
      "train loss:0.0005712634642026549\n",
      "train loss:2.904861727859009e-05\n",
      "train loss:0.0002950254322360758\n",
      "train loss:0.00014054143347228012\n",
      "train loss:0.0018604274491198433\n",
      "train loss:0.0004137127286212532\n",
      "train loss:0.00026831821883814766\n",
      "train loss:0.00012361392633409822\n",
      "train loss:0.0004957064268274334\n",
      "train loss:0.001290954789447533\n",
      "train loss:0.001474714952838442\n",
      "train loss:0.0010194338479486763\n",
      "train loss:7.722903186946975e-05\n",
      "train loss:0.00038197894356472065\n",
      "train loss:0.0010182627576527973\n",
      "train loss:0.001296435292461822\n",
      "train loss:0.00014786716914971579\n",
      "train loss:1.776835115030206e-05\n",
      "train loss:6.346858293625167e-05\n",
      "train loss:0.00035514131822675496\n",
      "train loss:7.254682318013854e-05\n",
      "train loss:0.002339172560194675\n",
      "train loss:0.0012820198858214183\n",
      "train loss:0.001884893224546467\n",
      "train loss:0.00026449164257724846\n",
      "train loss:0.00030395559858080803\n",
      "train loss:6.665040429980856e-05\n",
      "train loss:0.02397802423382652\n",
      "train loss:0.00010507335095283484\n",
      "train loss:0.0006671627374019946\n",
      "train loss:0.0001534561564252425\n",
      "train loss:0.003616711097019\n",
      "train loss:0.00014179970260106916\n",
      "train loss:0.000739230167927312\n",
      "train loss:0.0012523072787312434\n",
      "train loss:0.0033288857752440592\n",
      "train loss:0.014791744424559961\n",
      "train loss:0.002803526757109949\n",
      "train loss:0.005844096691336758\n",
      "train loss:0.0018629127507758924\n",
      "train loss:0.0006940599362221276\n",
      "train loss:0.0001976260181126614\n",
      "train loss:0.007749383655440737\n",
      "train loss:0.007014994300978589\n",
      "train loss:0.00045888278027992696\n",
      "train loss:0.00148284388710201\n",
      "train loss:0.0015156492165535153\n",
      "train loss:0.004745464660112943\n",
      "train loss:0.0010213014745467123\n",
      "train loss:0.0025279527908336347\n",
      "train loss:0.0004733452833540808\n",
      "train loss:0.004906841292780435\n",
      "train loss:0.00654723618971807\n",
      "train loss:0.0002097908894388011\n",
      "train loss:0.0011614533912431822\n",
      "train loss:0.0012148903329017677\n",
      "train loss:0.0004404595873716238\n",
      "train loss:0.005773136462754701\n",
      "train loss:0.001295767723305281\n",
      "train loss:0.0037455764071928645\n",
      "train loss:0.00015276633287527046\n",
      "train loss:0.0008552627946610304\n",
      "train loss:0.04452075556323685\n",
      "train loss:0.0009405243037205928\n",
      "train loss:0.0011655849251342026\n",
      "train loss:2.1123163906763105e-05\n",
      "train loss:5.597240621087046e-05\n",
      "train loss:9.487893012815135e-05\n",
      "train loss:0.0009200016392668351\n",
      "train loss:0.00019868350764481812\n",
      "train loss:0.00580112793803032\n",
      "train loss:0.00010712590436400762\n",
      "train loss:0.0006998878864758484\n",
      "train loss:0.00170343891644353\n",
      "train loss:0.0029648980800243262\n",
      "train loss:0.0029542372661099116\n",
      "train loss:0.0006092974888597932\n",
      "train loss:0.001272562213843312\n",
      "train loss:0.0010429196402851808\n",
      "train loss:0.002416429691164116\n",
      "train loss:0.0008461305648878589\n",
      "train loss:0.0003581965640442257\n",
      "train loss:0.0008069404712179328\n",
      "train loss:0.0009869989266168705\n",
      "train loss:0.011198982107859341\n",
      "train loss:0.0023965933265413008\n",
      "train loss:0.0002947185527629655\n",
      "train loss:0.0012891587288122483\n",
      "train loss:0.0024455946714523917\n",
      "train loss:0.0022731265193054293\n",
      "train loss:0.00873227805059385\n",
      "train loss:0.00032112401113707856\n",
      "train loss:5.7381131428576035e-05\n",
      "train loss:0.0007451037133889258\n",
      "train loss:0.0006828339275162064\n",
      "train loss:0.0006436585781803952\n",
      "train loss:0.00028587789430782937\n",
      "train loss:0.000132673002813271\n",
      "train loss:0.0011397763187994828\n",
      "train loss:0.0012181463595278112\n",
      "train loss:0.005045454295366189\n",
      "train loss:0.001270139615721264\n",
      "train loss:0.0008870529228849831\n",
      "train loss:0.006935865415723458\n",
      "train loss:0.004132438082510646\n",
      "train loss:0.0009273656464665211\n",
      "train loss:0.0017925294442758387\n",
      "train loss:0.0002959805484642708\n",
      "train loss:0.00229756882354894\n",
      "train loss:0.0026243397238604953\n",
      "train loss:0.0016537832764571257\n",
      "train loss:0.003046974844167696\n",
      "train loss:0.002322442247805361\n",
      "train loss:0.003788869619706282\n",
      "train loss:0.001868198105949439\n",
      "train loss:0.0021821574246962037\n",
      "train loss:0.0005566979617191079\n",
      "train loss:0.0003276409529021213\n",
      "train loss:1.1005175370238388e-05\n",
      "train loss:0.0004501258016203638\n",
      "train loss:0.0007691978649716415\n",
      "train loss:0.0030083070177836313\n",
      "train loss:0.00029375132642318696\n",
      "train loss:0.0003478183630139896\n",
      "train loss:0.003140217863219397\n",
      "train loss:0.0006633822080068539\n",
      "train loss:0.020124619458919045\n",
      "train loss:0.027217531461702315\n",
      "train loss:0.0034095213728766813\n",
      "train loss:0.0017758098621569456\n",
      "train loss:0.0007122783782782006\n",
      "train loss:0.00029036233746346815\n",
      "train loss:9.630369969441112e-05\n",
      "train loss:0.00397085862476112\n",
      "train loss:0.0003514113931775589\n",
      "train loss:0.03359204308977299\n",
      "train loss:0.0023841655686024864\n",
      "train loss:0.0005254189497827235\n",
      "train loss:0.0002139109034626427\n",
      "train loss:0.0009624820547722866\n",
      "train loss:0.0006662401796657565\n",
      "train loss:0.0006950672350373298\n",
      "train loss:0.0017926965000420992\n",
      "train loss:0.000862739532191449\n",
      "train loss:0.00011912953456164941\n",
      "train loss:0.0018955759261126185\n",
      "train loss:0.0011816716229819072\n",
      "train loss:0.0014993091433836991\n",
      "train loss:0.00275240781309142\n",
      "train loss:0.016051631520250634\n",
      "train loss:0.002796061450233392\n",
      "train loss:0.0005669802358314295\n",
      "train loss:0.00010275809594975152\n",
      "train loss:0.0002674704799646989\n",
      "train loss:0.010908401089272015\n",
      "train loss:0.0009641745884332101\n",
      "train loss:0.0011271153290999772\n",
      "train loss:0.0005502695984703266\n",
      "train loss:0.000703984528223151\n",
      "train loss:0.0001948608856830784\n",
      "train loss:0.02218413183567063\n",
      "train loss:0.0005134245445629173\n",
      "train loss:8.865008618440445e-05\n",
      "train loss:0.0014646007141114112\n",
      "train loss:0.00390514625652186\n",
      "train loss:0.003108268067585799\n",
      "train loss:0.0020676040028998733\n",
      "train loss:0.003830095876218796\n",
      "train loss:0.0011860313275615906\n",
      "train loss:0.00017894310080984128\n",
      "train loss:0.0024809072764534097\n",
      "train loss:0.0024398617534338645\n",
      "train loss:0.0009964166627224227\n",
      "train loss:0.0010479403729213574\n",
      "train loss:0.006691181805379738\n",
      "train loss:0.0019621726744671504\n",
      "train loss:0.0009726758897088898\n",
      "train loss:0.003658376242350624\n",
      "train loss:0.00035690275688609664\n",
      "train loss:0.0006614519626075163\n",
      "train loss:0.04268129788559607\n",
      "train loss:0.00023553615498627444\n",
      "train loss:0.0005521779673230358\n",
      "train loss:0.00046593295906344456\n",
      "train loss:0.00015001205558190826\n",
      "train loss:7.663857699963733e-05\n",
      "train loss:0.0003962605742985457\n",
      "train loss:0.0034285845933569037\n",
      "train loss:0.0016727387876780656\n",
      "train loss:0.004908919332120272\n",
      "train loss:0.005004138308329434\n",
      "train loss:0.00019160324557207864\n",
      "train loss:0.00010748139025404143\n",
      "train loss:0.0010664839259921738\n",
      "train loss:0.0025990472356228543\n",
      "train loss:0.00046778564166152976\n",
      "train loss:0.0024084851403885603\n",
      "train loss:0.0028135050551401047\n",
      "train loss:0.0013753150653272844\n",
      "train loss:0.0005582908889397937\n",
      "train loss:0.002959638274975682\n",
      "train loss:0.0022084323425091233\n",
      "train loss:0.0005392448850865372\n",
      "train loss:2.6219278410438974e-05\n",
      "train loss:0.0004361761118943401\n",
      "train loss:0.0020945539382538616\n",
      "train loss:0.0016270710693126334\n",
      "train loss:0.012579999844285403\n",
      "train loss:0.00020216550180996904\n",
      "train loss:0.0007292314266454777\n",
      "train loss:0.006111067392406354\n",
      "train loss:0.0004768471195778277\n",
      "train loss:0.00026002153240004195\n",
      "train loss:0.006728588558481765\n",
      "train loss:0.005581604481450904\n",
      "train loss:0.0001517569178494392\n",
      "train loss:0.000761511646874049\n",
      "train loss:0.007322031260602435\n",
      "train loss:7.659733322140194e-05\n",
      "train loss:0.0022787049910535945\n",
      "train loss:0.011823685752117348\n",
      "train loss:0.0007961748265686758\n",
      "train loss:0.006946350624152459\n",
      "train loss:0.006476958121574148\n",
      "train loss:0.0015157518131015807\n",
      "train loss:0.00017710282826047336\n",
      "train loss:0.0017939798094754105\n",
      "train loss:0.0007742053714924025\n",
      "train loss:0.003026287155693563\n",
      "train loss:0.00011434209422752686\n",
      "train loss:0.0007125478813861757\n",
      "train loss:0.00019399855792374542\n",
      "train loss:0.0011401455734303166\n",
      "train loss:5.1029725338533054e-05\n",
      "train loss:0.0009171143756988024\n",
      "train loss:0.00027528680978586864\n",
      "train loss:0.0003610142946349692\n",
      "train loss:0.0006929433287280775\n",
      "train loss:2.0033444964558312e-05\n",
      "train loss:0.00030525253480537446\n",
      "train loss:0.0002376867561906625\n",
      "train loss:0.0004925204112599455\n",
      "train loss:0.0010036045071226858\n",
      "train loss:0.003915891389983824\n",
      "train loss:0.00023073975591664825\n",
      "train loss:0.0022773092897081957\n",
      "train loss:0.00044476392081450904\n",
      "train loss:0.000300527775054001\n",
      "train loss:0.003039419928649561\n",
      "train loss:0.00016429927012366432\n",
      "train loss:0.00019856703123976048\n",
      "train loss:0.00014536384966195025\n",
      "train loss:0.0002862505120110842\n",
      "train loss:0.0005326787398722884\n",
      "train loss:0.004251658559660074\n",
      "train loss:0.0008732658654189089\n",
      "train loss:0.00012710109211584364\n",
      "train loss:0.0013363103208421168\n",
      "train loss:0.001960459667667612\n",
      "train loss:0.0001509236256963858\n",
      "train loss:0.0004487167764053312\n",
      "train loss:0.0012872758403343976\n",
      "train loss:0.00011582294075026135\n",
      "train loss:0.0007425940768461433\n",
      "train loss:8.31110958646519e-05\n",
      "train loss:0.0023603519171547255\n",
      "train loss:4.2635344579288186e-05\n",
      "train loss:7.013790733268295e-05\n",
      "train loss:0.0013245725875032325\n",
      "train loss:0.0007758474604486126\n",
      "train loss:0.0024549023712995364\n",
      "train loss:6.768003808077516e-05\n",
      "train loss:0.0005947519819299574\n",
      "train loss:9.300221998399779e-05\n",
      "train loss:0.001029708741014997\n",
      "train loss:0.0010205613938126246\n",
      "train loss:0.00013312530053620112\n",
      "train loss:0.0017980410998226853\n",
      "train loss:0.00011256536209347293\n",
      "train loss:0.0032197862896724046\n",
      "train loss:0.00034468079319633765\n",
      "train loss:2.616491184904082e-05\n",
      "train loss:0.00013213665817723763\n",
      "train loss:9.976406971071755e-05\n",
      "train loss:0.0074812244352550124\n",
      "train loss:0.0002699208026230939\n",
      "train loss:4.4841631361125815e-05\n",
      "train loss:0.0018741508792828295\n",
      "train loss:0.00039018751253170676\n",
      "train loss:0.01179860458580583\n",
      "train loss:9.124213308451223e-05\n",
      "train loss:0.0006659921758984075\n",
      "train loss:0.0040826922194594885\n",
      "train loss:0.0003699968568407729\n",
      "train loss:0.0018673370412305333\n",
      "train loss:0.008659948437438905\n",
      "train loss:0.00152920738151702\n",
      "train loss:0.00013672018498851778\n",
      "train loss:0.002260935375734758\n",
      "train loss:0.0009709938356699944\n",
      "train loss:0.00028887444221705806\n",
      "train loss:0.004078495011107952\n",
      "train loss:0.0014188617941912369\n",
      "train loss:0.002333936103167443\n",
      "train loss:4.3887125167889435e-05\n",
      "train loss:0.000417436457622775\n",
      "train loss:0.002236371970448474\n",
      "train loss:0.002235272830806837\n",
      "train loss:0.0003180277014676448\n",
      "train loss:0.0007795871479756812\n",
      "train loss:4.062710683974069e-05\n",
      "train loss:0.001694038205064312\n",
      "train loss:0.00021226536874563898\n",
      "train loss:0.003856316672563713\n",
      "train loss:0.0006569306925315734\n",
      "train loss:8.897673996273559e-05\n",
      "train loss:0.000818951124818221\n",
      "train loss:0.0004532808329543714\n",
      "train loss:0.0007891530320049572\n",
      "train loss:0.0002592323299881859\n",
      "train loss:0.0001741631772165161\n",
      "train loss:0.0013775516016337303\n",
      "train loss:0.0005535110103545487\n",
      "train loss:0.002024588886313122\n",
      "train loss:0.00010453309586000724\n",
      "train loss:0.000316475054425456\n",
      "train loss:0.00012996978243834362\n",
      "train loss:0.0006001604847230785\n",
      "train loss:0.001611255862336706\n",
      "train loss:0.0013229022790922926\n",
      "train loss:0.0008831349718245061\n",
      "train loss:0.00013047095872825248\n",
      "train loss:0.0016074648232478521\n",
      "train loss:0.0006689368472696157\n",
      "train loss:5.268974452448737e-05\n",
      "train loss:0.001374607788563874\n",
      "train loss:0.0005520442953564617\n",
      "train loss:0.0024800024445707264\n",
      "train loss:0.0009795846420180402\n",
      "train loss:0.00042757831465103035\n",
      "train loss:1.0769298878622873e-05\n",
      "train loss:0.0003879993599940393\n",
      "train loss:0.0004840401703899981\n",
      "train loss:0.013431976561137735\n",
      "train loss:0.0007254049945640867\n",
      "train loss:0.00021459165738798897\n",
      "train loss:4.08688033771415e-05\n",
      "train loss:0.00010606594069730019\n",
      "train loss:3.8947193543877334e-05\n",
      "train loss:0.002865931119416854\n",
      "train loss:9.307847283066405e-05\n",
      "train loss:7.676842099569833e-05\n",
      "train loss:0.00014984668768872324\n",
      "train loss:0.00010003673262776827\n",
      "train loss:0.0004432990667716566\n",
      "train loss:6.866600238435933e-05\n",
      "train loss:0.03678806087655615\n",
      "train loss:3.48775799507009e-05\n",
      "train loss:0.00025944779312314344\n",
      "train loss:0.00030125984289856985\n",
      "train loss:0.0010320476508350058\n",
      "train loss:3.434228686663592e-05\n",
      "train loss:0.00020967932651489745\n",
      "train loss:0.0009527454510015558\n",
      "train loss:0.0018036349323773656\n",
      "train loss:0.0013049013486343534\n",
      "train loss:0.00021659752786305474\n",
      "=== epoch:20, train acc:0.998, test acc:0.985 ===\n",
      "train loss:0.0009804069719836308\n",
      "train loss:0.0007613663748267978\n",
      "train loss:0.0008008603963732418\n",
      "train loss:0.0013325033435953898\n",
      "train loss:0.00015661322775215522\n",
      "train loss:0.0001711426555546241\n",
      "train loss:0.0009656139112712301\n",
      "train loss:0.0016112996989779303\n",
      "train loss:0.0012015215135957088\n",
      "train loss:0.0009811197774616885\n",
      "train loss:0.0013385461242999708\n",
      "train loss:0.0010837474747403038\n",
      "train loss:0.00029809552700292146\n",
      "train loss:0.0015980971532097916\n",
      "train loss:0.0011377656448288865\n",
      "train loss:0.0005058167532037806\n",
      "train loss:0.00021446108446251412\n",
      "train loss:0.0010079474554255121\n",
      "train loss:0.0009108884446889077\n",
      "train loss:0.0007500117169387295\n",
      "train loss:4.9247548676028234e-05\n",
      "train loss:0.0019868901816454012\n",
      "train loss:0.008022981474212976\n",
      "train loss:0.0003898761215393302\n",
      "train loss:0.0005609804312017558\n",
      "train loss:0.00011392404671574956\n",
      "train loss:0.0018843134850681825\n",
      "train loss:3.684785529494739e-05\n",
      "train loss:0.000237679195958738\n",
      "train loss:0.001373033123499368\n",
      "train loss:0.0008814570084439755\n",
      "train loss:0.003597921749472324\n",
      "train loss:0.0016469770472112841\n",
      "train loss:0.0009284478846292958\n",
      "train loss:0.0018399484409150502\n",
      "train loss:0.001462258499427117\n",
      "train loss:0.0014902164437461591\n",
      "train loss:0.0007481361579526989\n",
      "train loss:0.0021373670737376733\n",
      "train loss:0.0010756858440848893\n",
      "train loss:0.00022153762748350852\n",
      "train loss:0.11824933958616352\n",
      "train loss:0.006199565904089629\n",
      "train loss:0.00048054301762276625\n",
      "train loss:0.0062934959871368545\n",
      "train loss:0.0005364126603971994\n",
      "train loss:0.00018281006452276923\n",
      "train loss:0.0013716221834167489\n",
      "train loss:0.0005486998052903662\n",
      "train loss:0.00027496431625411534\n",
      "train loss:0.0007163314365376058\n",
      "train loss:0.0005999585458909654\n",
      "train loss:0.002426115157862641\n",
      "train loss:0.00022778880409032924\n",
      "train loss:0.0008428973425883379\n",
      "train loss:0.0005139347379081242\n",
      "train loss:0.00043403659158874994\n",
      "train loss:0.0023375463740247975\n",
      "train loss:0.0015468647994908713\n",
      "train loss:0.0011043603644521158\n",
      "train loss:0.0009128511447468339\n",
      "train loss:0.004007092843794444\n",
      "train loss:0.000953242864351346\n",
      "train loss:0.0018351145898195945\n",
      "train loss:0.0015215990340704784\n",
      "train loss:0.0006865334803168419\n",
      "train loss:0.001651977131607218\n",
      "train loss:0.0014058333794018634\n",
      "train loss:0.0037197768887473463\n",
      "train loss:0.0003955694595946876\n",
      "train loss:0.0019487086733864381\n",
      "train loss:0.0009355219594835388\n",
      "train loss:0.0025610287395576715\n",
      "train loss:0.0013518417050217425\n",
      "train loss:0.0018758551653130038\n",
      "train loss:0.0010997501897052867\n",
      "train loss:0.00806361877738576\n",
      "train loss:0.0002750011598527836\n",
      "train loss:0.001182260380435786\n",
      "train loss:6.912248248392867e-05\n",
      "train loss:0.00026488890464978587\n",
      "train loss:0.0009768984932459223\n",
      "train loss:8.625612091314917e-05\n",
      "train loss:0.0002210833862039821\n",
      "train loss:2.0189426265439663e-05\n",
      "train loss:0.0032897765335659257\n",
      "train loss:0.0017092470662374809\n",
      "train loss:0.0027897507346994125\n",
      "train loss:0.00021876709600593828\n",
      "train loss:0.0001306492712277404\n",
      "train loss:9.568789674415653e-05\n",
      "train loss:0.0026447727905703137\n",
      "train loss:0.0014388692304091839\n",
      "train loss:0.002571155883024842\n",
      "train loss:0.0005209350955503716\n",
      "train loss:0.007027884617256992\n",
      "train loss:0.00025380545520643604\n",
      "train loss:0.0009425547467603972\n",
      "train loss:0.0009538810086347869\n",
      "train loss:3.79250388166006e-05\n",
      "train loss:0.00018501959681912675\n",
      "train loss:0.0016303406842251306\n",
      "train loss:0.0007837015261067505\n",
      "train loss:0.0002173189482692061\n",
      "train loss:0.0010220801897067998\n",
      "train loss:0.003643591358100923\n",
      "train loss:0.00043902462317378606\n",
      "train loss:0.0005710734849223775\n",
      "train loss:0.00014625777184451805\n",
      "train loss:0.004063800604395904\n",
      "train loss:0.002385178610986349\n",
      "train loss:0.0016263166727740893\n",
      "train loss:0.00015319135503137269\n",
      "train loss:0.001275347242735806\n",
      "train loss:0.001063926151107598\n",
      "train loss:0.0004677602274932615\n",
      "train loss:0.00027539650545739183\n",
      "train loss:0.014313262104350992\n",
      "train loss:0.0012984903461091795\n",
      "train loss:0.0027693394711009512\n",
      "train loss:0.0006959757135120911\n",
      "train loss:0.004878239044567376\n",
      "train loss:0.0013891435551957442\n",
      "train loss:0.00024552072717932673\n",
      "train loss:0.0026217638539574885\n",
      "train loss:0.0010544772258562923\n",
      "train loss:0.0024641646205689283\n",
      "train loss:0.002308637259739728\n",
      "train loss:0.002760407200005019\n",
      "train loss:0.00016464394253809772\n",
      "train loss:0.0002755510566000491\n",
      "train loss:8.821225135546249e-05\n",
      "train loss:0.00023632777452222225\n",
      "train loss:0.0008004582397434973\n",
      "train loss:0.0011857797042322614\n",
      "train loss:3.672721146166681e-05\n",
      "train loss:0.0001982900393139942\n",
      "train loss:0.0006783594822368535\n",
      "train loss:0.0003054676253838242\n",
      "train loss:0.0009974877386976185\n",
      "train loss:0.0008424557026758233\n",
      "train loss:1.4715172343908776e-05\n",
      "train loss:0.00056624976312025\n",
      "train loss:0.0005756742134186626\n",
      "train loss:0.0001964499317834106\n",
      "train loss:0.000193564169943073\n",
      "train loss:0.0002842749031538204\n",
      "train loss:0.0028039255471458604\n",
      "train loss:0.003908286618817386\n",
      "train loss:0.00012877239880392105\n",
      "train loss:2.2885049249044857e-05\n",
      "train loss:0.0004395754870787462\n",
      "train loss:0.00018783347834145983\n",
      "train loss:0.02645801813706267\n",
      "train loss:0.0001662379459185366\n",
      "train loss:0.00025571666561556474\n",
      "train loss:0.0014136373502764185\n",
      "train loss:0.0003393969145534794\n",
      "train loss:0.00097104044886984\n",
      "train loss:0.0025116997763648423\n",
      "train loss:0.00025000227504427765\n",
      "train loss:0.0011216136468025122\n",
      "train loss:0.0020125945368697374\n",
      "train loss:7.89293631291135e-05\n",
      "train loss:7.272359675091682e-05\n",
      "train loss:0.0011022916995028734\n",
      "train loss:0.0005836568353206735\n",
      "train loss:0.0003489997764116232\n",
      "train loss:0.0035923504783464044\n",
      "train loss:0.0021827243712421412\n",
      "train loss:0.0007563626196390263\n",
      "train loss:0.00020459464982822002\n",
      "train loss:0.002513690974165\n",
      "train loss:0.00010448244914335304\n",
      "train loss:0.00026241641820182497\n",
      "train loss:4.710869007558265e-05\n",
      "train loss:0.00019273991404909982\n",
      "train loss:0.0011923426647747568\n",
      "train loss:0.000911274522385221\n",
      "train loss:7.372837259731105e-05\n",
      "train loss:2.0381813724795892e-05\n",
      "train loss:0.003184223195894898\n",
      "train loss:0.002090066008487009\n",
      "train loss:0.003181033722596752\n",
      "train loss:7.84163960949555e-05\n",
      "train loss:0.00022816330123465485\n",
      "train loss:0.004051721820841652\n",
      "train loss:0.00017446199716295626\n",
      "train loss:0.0007983660041864338\n",
      "train loss:0.0002510544453329258\n",
      "train loss:8.439721144841098e-05\n",
      "train loss:1.3038293153622648e-05\n",
      "train loss:0.0006505525889485502\n",
      "train loss:0.014147283019733607\n",
      "train loss:0.0005919136670024955\n",
      "train loss:0.005631607463889164\n",
      "train loss:0.00226904175348224\n",
      "train loss:0.00010112288117479159\n",
      "train loss:0.0010501794871014907\n",
      "train loss:0.0020183966169406336\n",
      "train loss:0.00027232196835686953\n",
      "train loss:0.0001277941588177353\n",
      "train loss:0.0038477860535481617\n",
      "train loss:0.0001395949700483837\n",
      "train loss:0.0011768873589342813\n",
      "train loss:0.0005934337671094879\n",
      "train loss:0.00035240049437059166\n",
      "train loss:0.0009402794651842823\n",
      "train loss:0.00013937678758028268\n",
      "train loss:0.0017590514963247048\n",
      "train loss:0.0008089637687132726\n",
      "train loss:6.90647717560225e-05\n",
      "train loss:0.0006475323915958796\n",
      "train loss:0.0010494319246261265\n",
      "train loss:8.499554384863814e-06\n",
      "train loss:0.00036121959843038784\n",
      "train loss:0.00038011303991094376\n",
      "train loss:0.0029139311102150695\n",
      "train loss:0.0009156575172181352\n",
      "train loss:0.0004341343277911547\n",
      "train loss:0.0006372258000407868\n",
      "train loss:8.398540969333318e-05\n",
      "train loss:0.0003155894459070687\n",
      "train loss:0.0017077720642815\n",
      "train loss:9.634845612640004e-05\n",
      "train loss:0.0001705842451696603\n",
      "train loss:0.0003727801805806994\n",
      "train loss:0.00021654014361872215\n",
      "train loss:0.00020062394675822215\n",
      "train loss:0.0013763075290501325\n",
      "train loss:0.00021430587543885707\n",
      "train loss:0.0013744570799552646\n",
      "train loss:6.155179546150444e-05\n",
      "train loss:0.0013685670489385598\n",
      "train loss:0.0007163631553664733\n",
      "train loss:0.0007734615347374523\n",
      "train loss:0.002243791877706487\n",
      "train loss:0.0006223397561938446\n",
      "train loss:0.0011153950238674704\n",
      "train loss:0.001085592262768595\n",
      "train loss:0.0007809815144532795\n",
      "train loss:0.0015091105009272473\n",
      "train loss:0.0006387337319913039\n",
      "train loss:0.0004662634760646109\n",
      "train loss:0.00035406095031213677\n",
      "train loss:0.0001762666260770098\n",
      "train loss:0.0011832261535060663\n",
      "train loss:0.0014186909541285375\n",
      "train loss:0.002461412778345578\n",
      "train loss:0.00027296327053113496\n",
      "train loss:0.0025365269658630557\n",
      "train loss:0.0024006475269204366\n",
      "train loss:0.0004578163060785509\n",
      "train loss:0.0011632696064446427\n",
      "train loss:0.0015012588705016496\n",
      "train loss:0.00024908680769676844\n",
      "train loss:0.005091199390981439\n",
      "train loss:0.007919576151923485\n",
      "train loss:0.0001815254411442815\n",
      "train loss:0.0006472152988183988\n",
      "train loss:0.0021224463548406057\n",
      "train loss:0.00021902390742213168\n",
      "train loss:2.0728602158386917e-05\n",
      "train loss:0.0037769072683628497\n",
      "train loss:0.012460013452832222\n",
      "train loss:0.0003553579887129428\n",
      "train loss:0.002362318926436652\n",
      "train loss:0.0010312820612838481\n",
      "train loss:0.0014888339854821242\n",
      "train loss:0.00036145179469272487\n",
      "train loss:0.000982127066823125\n",
      "train loss:0.010051055352118693\n",
      "train loss:0.0013458588234442385\n",
      "train loss:0.0029395651177571243\n",
      "train loss:0.0028113639861819034\n",
      "train loss:0.005523988732104972\n",
      "train loss:0.00035328624770355685\n",
      "train loss:0.001781737667301492\n",
      "train loss:0.0027082309067542098\n",
      "train loss:8.17765831400178e-05\n",
      "train loss:0.0012907744371305396\n",
      "train loss:0.0030088839106178523\n",
      "train loss:0.00021831297824378992\n",
      "train loss:0.00021883370140285692\n",
      "train loss:0.00024251768023841492\n",
      "train loss:0.003755016426032852\n",
      "train loss:0.00033365202215859764\n",
      "train loss:0.00023239664038757928\n",
      "train loss:0.005039837466045353\n",
      "train loss:0.0013366647365278892\n",
      "train loss:0.002189791941938991\n",
      "train loss:0.001477807875148507\n",
      "train loss:0.002371449996695823\n",
      "train loss:0.0023236776596103404\n",
      "train loss:5.944174090598693e-05\n",
      "train loss:0.0007026084310808143\n",
      "train loss:0.00011362419807630767\n",
      "train loss:2.9588417351697772e-05\n",
      "train loss:4.501117965276944e-05\n",
      "train loss:0.0014563128389748718\n",
      "train loss:0.001218672249524083\n",
      "train loss:0.0012817414575709194\n",
      "train loss:0.000286448002190074\n",
      "train loss:0.0007344903991415568\n",
      "train loss:7.908408550912312e-05\n",
      "train loss:0.0008295677392772965\n",
      "train loss:0.0011502056702585425\n",
      "train loss:0.00024099443722845281\n",
      "train loss:0.001440361094862463\n",
      "train loss:0.001244825221643148\n",
      "train loss:0.026679124809209465\n",
      "train loss:0.003608239199730166\n",
      "train loss:0.00016479069194572652\n",
      "train loss:0.0011887036456962776\n",
      "train loss:0.0018838000847267813\n",
      "train loss:6.464256742438342e-05\n",
      "train loss:0.0006635085817307505\n",
      "train loss:9.06390365119501e-06\n",
      "train loss:0.0024228315092119303\n",
      "train loss:0.002722900261235285\n",
      "train loss:0.003255225247659081\n",
      "train loss:0.0005119900234376788\n",
      "train loss:0.0005482901908518746\n",
      "train loss:0.0028099051720484923\n",
      "train loss:0.002772402880320601\n",
      "train loss:0.00016302006736648454\n",
      "train loss:0.0015481571882086749\n",
      "train loss:0.00020490911051512908\n",
      "train loss:0.0005489926608369044\n",
      "train loss:0.002917725477727669\n",
      "train loss:0.0010860274995216851\n",
      "train loss:0.014301281812176065\n",
      "train loss:0.00204769196969253\n",
      "train loss:0.0003111878139552371\n",
      "train loss:0.0003521675430772673\n",
      "train loss:0.0018808908881424616\n",
      "train loss:0.0013517357402692018\n",
      "train loss:0.0039594577279400914\n",
      "train loss:0.0030626756799897853\n",
      "train loss:0.0003500001734574596\n",
      "train loss:0.00019324340789710266\n",
      "train loss:0.0002126842911396148\n",
      "train loss:0.013070992398513573\n",
      "train loss:0.0003539012528165261\n",
      "train loss:0.003578738624738858\n",
      "train loss:0.001960498567277988\n",
      "train loss:8.113223601310075e-05\n",
      "train loss:0.0008528222705691893\n",
      "train loss:0.0006465681513640242\n",
      "train loss:0.005260623068648798\n",
      "train loss:0.0010319945958879387\n",
      "train loss:8.196350033959689e-05\n",
      "train loss:0.00038826715366410263\n",
      "train loss:0.0005340599123277881\n",
      "train loss:0.000584389116207786\n",
      "train loss:0.0005269802229797762\n",
      "train loss:0.0006326606831805083\n",
      "train loss:0.0005183730722535379\n",
      "train loss:0.0009163379506248386\n",
      "train loss:0.001663889905214464\n",
      "train loss:6.098144098885405e-05\n",
      "train loss:0.0010790187861296156\n",
      "train loss:7.790640556562538e-05\n",
      "train loss:0.002984202519112705\n",
      "train loss:9.398509679423806e-05\n",
      "train loss:0.004880317633993685\n",
      "train loss:0.0018016915343714334\n",
      "train loss:0.0016411790849698731\n",
      "train loss:2.0624426958811918e-05\n",
      "train loss:0.0004280115560431666\n",
      "train loss:0.0025222171093690366\n",
      "train loss:0.0033762302585659525\n",
      "train loss:0.00175426527667527\n",
      "train loss:0.00014292263604325222\n",
      "train loss:0.0010754733777523177\n",
      "train loss:9.567501936112242e-06\n",
      "train loss:0.009334886723974421\n",
      "train loss:3.0644618321102544e-05\n",
      "train loss:4.6478038040579954e-05\n",
      "train loss:0.0003275057752363116\n",
      "train loss:0.00034221856946578777\n",
      "train loss:0.0012249833506359263\n",
      "train loss:0.00014292685631371507\n",
      "train loss:0.0006919694245445906\n",
      "train loss:1.4901563667805097e-05\n",
      "train loss:0.0014720083008909202\n",
      "train loss:0.0010367258599462388\n",
      "train loss:0.00040728929563572496\n",
      "train loss:0.002365782201456802\n",
      "train loss:0.00034633260147888106\n",
      "train loss:0.0002067934016661833\n",
      "train loss:0.0002270738786557738\n",
      "train loss:5.6457601214519004e-05\n",
      "train loss:0.0002739318167700849\n",
      "train loss:9.4714589419657e-05\n",
      "train loss:0.0019293271409379495\n",
      "train loss:0.0021076679215491683\n",
      "train loss:0.00025863392169354216\n",
      "train loss:0.0010258903683308934\n",
      "train loss:0.0008893248195069596\n",
      "train loss:0.00012744743658436885\n",
      "train loss:0.0004500676826957681\n",
      "train loss:0.00013263515181102738\n",
      "train loss:0.0008626826887253159\n",
      "train loss:0.0008033613833161272\n",
      "train loss:0.0025957140568803033\n",
      "train loss:2.5828665777302928e-05\n",
      "train loss:0.000976262992292537\n",
      "train loss:0.00019866119117088396\n",
      "train loss:0.0002708619886733898\n",
      "train loss:0.0005235444016753462\n",
      "train loss:0.000756343714269689\n",
      "train loss:0.0006627275993718857\n",
      "train loss:0.0006540288441174719\n",
      "train loss:0.005948202455625507\n",
      "train loss:1.505260870390348e-06\n",
      "train loss:0.0009130380302781245\n",
      "train loss:6.509126842710971e-05\n",
      "train loss:0.003976897956828592\n",
      "train loss:0.00016832040188114318\n",
      "train loss:0.00018994068609025832\n",
      "train loss:0.0003639284008307611\n",
      "train loss:9.682369219811301e-05\n",
      "train loss:8.612429110593609e-05\n",
      "train loss:0.0026272370126038617\n",
      "train loss:0.00021171152421077194\n",
      "train loss:1.400706697013192e-05\n",
      "train loss:0.0006037699490458123\n",
      "train loss:0.00036175316749099566\n",
      "train loss:0.0003617419926758654\n",
      "train loss:0.0023980318892323172\n",
      "train loss:0.0008413637261352663\n",
      "train loss:0.0008857367320567492\n",
      "train loss:0.0011695344590462074\n",
      "train loss:0.0004923318858752873\n",
      "train loss:0.0006441856247137949\n",
      "train loss:0.002572681382516645\n",
      "train loss:0.0012611318008030376\n",
      "train loss:0.00029324725205447127\n",
      "train loss:0.0039650487181204\n",
      "train loss:0.001997130909908617\n",
      "train loss:0.0014641146819518753\n",
      "train loss:0.0007697346371197187\n",
      "train loss:1.5252797471180506e-05\n",
      "train loss:0.001122827852506087\n",
      "train loss:0.0009589523933514679\n",
      "train loss:0.000615931149780083\n",
      "train loss:0.0021802944479899287\n",
      "train loss:0.0001118681777996913\n",
      "train loss:2.841287795530174e-05\n",
      "train loss:0.003165250983305981\n",
      "train loss:0.0005732460056090744\n",
      "train loss:0.0003686613569935768\n",
      "train loss:0.0026357628202281365\n",
      "train loss:9.840000586996571e-05\n",
      "train loss:0.0005125029860974811\n",
      "train loss:0.0011463926393184857\n",
      "train loss:0.0002610022163475156\n",
      "train loss:0.0010914346276483403\n",
      "train loss:0.00029860880077379245\n",
      "train loss:0.0007126686853415749\n",
      "train loss:0.0010469731766306952\n",
      "train loss:0.0009763810005724556\n",
      "train loss:0.0006985947024434354\n",
      "train loss:0.0011687520977790378\n",
      "train loss:0.0011992718456532493\n",
      "train loss:0.00018921894040487626\n",
      "train loss:0.0005047541128570998\n",
      "train loss:0.0005669752049768651\n",
      "train loss:0.0003234985319187364\n",
      "train loss:0.00032070993763545384\n",
      "train loss:0.001012876569684008\n",
      "train loss:0.00039776808588523545\n",
      "train loss:0.0010780656710418731\n",
      "train loss:0.002954493705343616\n",
      "train loss:0.0006841228611607324\n",
      "train loss:0.001781300668166164\n",
      "train loss:0.0014940636643969993\n",
      "train loss:0.0011935040706683665\n",
      "train loss:9.230960251587381e-05\n",
      "train loss:0.0002243296475343419\n",
      "train loss:6.760853422920132e-05\n",
      "train loss:0.00482745221495381\n",
      "train loss:0.0011063965393363687\n",
      "train loss:7.280013141710849e-05\n",
      "train loss:0.0013004599056087296\n",
      "train loss:0.0027843122522745204\n",
      "train loss:0.0007265169142213206\n",
      "train loss:9.631329692387597e-05\n",
      "train loss:0.0007230455530627322\n",
      "train loss:0.00017294680675171843\n",
      "train loss:0.0008479440569318628\n",
      "train loss:0.0014278391766815893\n",
      "train loss:0.0006693535087896935\n",
      "train loss:0.00037264449439890137\n",
      "train loss:7.509846718100184e-05\n",
      "train loss:0.0003079814139045919\n",
      "train loss:0.00025018313190650255\n",
      "train loss:0.00048712673469211346\n",
      "train loss:0.0006540203633864101\n",
      "train loss:0.0017526826248208103\n",
      "train loss:0.003603837081092442\n",
      "train loss:0.0014151972249139238\n",
      "train loss:0.0020805908766078733\n",
      "train loss:0.0006195576536088289\n",
      "train loss:0.0008430005122740921\n",
      "train loss:0.000505729770622497\n",
      "train loss:0.00021211485208303709\n",
      "train loss:5.5056029175652436e-05\n",
      "train loss:0.001037508452678939\n",
      "train loss:0.0008880137518304805\n",
      "train loss:0.001450521900821857\n",
      "train loss:0.0004919354144440531\n",
      "train loss:0.0012059545404821228\n",
      "train loss:0.00023058021313463375\n",
      "train loss:0.006889421317097013\n",
      "train loss:0.0008708622105288319\n",
      "train loss:3.3871927926028484e-05\n",
      "train loss:1.6790152728748407e-05\n",
      "train loss:0.0005179013281651624\n",
      "train loss:0.0028229385090999502\n",
      "train loss:0.00015245752007487042\n",
      "train loss:0.0003357611815292747\n",
      "train loss:0.00019758993098625323\n",
      "train loss:0.0012647347027972683\n",
      "train loss:0.003277023467534773\n",
      "train loss:0.001369994454914104\n",
      "train loss:3.577729301737253e-05\n",
      "train loss:0.000187151941351185\n",
      "train loss:0.0001781995758405198\n",
      "train loss:0.000391177889348681\n",
      "train loss:0.0017789503456801582\n",
      "train loss:0.001982065804152069\n",
      "train loss:0.00022580196793548803\n",
      "train loss:0.0009156303417208776\n",
      "train loss:0.007272265438155224\n",
      "train loss:5.9434715670581134e-05\n",
      "train loss:0.00016607950167867546\n",
      "train loss:0.0005542844987146137\n",
      "train loss:0.00013361081629240681\n",
      "train loss:0.000277136346721202\n",
      "train loss:0.001834434475496543\n",
      "train loss:0.000108978215646793\n",
      "train loss:0.00011209871125118154\n",
      "train loss:0.00013386494004831052\n",
      "train loss:0.00020866967803473337\n",
      "train loss:9.366869742958335e-05\n",
      "train loss:0.00025343710239931073\n",
      "train loss:0.0010399501158696273\n",
      "train loss:0.0016590845316189111\n",
      "train loss:0.0003581712760527167\n",
      "train loss:0.00031897206284877593\n",
      "train loss:0.000419904642504667\n",
      "train loss:0.00011104142311741236\n",
      "train loss:0.00025720205687969236\n",
      "train loss:0.00019735032024715017\n",
      "train loss:0.0003025081153359357\n",
      "train loss:0.0005877560303732703\n",
      "train loss:0.00019811062728839819\n",
      "train loss:0.0002572403150371943\n",
      "train loss:5.352662796241348e-05\n",
      "train loss:0.0003278116548654094\n",
      "train loss:7.593878616743788e-05\n",
      "train loss:0.0006644738133877834\n",
      "train loss:6.199312801374789e-05\n",
      "train loss:0.0009951285833324282\n",
      "train loss:0.0005892988470415366\n",
      "train loss:0.0029027256811799125\n",
      "train loss:0.000147092632703594\n",
      "train loss:0.001131959657937472\n",
      "train loss:0.0003889296351881445\n",
      "train loss:2.4452998166459822e-05\n",
      "train loss:9.414881781245919e-05\n",
      "train loss:0.000404830719685899\n",
      "train loss:8.263144892595213e-05\n",
      "train loss:0.0006182898050245654\n",
      "train loss:0.00039997472118423764\n",
      "train loss:1.9528986793201544e-05\n",
      "train loss:7.530083357662908e-05\n",
      "train loss:0.0003547095681948009\n",
      "train loss:0.00017934032755783817\n",
      "train loss:7.657841070082998e-05\n",
      "train loss:0.0003909614349050031\n",
      "train loss:4.041312309392043e-05\n",
      "train loss:0.00017002303416301798\n",
      "train loss:0.0010962027434067336\n",
      "train loss:0.0030998744598408345\n",
      "train loss:0.0009754161332629401\n",
      "train loss:1.6823078844697164e-05\n",
      "train loss:0.00028259848695105544\n",
      "train loss:3.064808833868711e-05\n",
      "train loss:0.00012361669069231376\n",
      "train loss:0.0001198015563947939\n",
      "train loss:0.00034897044001983317\n",
      "train loss:0.00010616578147073877\n",
      "train loss:0.00010948106474130867\n",
      "train loss:0.0013668531526140374\n",
      "train loss:0.0024490213728147244\n",
      "train loss:0.0001218119634453386\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9903\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\deep-learning-from-scratch-master')  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_iuZyRYWq52"
   },
   "source": [
    "# CNN 시각화 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vf2OkfrBX119"
   },
   "source": [
    "필터 시각화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "executionInfo": {
     "elapsed": 1597,
     "status": "error",
     "timestamp": 1606399434763,
     "user": {
      "displayName": "‍구병모[ 학부재학 / 통계학과 ]",
      "photoUrl": "",
      "userId": "10190861927292343283"
     },
     "user_tz": -540
    },
    "id": "QHpiQKZ-X40g",
    "outputId": "d0b1e736-a0e2-4b80-a1dc-2e62f52b2892"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcEAAAEgCAYAAADMo8jPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAcxUlEQVR4nO3ceXSU5d3G8d+QhKxkNSEsAlWwVBBRbLEcqagFXEAR94VNpYhSEdkUCwW0aEEWkVPxcNxwa09RKVpQtEo9VYSCqIBQ1oRVwwSyQRK25/0DZ97Qg72v511r7u/nr0fPdf+4Z+aZXJmcM3ckCAIDAMBHDf6/NwAAwP8XShAA4C1KEADgLUoQAOAtShAA4C1KEADgrcQw4UgkEkQiEWcuJydHntmggdbDyr8b06xZM2dmx44dVlpaGjEzS0hICJKSkpxr2rdvL+9h165dUq5hw4byzPLycilXUVERDYIgPy0tLcjOznbm1dfAzKy6ulrKZWVlyTNTUlKk3IYNG6JBEOR/Oz8oLCx0rikpKZH3oX5dSLlXYo4fP+7MHDx40GpqaiJmZpmZmUFBQYFzTTQalfegvmYJCQnyTPU5iN2LSUlJgfI6q/eCmVlaWpq6B3mm+hyUlpbG78WUlJQgIyPDuaa2tlbeh/K+NTPbu3evPDM1NdWZqampsSNHjkTMzNTXTN2rmdmxY8ekXGKiXk3qzD179sRfs5P+LflfshNFlJyc7Mz17NlTntmoUSMpF+YNOmXKFGfmkksuiV8nJSVZixYtnGtWrVol72Hs2LFSrmnTpvLMJUuWSLl333232OzEzTl48GBnPj09Xd7DunXrpNzll18uz2zbtq2U69SpU3HsurCw0J555hnnmtmzZ8v7UArLzEwpqRilgBYvXnzS7OnTpzvXPPvss/Ie1q5dK+XC/OLSuHFjKbd06dJisxPldv755zvzZ511lryH8847T8q9//778kz159H8+fPj92JGRob17t3buWbr1q3yPvr27SvlJk6cKM/s2LGjM7N69er4dUpKivQc9+nTR96D+gvJaaedJs88cOCAlJswYULxqf4/fw4FAHiLEgQAeIsSBAB4ixIEAHiLEgQAeIsSBAB4ixIEAHiLEgQAeCvUl+WzsrJO+pL5d5k7d648MzMzU8qF+aL6zTff7Mxs27Ytft20aVObMGGCc82QIUPkPbz00ktSbuHChfLMv/3tb3LW7MSX4H/84x87c0VFRfJM5TQeM7PbbrtNnvn555/L2ZjKykpbtmyZMxfmdJdXX31Vyl111VXyTOXLzHXv7T179khr1qxZI+9B/WL92WefLc+85ZZb5KzZidNKwsxXNGnSRMpt375dnjl+/HgpN3/+/Pj18ePHpUMRJk+eLO/j/vvvl3KjRo2SZ6qPLUY9HCXMYRu9evWScrt375Zndu7cWcp91894PgkCALxFCQIAvEUJAgC8RQkCALxFCQIAvEUJAgC8RQkCALxFCQIAvEUJAgC8RQkCALwV6ti0vLw869evnzPXtm1beWZBQYGUq6mpkWcOGzbMmfnqq6/i19Fo1J5//nnnmtzcXHkPl19+uZTr2bOnPPPqq6+Ws2Zm+/bts3nz5jlzb775pjxT3e/QoUPlmceOHZOzMQcPHrQVK1Y4c9ddd508MzFRezuEOTJs/fr1zkzdI7fy8vKsf//+zjV/+MMf5D3cddddUu6mm26SZ7788stSrmvXrmZmlp2dbX379nXmw9yLyr1tZtaoUSN5Zpj7JSYtLc06duzozL3wwgvyzJ07d0q5adOmyTODIHBmLrjggpPyR44cca5p06aNvIeNGzdKOeUYupipU6fK2VPhkyAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBboU6MSUlJsR/+8IfO3MCBA+WZjz32mJQLc1qKcpJENBqNX+fk5NiNN97oXPPss8/Ke/jFL34h5T7++GN5pnLihpnZokWLzMwsKyvLrrzySmd+7ty58h4SEhKkXEpKijxz06ZNcjZGPaXjr3/9qzxTPdGjvLxcnqmc7LJ///6TZr/11lvONXv37pX3oJ4EE+ZEpKKiIjkbo5xYcs8998jzJk2aJOUWL14szxwxYoSUmzVrVvy6rKws/p77VzZv3izv43e/+52UU05NinnyySedmZKSkvh1w4YNrWnTps41l156qbyHSCQi5Xr06CHP/Oijj+TsqfBJEADgLUoQAOAtShAA4C1KEADgLUoQAOAtShAA4C1KEADgLUoQAOAtShAA4C1KEADgrVDHpq1fv97atWvnzHXv3l2eWVpaKuXKysrkmePGjXNmdu7cGb/esWOH3X333c41kydPlvdQ9/ihf6V58+byzHfffVfOmpnl5+fbkCFDnLmxY8fKMzt06CDlwhxVlZycLGdjmjVrJh25N2fOHHlmkyZNpNzy5cvlmcOGDXNmtm3bFr8+fPjwSffmd8nIyJD30KtXLym3ceNGeeauXbvkrNmJY/TOPvtsZ27ChAnyzAsuuEDK/fKXv5Rndu7cWc7G5OXlWf/+/Z059Sg0M/0Iv5/85CfyzHXr1jkz1dXV8eu8vDwbNGiQc83Ro0flPWzZskXOqqqqqqTcdx2zyCdBAIC3KEEAgLcoQQCAtyhBAIC3KEEAgLcoQQCAtyhBAIC3KEEAgLcoQQCAtyJBEOjhSGSfmRX/723n/1TLIAjyzerd4zL79rHV18dlVu9es/r6uMy4F79v6uvjMqvz2OoKVYIAANQn/DkUAOAtShAA4C1KEADgLUoQAOAtShAA4C1KEADgLUoQAOAtShAA4C1KEADgLUoQAOAtShAA4C1KEADgLUoQAOAtShAA4C1KEADgLUoQAOAtShAA4C1KEADgLUoQAOCtxDDhpKSkIDk52ZnLysqSZ5aWlkq5/Px8eWZGRoYzs3fvXisrK4uYmSUnJwfKmqZNm8p7qKyslHLFxcXyzIYNG0q5w4cPR4MgyE9NTQ2U1yI7O1veg/I8mZlVVVXJM2tra6VcUVFRNAiCfDOztLQ06bE1a9ZM3sfevXulXIMG+u+O/5V7MT093bmmsLBQ3sPhw4elXEJCgjyzrKxMypWUlESDIMiPRCKBkj/99NPlPUQiESmXmZkpz1Sfq02bNsXvxczMzKCgoMC55uDBg/I+1J+LHTp0kGfW1NQ4M7t377YDBw5EzE48LuXnrvqcmek/a5Seidm5c6eUi92L//z/Q5VgcnKydezY0Zm78sor5Znz58+Xcnfffbc8s3Pnzs7MHXfcEb/OyMiwnj17OtdMnjxZ3sNHH30k5e688055pvoDffv27cVmJ34Z6d+/vzPfp08feQ9dunSRcp988ok8c+vWrVKuf//+8d8YsrKybNCgQc41U6ZMkffx6KOPSrnU1FR5ZteuXZ2ZAQMGxK/T09Ole3HMmDHyHoqKiqRcTk6OPHPRokVSbubMmfpveRbucam/FHbv3l2euWPHDinXrVu3+OMqKCiwGTNmONesXLlS3sdzzz0n5VatWiXP/Oqrr5yZG2+8MX6dn59vjz/+uHON+pyZ6T9rzjzzTHnmiBEjpNysWbNOeS/y51AAgLcoQQCAtyhBAIC3KEEAgLcoQQCAtyhBAIC3KEEAgLcoQQCAt0J9Wb6goED60rrypcwY9WSRjRs3yjOVEz3qnt6Qk5NjN910k3NNmC/Lr1mzRsp9+OGH8szERO3lin1BOzMz0y699FJn/sILL5T3oJ7SsXr1anmmegrNP1Ne5zBfAO/Xr5+Uu/322+WZjz32mDPz9ddfx6+zsrLsiiuucK45//zz5T08+eSTUi7Me0w5hMHMbObMmWZm1rp1a5s9e7Yz//TTT8t7UL+wP2zYMHnmvffeK2djSkpKbNasWc7c0qVL5Zlt27aVcseOHZNnLly40Jk5cODASbOVk69GjRol7+Hmm2+Wcvfdd5888+GHH5Zy3/Ua8UkQAOAtShAA4C1KEADgLUoQAOAtShAA4C1KEADgLUoQAOAtShAA4C1KEADgLUoQAOCtUMembd++XTpa6le/+pU8c9q0aVJuwYIF8syioiJnpra2Nn6dnJxsrVq1cq4pKyuT96Acw2YW7rlSjxyKqampsU2bNjlzdY/tcnnuueekXHZ2tjxz5cqVcjamsrLSPvjgA2dOOeYvZvHixVJu+PDh8szjx487M0EQxK9ra2tt27ZtzjW9evWS9/DnP/9Zyr377rvyzJEjR8pZM7NDhw7ZqlWrnLkVK1bIMx9//HEp16NHD3nm5ZdfLmdjkpOTrU2bNs7c3Llz5Znq81D3mDMX9XixmISEBGvUqJEzN3bsWHmmcg+Ymd1www3yzOeff17OngqfBAEA3qIEAQDeogQBAN6iBAEA3qIEAQDeogQBAN6iBAEA3qIEAQDeogQBAN4KdWJMYWGhDRgwwJnLysqSZ7711ltS7vXXX5dn7tixw5l5//3349cVFRXS6SOLFi2S9/CDH/xAyoU5RWL69OlyNowtW7bI2VGjRkm56upqeaby3P+z7Oxs6927tzNX92QglxdeeEHK3X///fLMM844w5n59NNP49dpaWl27rnnOtekpKTIe3jxxRelXN33hMvf//53OWt24lSiqVOnOnPjx4+XZ65Zs0bKNW/eXJ75yCOPSLm6PweDILCjR48616xevVreh/peX79+vTzzqaeecmbqvkZHjx610tJS55rRo0fLe3jggQekXIMG+uez5cuXy9lT/lv/rdUAAHyPUYIAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvhTo2LSkpSTqCaO3atfJM9cibKVOmyDOVI7ui0Wj8Ojk52Vq2bOlck5eXJ+/hsssuk3Jhjk374osv5KyZWXp6ul144YXO3IoVK+SZS5YskXI1NTXyzDD3S0wQBHbs2DFnbvDgwfJM9Xithg0byjPHjh3rzCQkJJw0u0WLFs41y5Ytk/egPE9hZ6rHi3Xr1s3MzM455xzpWLa77rpL3oNybJ6Z2XPPPSfPnDhxopyNycjIsC5dujhzL7/8sjxzzpw5Uu5HP/qRPFM5zrLuvZiZmWndu3d3rpk0aZK8h/POO0/KhTl28fzzz5dy99133yn/P58EAQDeogQBAN6iBAEA3qIEAQDeogQBAN6iBAEA3qIEAQDeogQBAN6iBAEA3ooEQaCHI5F9Zlb8v7ed/1MtgyDIN6t3j8vs28dWXx+XWb17zerr4zLjXvy+qa+Py6zOY6srVAkCAFCf8OdQAIC3KEEAgLcoQQCAtyhBAIC3KEEAgLcoQQCAtyhBAIC3KEEAgLcoQQCAtyhBAIC3KEEAgLcoQQCAtyhBAIC3KEEAgLcoQQCAtyhBAIC3KEEAgLcoQQCAtyhBAIC3EsOEMzIygtzcXGeuoKBAnllWViblUlJS5JmlpaXOTHl5uR06dChiZtawYcMgNTXVuaawsFDeQ01NjZRLSEiQZ6anp0u5devWRYMgyM/MzAwaN27szB8/flzew44dO6Sc8u/GHD58WMrt27cvGgRBvplZZmZmkJ+f71yTk5Mj7+PQoUNS7uuvv5ZnJiUlOTMVFRVWXV0dMTNLTU0NGjVq5FxTVVUl7yExUXubJycnyzOVPZqZbd++PRoEQX5KSkqg3L8ZGRnyHo4cOSLlotGoPFN9rqqrq+P3YnJysvTzQ33/mpnt2bNHyuXl5ckzW7Vq5cwUFRVZNBqNmJllZWVJPz/UvZrpP0PVXjDT75ni4uL4a1ZXqBLMzc21MWPGOHPDhg2TZ77xxhtSrn379vLM559/3pl54YUX4tepqan205/+1LnmoYcekvewYcMGKRfmTd+5c2cpd9ZZZxWbnSiiGTNmOPNqYZuZ3XPPPVIuzD1QXFws5ebOnRsP5ufn29SpU51rrrvuOnkfq1evlnLTpk2TZypv+tdeey1+3ahRI7v++uudaz755BN5D8ovC2ZmZ5xxhjyza9euUq5fv37FZicK4Iorrvgfm2tmtmvXLilX973uctppp0m5zz//PH4vpqamWrdu3ZxrOnXqJO9jwoQJUq53797yTOXn4gUXXBC/bty4sc2ZM8e55te//rW8h9GjR0u5t99+W57ZpUsXKTd48OBT/qDhz6EAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG+F+p5gYmKiKV+Wnzx5sjzz1ltvlXKLFy+WZypfIK37pdiWLVvavHnznGvqfp/LZfz48VKuvLxcnvnAAw/IWTOzyspKW7ZsmTN35plnyjMPHjwo5cJ8j23SpElSbu7cufHrI0eOSN8TU17XmG3btkm5SCQiz/ziiy+cmbpf0q+trbWioiLnmjBfbFdf30svvVSeGea7d2Ynvn83ePBgZ+7mm2+WZ6qHFqivq5nZU089JeU+//zz+HWrVq2k+2zs2LHyPtTvJH/88cfyzEGDBjkzde+9nTt32siRI51r1q5dK+/hyiuvlHLXXHONPDPMe/xU+CQIAPAWJQgA8BYlCADwFiUIAPAWJQgA8BYlCADwFiUIAPAWJQgA8BYlCADwFiUIAPBWqGPTkpKSrLCw0JlTjn2KmT59upRbsGCBPPP66693Zmpra+PXmzdvth49ejjXvPfee/IePv30UynXoUMHeeaePXvkrJlZdXW1rV+/3pkLc8TZwIEDpdzTTz8tz7z99tvlbExSUpI1adLEmauoqJBnqkf4KUehxXzwwQfOTN1jpwoKCuyee+5xrpk9e7a8hzvuuEPK1T2WzuWcc86RckuWLDGzE8cDKkcfqq+BmdmBAweknPpeNDM7evSonI2prKyUXud+/frJM9Xj2wYMGCDP/Mc//uHM1D1OMj8/34YMGeJc0759e3kPq1atknKvvPKKPLN3795SbuXKlaf8/3wSBAB4ixIEAHiLEgQAeIsSBAB4ixIEAHiLEgQAeIsSBAB4ixIEAHiLEgQAeCvUiTFmZgkJCc7Mww8/LM9TT0a4+uqr5Zn9+/d3ZsrKyuLXrVu3tjfffNO5ZurUqfIeqqqqpNzmzZvlmZdccomU+/DDD83MrGnTpjZx4kRnPsyJMStWrJByYU6yeOONN+RsTFlZmS1cuNCZO/PMM+WZKSkpUm7v3r3yzKZNmzozSUlJ8euSkhKbM2eOc83SpUvlPUQiESn3+OOPyzO3bNkiZ83M0tLSpFNmwtwLR44ckXJt2rSRZ/5X7sWMjAy76KKLnLmsrCx55q5du6RcmBOn0tLSnJk//vGP8evKysr4z5J/pW/fvvIeUlNTpdzvf/97eeZnn30mZ0+FT4IAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG+FOjatsrLSli1b5syVl5fLM++9914p17ZtW3nmhg0bnJmf//zn8esjR47Y119/7Vzz5ZdfyntQniezcMc0vf/++1IudtTR7t27bfz48c78e++9J++hffv2Um7nzp3yzNtuu03OxlRVVdny5cudubrHQLmMGzdOyk2ePFmeOWPGDGcmMfE/34ZVVVXSMXavv/66vIfWrVtLuTDv261bt8pZM7Pc3Fy7/fbbnbm6R8i5tGvXTsq9+uqr8szi4mI5G7N27VrpeL5169bJM0eOHCnlnnjiCXnm3LlznZnf/va38evmzZtLR0Xm5eXJe/joo4+k3M9+9jN55kMPPSRnT4VPggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG9FgiDQw5HIPjMLf6TCv6eWQRDkm9W7x2X27WOrr4/LrN69ZvX1cZlxL37f1NfHZVbnsdUVqgQBAKhP+HMoAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuJYcLp6elBTk6OMxcEgTyztLRUyhUWFsozq6qqpExNTU3EzCw3Nzdo3ry5c83WrVvlPaj7PXr0qDyzpKREytXU1ESDIMg/7bTTglatWjnzmzZtkvdQWVkp5Ro3bizPzM/Pl3Lr1q2LBkGQb2aWmZkZFBQUONccPnxY3kejRo2k3K5du+SZqampzkx5eblVV1dHzE68x3Jzc51rwtw36vsxOztbnrl7924pV1VVFQ2CID8rK0t6vY4fPy7vIcxzoDp06JCUi0aj8XsR32+hSjAnJ8eGDx/uzNXU1MgzX3nlFSk3ZswYeebHH3/szPzpT3+KXzdv3twWLVrkXHPjjTfKexg1apSU279/vzxz9uzZUm7Dhg3FZmatWrWyVatWOfOXXXaZvIcPPvhAyt12223yzKFDh0q5Nm3aFMeuCwoKbMaMGc41xcXFzkzMxRdfLOUefPBBeWa7du2cmZdeeil+nZubayNGjHCuCXPfVFdXS7lrr71Wnjl+/Hgpt2zZsmKzE6/XzJkznXm1hMzMotGolGvQQP+D12effSbl5s2bp99Y+LfGn0MBAN6iBAEA3qIEAQDeogQBAN6iBAEA3qIEAQDeogQBAN4K9T3BwsJCGz16tDN36623yjP79u0r5e688055pvI9q4SEhPh1eXm5vfPOO8416l7N9C8TL1y4UJ65YMECKRf7btqePXts0qRJzvwll1wi70F5nszMBg8eLM+85ZZb5GzM1q1b7ZprrnHm6n4f1EU94KBTp07yzEceecSZ+fDDD+PX33zzjT3xxBPONVdffbW8h/bt20u5Tz75RJ45cOBAKbds2TIzO3E4hTI/zHeMle+JmoV731511VVSbt68efJM/HvjkyAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuhjk3bunWrdARRt27d5Jnp6elSbtCgQfLMJUuWODMVFRXx62PHjp3039/lxRdflPegHOllpj9+M7PVq1fLWTOzysrKk47k+i7PPvusPPOiiy6Scvfff7888/TTT5dyq1atil+3aNHCHnzwQeeaMM/v/PnzpVzPnj3lmbNmzXJmvvnmm/h1w4YNrWXLlv+jexg3bpyUi0aj8sxnnnlGzpqZNWjQwNLS0py5c889V545ffp0KRfmGMdjx47JWdQPfBIEAHiLEgQAeIsSBAB4ixIEAHiLEgQAeIsSBAB4ixIEAHiLEgQAeIsSBAB4K9SJMUlJSVZYWOjM1dTUyDObN28u5fr06SPPbN++vTMzc+bM+HVhYaGNGTPGuWb//v3yHpKSkqRcQkKCPPO1116Ts2YnTulITU115kaPHh1qrqKyslLOjh07Vso9+uij8evc3Fy75ZZbnGt69+4t70M96egvf/mLPFO5v+veAzk5OXbTTTc513z55ZfyHjZu3CjlwpzWopwcVVdaWpp16tTJmTt48KA8MzEx1I8vSZj3OOoHPgkCALxFCQIAvEUJAgC8RQkCALxFCQIAvEUJAgC8RQkCALxFCQIAvEUJAgC8RQkCALwV6tyhgwcP2urVq525t99+W565e/duKdeuXTt55qRJk5yZ5OTk+HVFRYW98847zjULFy6U91BcXCzlrr32Wnlm2GPTEhMTLScnx5lbu3atPFOZZ2Z2xhlnyDOHDh0qZ2NKS0vtpZdecuYuvvhieWaDBtrvhOpzYGb2m9/8xpnZu3dv/LqiosKWLl3qXLN8+XJ5D6ra2lo5O3z48FCzjx8/bocOHXLmbrjhBnnm/PnzpZxy1GNM//795SzqBz4JAgC8RQkCALxFCQIAvEUJAgC8RQkCALxFCQIAvEUJAgC8RQkCALxFCQIAvBUJgkAPRyL7zEw7CuXfX8sgCPLN6t3jMvv2sdXXx2VW716z+vq4zDy4F/H9FqoEAQCoT/hzKADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFv/AUkh+k48qNLOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\deep-learning-from-scratch-master')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ch07.simple_convnet import SimpleConvNet\n",
    "\n",
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "network = SimpleConvNet()\n",
    "# 무작위(랜덤) 초기화 후의 가중치\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "# 학습된 가중치\n",
    "network.load_params(\"C:\\deep-learning-from-scratch-master\\ch07/params.pkl\")\n",
    "filter_show(network.params['W1'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNBt6jfVSX0drxm9oHO+NMc",
   "name": "Ch7.합성곱 신경망(CNN).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
